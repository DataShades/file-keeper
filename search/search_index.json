{"config":{"lang":["en"],"separator":"[\\s\\-\\.\\_]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>file-keeper provides an abstraction layer for storing and retrieving files, supporting various storage backends like local filesystems, cloud storage (S3, GCS), and more.  It simplifies file management by providing a consistent API regardless of the underlying storage.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install file-keeper using pip:</p> <pre><code>pip install file-keeper\n</code></pre> <p>Note</p> <p>To use specific storage adapters, you may need to install extra dependencies. Most standard adapters do not require extras, but some \u2013 like those interfacing with external cloud providers \u2013 do. Here's a table of available extras:</p> Storage Type Adapter Name Extras Driver AWS S3 <code>file_keeper:s3</code> <code>s3</code> boto3 Apache Libcloud <code>file_keeper:libcloud</code> <code>libcloud</code> apache-libcloud Apache OpenDAL <code>file_keeper:opendal</code> <code>opendal</code> opendal Azure Blob Storage <code>file_keeper:azure_blob</code> <code>azure</code> azure-storage-blob Google Cloud Storage <code>file_keeper:gcs</code> <code>gcs</code> google-cloud-storage Redis <code>file_keeper:redis</code> <code>redis</code> redis SQLAlchemy <code>file_keeper:sqlalchemy</code> <code>sqlalchemy</code> SQLAlchemy <p>For example, to install file-keeper with S3 support:</p> <pre><code>pip install 'file-keeper[s3]'\n</code></pre>"},{"location":"#basic-configuration-and-usage-fs-adapter","title":"Basic configuration and usage (FS adapter)","text":"<p>Let's start with a simple example using the local filesystem (FS) adapter.</p> <p>Example</p> <pre><code>from file_keeper import make_storage, make_upload\n\n# Create a storage instance.  The 'path' setting specifies the root directory\n# for storing files. 'initialize' will automatically create the directory\n# if it doesn't exist.\nstorage = make_storage(\n    \"my_fs_storage\",  # A name for your storage (for logging/debugging)\n    {\n        \"type\": \"file_keeper:fs\",\n        \"path\": \"/tmp/my_filekeeper_files\",\n        \"initialize\": True,\n    },\n)\n\n# Create an upload object from a byte string.\nupload = make_upload(b\"Hello, file-keeper!\")\n\n# Upload the file.  This returns a FileData object containing information\n# about the uploaded file.\nfile_data = storage.upload(\"my_file.txt\", upload)\n\n# Print the file data.\nprint(file_data)\n\n# The file is now stored in /tmp/my_filekeeper_files/my_file.txt\n\n# Get the content of file using corresponding FileData object\ncontent: bytes = storage.content(file_data)\n</code></pre> <p>Explanation:</p> <ul> <li><code>make_storage()</code>: Creates a storage instance with the specified configuration.</li> <li><code>make_upload()</code>: Creates an <code>Upload</code> object from the data you want to store.</li> <li><code>storage.upload()</code>: Uploads the data to the storage.</li> <li><code>FileData</code>:  A dataclass that contains metadata about the uploaded file, including its location, size, content type, and hash.</li> <li><code>storage.content()</code>: Locates file using <code>FileData</code> and returs byte string with its content</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Utilities available for import directly from the <code>file_keeper</code> module.</p>"},{"location":"api/#file_keeper.make_storage","title":"<code>make_storage(name, settings)</code>","text":"<p>Initialize storage instance with specified settings.</p> <p>Storage adapter is defined by <code>type</code> key of the settings. The rest of settings depends on the specific adapter.</p> PARAMETER DESCRIPTION <code>name</code> <p>name of the storage</p> <p> TYPE: <code>str</code> </p> <code>settings</code> <p>configuration for the storage</p> <p> TYPE: <code>dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>Storage</code> <p>storage instance</p> RAISES DESCRIPTION <code>UnknownAdapterError</code> <p>storage adapter is not registered</p> Example <pre><code>storage = make_storage(\"memo\", {\"type\": \"files:redis\"})\n</code></pre> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def make_storage(name: str, settings: dict[str, Any]) -&gt; Storage:\n    \"\"\"Initialize storage instance with specified settings.\n\n    Storage adapter is defined by `type` key of the settings. The rest of\n    settings depends on the specific adapter.\n\n    Args:\n        name: name of the storage\n        settings: configuration for the storage\n\n    Returns:\n        storage instance\n\n    Raises:\n        exceptions.UnknownAdapterError: storage adapter is not registered\n\n    Example:\n        ```\n        storage = make_storage(\"memo\", {\"type\": \"files:redis\"})\n        ```\n\n    \"\"\"\n    adapter_type = settings.pop(\"type\", None)\n    adapter = adapters.get(adapter_type)\n    if not adapter:\n        raise exceptions.UnknownAdapterError(adapter_type)\n\n    settings.setdefault(\"name\", name)\n\n    return adapter(settings)\n</code></pre>"},{"location":"api/#file_keeper.get_storage","title":"<code>get_storage(name, settings=None)</code>","text":"<p>Get storage from the pool.</p> <p>If storage accessed for the first time, it's initialized and added to the pool. After that the same storage is returned every time the function is called with the given name.</p> <p>Settings are required only for initialization, so you can omit them if you are sure that storage exists. Additionally, if <code>settins</code> are not specified but storage is missing from the pool, file-keeper makes an attempt to initialize storage usign global configuration. Global configuration can be provided as:</p> <ul> <li><code>FILE_KEEPER_CONFIG</code> environment variable that points to a file with configuration</li> <li> <p><code>.file-keeper.json</code> in the current directory hierarchy</p> </li> <li> <p><code>file-keeper/file-keeper.json</code> in the user's config directory(usually,   <code>~/.config/</code>) when platformdirs   installed in the environment, for example via <code>pip install   'file-keeper[user_config]'</code> extras.</p> </li> </ul> <p>File must contain storage configuration provided in format</p> <pre><code>{\n    \"storages\": {\n        # storage name\n        \"my_storage\": {\n            # storage options\n            \"type\": \"file_keeper:memory\"\n        }\n    }\n}\n</code></pre> <p>JSON configuration is used by default, because python has built-in JSON support. Additional file extensions are checked if environment contains correspoinding package:</p> Package Extension tomllib <code>.toml</code> tomli <code>.toml</code> pyyaml <code>.yaml</code>, <code>.yml</code> <p>Extensions are checked in order <code>.toml</code>, <code>.yaml</code>, <code>.yml</code>, <code>.json</code>.</p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def get_storage(name: str, settings: dict[str, Any] | None = None) -&gt; Storage:\n    \"\"\"Get storage from the pool.\n\n    If storage accessed for the first time, it's initialized and added to the\n    pool. After that the same storage is returned every time the function is\n    called with the given name.\n\n    Settings are required only for initialization, so you can omit them if you\n    are sure that storage exists. Additionally, if `settins` are not specified\n    but storage is missing from the pool, file-keeper makes an attempt to\n    initialize storage usign global configuration. Global configuration can be\n    provided as:\n\n    * `FILE_KEEPER_CONFIG` environment variable that points to a file with configuration\n    * `.file-keeper.json` in the current directory hierarchy\n\n    * `file-keeper/file-keeper.json` in the user's config directory(usually,\n      `~/.config/`) when [platformdirs](https://pypi.org/project/platformdirs/)\n      installed in the environment, for example via `pip install\n      'file-keeper[user_config]'` extras.\n\n    File must contain storage configuration provided in format\n\n    ```json\n    {\n        \"storages\": {\n            # storage name\n            \"my_storage\": {\n                # storage options\n                \"type\": \"file_keeper:memory\"\n            }\n        }\n    }\n    ```\n\n    JSON configuration is used by default, because python has built-in JSON\n    support. Additional file extensions are checked if environment contains\n    correspoinding package:\n\n    | Package | Extension |\n    |---|---|\n    | [tomllib](https://docs.python.org/3/library/tomllib.html) | `.toml` |\n    | [tomli](https://pypi.org/project/tomli/) | `.toml` |\n    | [pyyaml](https://pypi.org/project/PyYAML/) | `.yaml`, `.yml` |\n\n    Extensions are checked in order `.toml`, `.yaml`, `.yml`, `.json`.\n\n    \"\"\"\n    if name not in storages:\n        if settings is None:\n            config_file = os.getenv(\"FILE_KEEPER_CONFIG\")\n\n            if not config_file and (config_dir := user_config_dir(\"file-keeper\")):\n                config_file = _get_config_file_name(config_dir)\n\n            if not config_file:\n                path = pathlib.Path().absolute()\n\n                while len(path.parts) &gt; 1:\n                    if config_file := _get_config_file_name(str(path), hidden=True):\n                        break\n\n                    path = path.parent\n\n            if config_file:\n                log.debug(\"Load configuration from %s\", config_file)\n                cfg = _load_config_file(config_file)\n                settings = cfg.get(\"storages\", {}).get(name)\n\n            if not settings:\n                raise exceptions.UnknownStorageError(name)\n\n        storages.register(name, make_storage(name, settings))\n\n    return storages[name]\n</code></pre>"},{"location":"api/#file_keeper.make_upload","title":"<code>make_upload(value)</code>","text":"<p>Convert value into Upload object.</p> <p>Use this function for simple and reliable initialization of Upload object. Avoid creating Upload manually, unless you are 100% sure you can provide correct MIMEtype, size and stream.</p> PARAMETER DESCRIPTION <code>value</code> <p>content of the file</p> <p> TYPE: <code>Any</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>content has unsupported type</p> RETURNS DESCRIPTION <code>Upload</code> <p>upload object with specified content</p> Example <pre><code>upload = storage.upload(\"file.txt\", make_upload(b\"hello world\"))\n</code></pre> Source code in <code>src/file_keeper/core/upload.py</code> <pre><code>def make_upload(value: Any) -&gt; Upload:\n    \"\"\"Convert value into [Upload][file_keeper.Upload] object.\n\n    Use this function for simple and reliable initialization of\n    [Upload][file_keeper.Upload] object. Avoid creating\n    [Upload][file_keeper.Upload] manually, unless you are 100% sure you can\n    provide correct MIMEtype, size and stream.\n\n    Args:\n        value: content of the file\n\n    Raises:\n        TypeError: content has unsupported type\n\n    Returns:\n        upload object with specified content\n\n    Example:\n        ```python\n        upload = storage.upload(\"file.txt\", make_upload(b\"hello world\"))\n        ```\n\n    \"\"\"\n    if isinstance(value, Upload):\n        return value\n\n    initial_type: type = type(value)  # pyright: ignore[reportUnknownVariableType]\n\n    fallback_factory = None\n    for t in upload_factories:\n        if initial_type is t:\n            transformed_value = upload_factories[t](value)\n            if transformed_value is not None:\n                value = transformed_value\n                break\n\n        if not fallback_factory and issubclass(initial_type, t):\n            fallback_factory = upload_factories[t]\n\n    else:\n        if fallback_factory:\n            value = fallback_factory(value)\n\n    # ideal situation: factory produced the Upload object\n    if isinstance(value, Upload):\n        return value\n\n    if isinstance(value, bytes | bytearray):\n        value = BytesIO(value)\n\n    # convenient situation: factory produced binary buffer and we know how to\n    # transform it into an Upload. Factories will choose this option to avoid\n    # repeating mimetype detection logic\n    if isinstance(value, BytesIO | BufferedReader):\n        mime = magic.from_buffer(value.read(SAMPLE_SIZE), True)\n        _ = value.seek(0, 2)\n        size = value.tell()\n        _ = value.seek(0)\n\n        return Upload(value, getattr(value, \"name\", \"\"), size, mime)\n\n    raise TypeError(type(value))\n</code></pre>"},{"location":"api/#file_keeper.Storage","title":"<code>Storage</code>","text":"<p>Base class for storage implementation.</p> PARAMETER DESCRIPTION <code>settings</code> <p>storage configuration</p> <p> TYPE: <code>Mapping[str, Any] | Settings</code> </p> Example <pre><code>class MyStorage(Storage):\n    SettingsFactory = MySettings\n    UploaderFactory = MyUploader\n    ManagerFactory = MyManager\n    ReaderFactory = MyReader\n</code></pre> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>class Storage(ABC):  # noqa: B024\n    \"\"\"Base class for storage implementation.\n\n    Args:\n        settings: storage configuration\n\n    Example:\n        ```py\n        class MyStorage(Storage):\n            SettingsFactory = MySettings\n            UploaderFactory = MyUploader\n            ManagerFactory = MyManager\n            ReaderFactory = MyReader\n        ```\n    \"\"\"\n\n    # do not show storage adapter\n    hidden: bool = False\n    \"\"\"Flag that marks unsafe/experimental storages.\"\"\"\n\n    capabilities: Capability = Capability.NONE\n    \"\"\"Operations supported by storage. Computed from capabilities of\n    services during storage initialization.\"\"\"\n\n    SettingsFactory: ClassVar[type[Settings]] = Settings\n    \"\"\"Factory class for storage settings.\"\"\"\n    UploaderFactory: ClassVar[type[Uploader]] = Uploader\n    \"\"\"Factory class for uploader service.\"\"\"\n    ManagerFactory: ClassVar[type[Manager]] = Manager\n    \"\"\"Factory class for manager service.\"\"\"\n    ReaderFactory: ClassVar[type[Reader]] = Reader\n    \"\"\"Factory class for reader service.\"\"\"\n\n    @override\n    def __str__(self) -&gt; str:\n        return self.settings.name\n\n    def __init__(self, settings: Mapping[str, Any] | Settings, /):\n        self.settings = self.configure(settings)\n        self.uploader = self.make_uploader()\n        self.manager = self.make_manager()\n        self.reader = self.make_reader()\n\n        self.capabilities = self.compute_capabilities()\n\n    def make_uploader(self):\n        \"\"\"Initialize [uploader][file_keeper.Uploader] service.\"\"\"\n        return self.UploaderFactory(self)\n\n    def make_manager(self):\n        \"\"\"Initialize [manager][file_keeper.Manager] service.\"\"\"\n        return self.ManagerFactory(self)\n\n    def make_reader(self):\n        \"\"\"Initialize [reader][file_keeper.Reader] service.\"\"\"\n        return self.ReaderFactory(self)\n\n    @classmethod\n    def configure(cls, settings: Mapping[str, Any] | Settings) -&gt; Settings:\n        \"\"\"Initialize storage configuration.\n\n        This method is responsible for transforming mapping with options into\n        storage's settings. It also can initialize additional services and\n        perform extra work, like verifying that storage is ready to accept\n        uploads.\n\n        Args:\n            settings: mapping with storage configuration\n\n        \"\"\"\n        if isinstance(settings, Settings):\n            return settings\n\n        return cls.SettingsFactory.from_dict(settings)\n\n    def compute_capabilities(self) -&gt; Capability:\n        \"\"\"Computes the capabilities of the storage based on its services.\"\"\"\n        result = self.uploader.capabilities | self.manager.capabilities | self.reader.capabilities\n\n        for name in self.settings.disabled_capabilities:\n            result = result.exclude(Capability[name])\n\n        return result\n\n    def supports(self, operation: Capability) -&gt; bool:\n        \"\"\"Check whether the storage supports operation.\"\"\"\n        return self.capabilities.can(operation)\n\n    def supports_synthetic(self, operation: Capability, dest: Storage) -&gt; bool:\n        \"\"\"Check if the storage can emulate operation using other operations.\"\"\"\n        if operation is Capability.RANGE:\n            return self.supports(Capability.STREAM)\n\n        if operation is Capability.COPY:\n            return self.supports(Capability.STREAM) and dest.supports(\n                Capability.CREATE,\n            )\n\n        if operation is Capability.MOVE:\n            return self.supports(\n                Capability.STREAM | Capability.REMOVE,\n            ) and dest.supports(Capability.CREATE)\n\n        if operation is Capability.COMPOSE:\n            return self.supports(Capability.STREAM) and dest.supports(\n                Capability.CREATE | Capability.APPEND | Capability.REMOVE\n            )\n\n        return False\n\n    def full_path(self, location: types.Location, /, **kwargs: Any) -&gt; str:\n        \"\"\"Compute path to the file from the storage's root.\n\n        Args:\n            location: location of the file object\n            **kwargs: exra parameters for custom storages\n\n        Returns:\n            full path required to access location\n        \"\"\"\n        return os.path.join(self.settings.path, location)\n\n    def prepare_location(\n        self, location: str, upload_or_data: data.BaseData | Upload | None = None, /, **kwargs: Any\n    ) -&gt; types.Location:\n        \"\"\"Transform and sanitize location using configured functions.\"\"\"\n        for name in self.settings.location_transformers:\n            if transformer := location_transformers.get(name):\n                location = transformer(location, upload_or_data, kwargs)\n            else:\n                raise exceptions.LocationTransformerError(name)\n\n        return types.Location(location)\n\n    def file_as_upload(self, data: data.FileData, **kwargs: Any) -&gt; Upload:\n        \"\"\"Make an [Upload][file_keeper.Upload] with file content.\n\n        Args:\n            data: The FileData object to wrap into Upload\n            **kwargs: Additional metadata for the upload.\n        \"\"\"\n        stream = self.stream(data, **kwargs)\n        stream = cast(types.PStream, stream) if hasattr(stream, \"read\") else utils.IterableBytesReader(stream)\n\n        return Upload(\n            stream,\n            data.location,\n            data.size,\n            data.content_type,\n        )\n\n    @requires_capability(Capability.CREATE)\n    def upload(self, location: types.Location, upload: Upload, /, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Upload file using single stream.\n\n        Args:\n            location: The destination location for the upload.\n            upload: The Upload object containing the file data.\n            **kwargs: Additional metadata for the upload.\n        \"\"\"\n        return self.uploader.upload(location, upload, kwargs)\n\n    @requires_capability(Capability.RESUMABLE)\n    def resumable_start(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Prepare everything for resumable upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            **kwargs: Additional metadata for the upload.\n        \"\"\"\n        return self.uploader.resumable_start(data, kwargs)\n\n    @requires_capability(Capability.RESUMABLE)\n    def resumable_refresh(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Show details of the incomplete resumable upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            **kwargs: Additional metadata for the upload.\n        \"\"\"\n        return self.uploader.resumable_refresh(data, kwargs)\n\n    @requires_capability(Capability.RESUMABLE)\n    def resumable_resume(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Resume the interrupted resumable upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            **kwargs: Additional metadata for the upload.\n        \"\"\"\n        return self.uploader.resumable_resume(data, kwargs)\n\n    @requires_capability(Capability.RESUMABLE)\n    def resumable_remove(self, data: data.FileData, /, **kwargs: Any) -&gt; bool:\n        \"\"\"Remove incomplete resumable upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            **kwargs: Additional metadata for the upload.\n        \"\"\"\n        return self.uploader.resumable_remove(data, kwargs)\n\n    @requires_capability(Capability.MULTIPART)\n    def multipart_start(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Prepare everything for multipart upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            **kwargs: Additional metadata for the upload.\n        \"\"\"\n        return self.uploader.multipart_start(data, kwargs)\n\n    @requires_capability(Capability.MULTIPART)\n    def multipart_refresh(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Show details of the incomplete upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            **kwargs: Additional metadata for the upload.\n        \"\"\"\n        return self.uploader.multipart_refresh(data, kwargs)\n\n    @requires_capability(Capability.MULTIPART)\n    def multipart_update(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Add data to the incomplete upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            **kwargs: Additional metadata for the upload.\n        \"\"\"\n        return self.uploader.multipart_update(data, kwargs)\n\n    @requires_capability(Capability.MULTIPART)\n    def multipart_complete(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Verify file integrity and finalize incomplete upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            **kwargs: Additional metadata for the upload.\n        \"\"\"\n        return self.uploader.multipart_complete(data, kwargs)\n\n    @requires_capability(Capability.MULTIPART)\n    def multipart_remove(self, data: data.FileData, /, **kwargs: Any) -&gt; bool:\n        \"\"\"Interrupt and remove incomplete upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            **kwargs: Additional metadata for the upload.\n        \"\"\"\n        return self.uploader.multipart_remove(data, kwargs)\n\n    @requires_capability(Capability.EXISTS)\n    def exists(self, data: data.FileData, /, **kwargs: Any) -&gt; bool:\n        \"\"\"Check if file exists in the storage.\n\n        Args:\n            data: The FileData object representing the file to check.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return self.manager.exists(data, kwargs)\n\n    @requires_capability(Capability.REMOVE)\n    def remove(self, data: data.FileData, /, **kwargs: Any) -&gt; bool:\n        \"\"\"Remove file from the storage.\n\n        Args:\n            data: The FileData object representing the file to remove.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return self.manager.remove(data, kwargs)\n\n    @requires_capability(Capability.SCAN)\n    def scan(self, **kwargs: Any) -&gt; Iterable[str]:\n        \"\"\"List all locations(filenames) in storage.\n\n        Args:\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return self.manager.scan(kwargs)\n\n    @requires_capability(Capability.ANALYZE)\n    def analyze(self, location: types.Location, /, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Return file details.\n\n        Args:\n            location: The location of the file to analyze.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return self.manager.analyze(location, kwargs)\n\n    @requires_capability(Capability.SIGNED)\n    def signed(\n        self,\n        action: types.SignedAction,\n        duration: int,\n        location: types.Location,\n        **kwargs: Any,\n    ) -&gt; str:\n        \"\"\"Make an URL for signed action.\n\n        Args:\n            action: The action to sign (e.g., \"upload\", \"download\").\n            duration: The duration for which the signed URL is valid.\n            location: The location of the file to sign.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return self.manager.signed(action, duration, location, kwargs)\n\n    @requires_capability(Capability.STREAM)\n    def stream(self, data: data.FileData, /, **kwargs: Any) -&gt; Iterable[bytes]:\n        \"\"\"Return byte-stream of the file content.\n\n        Args:\n            data: The FileData object representing the file to stream.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return self.reader.stream(data, kwargs)\n\n    @requires_capability(Capability.RANGE)\n    def range(self, data: data.FileData, start: int = 0, end: int | None = None, /, **kwargs: Any) -&gt; Iterable[bytes]:\n        \"\"\"Return slice of the file content.\n\n        Args:\n            data: The FileData object representing the file to read.\n            start: The starting byte offset.\n            end: The ending byte offset (inclusive).\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return self.reader.range(data, start, end, kwargs)\n\n    def range_synthetic(\n        self, data: data.FileData, start: int = 0, end: int | None = None, /, **kwargs: Any\n    ) -&gt; Iterable[bytes]:\n        \"\"\"Generic implementation of range operation that relies on [STREAM][file_keeper.Capability.STREAM].\n\n        Args:\n            data: The FileData object representing the file to read.\n            start: The starting byte offset.\n            end: The ending byte offset (inclusive).\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        if end is None:\n            end = cast(int, float(\"inf\"))\n\n        end -= start\n        if end &lt;= 0:\n            return\n\n        for chunk in self.stream(data, **kwargs):\n            if start &gt; 0:\n                start -= len(chunk)\n                if start &lt; 0:\n                    chunk = chunk[start:]  # noqa: PLW2901\n                else:\n                    continue\n\n            yield chunk[: end and None]\n            end -= len(chunk)\n            if end &lt;= 0:\n                break\n\n    @requires_capability(Capability.STREAM)\n    def content(self, data: data.FileData, /, **kwargs: Any) -&gt; bytes:\n        \"\"\"Return file content as a single byte object.\n\n        Args:\n            data: The FileData object representing the file to read.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return self.reader.content(data, kwargs)\n\n    @requires_capability(Capability.APPEND)\n    def append(self, data: data.FileData, upload: Upload, /, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Append content to existing file.\n\n        Args:\n            data: The FileData object representing the file to append to.\n            upload: The Upload object containing the content to append.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return self.manager.append(data, upload, kwargs)\n\n    @requires_capability(Capability.COPY)\n    def copy(self, location: types.Location, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Copy file inside the storage.\n\n        Args:\n            location: The destination location for the copied file.\n            data: The FileData object representing the file to copy.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return self.manager.copy(location, data, kwargs)\n\n    def copy_synthetic(\n        self, location: types.Location, data: data.FileData, dest_storage: Storage, /, **kwargs: Any\n    ) -&gt; data.FileData:\n        \"\"\"Generic implementation of the copy operation that relies on [CREATE][file_keeper.Capability.CREATE].\n\n        Args:\n            location: The destination location for the copied file.\n            data: The FileData object representing the file to copy.\n            dest_storage: The destination storage\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return dest_storage.upload(\n            location,\n            self.file_as_upload(data, **kwargs),\n            **kwargs,\n        )\n\n    @requires_capability(Capability.MOVE)\n    def move(self, location: types.Location, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Move file to a different location inside the storage.\n\n        Args:\n            location: The destination location for the moved file.\n            data: The FileData object representing the file to move.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return self.manager.move(location, data, kwargs)\n\n    def move_synthetic(\n        self, location: types.Location, data: data.FileData, dest_storage: Storage, /, **kwargs: Any\n    ) -&gt; data.FileData:\n        \"\"\"Generic implementation of move operation.\n\n        Relies on [CREATE][file_keeper.Capability.CREATE] and\n        [REMOVE][file_keeper.Capability.REMOVE].\n\n        Args:\n            location: The destination location for the moved file.\n            data: The FileData object representing the file to move.\n            dest_storage: The destination storage\n            **kwargs: Additional metadata for the operation.\n\n        \"\"\"\n        result = dest_storage.upload(location, self.file_as_upload(data, **kwargs), **kwargs)\n        self.remove(data)\n        return result\n\n    @requires_capability(Capability.COMPOSE)\n    def compose(self, location: types.Location, /, *files: data.FileData, **kwargs: Any) -&gt; data.FileData:\n        \"\"\"Combine multiple files into a new file.\n\n        Args:\n            location: The destination location for the composed file.\n            *files: FileData objects representing the files to combine.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        return self.manager.compose(location, files, kwargs)\n\n    def compose_synthetic(\n        self, location: types.Location, dest_storage: Storage, /, *files: data.FileData, **kwargs: Any\n    ) -&gt; data.FileData:\n        \"\"\"Generic composition that relies on [APPEND][file_keeper.Capability.APPEND].\n\n        Args:\n            location: The destination location for the composed file.\n            dest_storage: The destination storage\n            *files: FileData objects representing the files to combine.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        result = dest_storage.upload(location, make_upload(b\"\"), **kwargs)\n\n        # when first append succeeded with the fragment of the file added\n        # in the storage, and the following append failed, this incomplete\n        # fragment must be removed.\n        #\n        # Expected reasons of failure are:\n        #\n        # * one of the source fiels is missing\n        # * file will go over the size limit after the following append\n        try:\n            for item in files:\n                result = dest_storage.append(\n                    result,\n                    self.file_as_upload(item, **kwargs),\n                    **kwargs,\n                )\n        except (exceptions.MissingFileError, exceptions.UploadError):\n            self.remove(result, **kwargs)\n            raise\n\n        return result\n\n    def one_time_link(self, data: data.FileData, /, **kwargs: Any) -&gt; str | None:\n        \"\"\"Return one-time download link.\n\n        Args:\n            data: The FileData object representing the file.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        if self.supports(Capability.LINK_ONE_TIME):\n            return self.reader.one_time_link(data, kwargs)\n\n    def temporal_link(self, data: data.FileData, duration: int, /, **kwargs: Any) -&gt; str | None:\n        \"\"\"Return temporal download link.\n\n        Args:\n            data: The FileData object representing the file.\n            duration: The duration for which the link is valid.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        if self.supports(Capability.LINK_TEMPORAL):\n            return self.reader.temporal_link(data, duration, kwargs)\n\n    def permanent_link(self, data: data.FileData, /, **kwargs: Any) -&gt; str | None:\n        \"\"\"Return permanent download link.\n\n        Args:\n            data: The FileData object representing the file.\n            **kwargs: Additional metadata for the operation.\n        \"\"\"\n        if self.supports(Capability.LINK_PERMANENT):\n            return self.reader.permanent_link(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.ManagerFactory","title":"<code>ManagerFactory = Manager</code>  <code>class-attribute</code>","text":"<p>Factory class for manager service.</p>"},{"location":"api/#file_keeper.Storage.ReaderFactory","title":"<code>ReaderFactory = Reader</code>  <code>class-attribute</code>","text":"<p>Factory class for reader service.</p>"},{"location":"api/#file_keeper.Storage.SettingsFactory","title":"<code>SettingsFactory = Settings</code>  <code>class-attribute</code>","text":"<p>Factory class for storage settings.</p>"},{"location":"api/#file_keeper.Storage.UploaderFactory","title":"<code>UploaderFactory = Uploader</code>  <code>class-attribute</code>","text":"<p>Factory class for uploader service.</p>"},{"location":"api/#file_keeper.Storage.capabilities","title":"<code>capabilities = self.compute_capabilities()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Operations supported by storage. Computed from capabilities of services during storage initialization.</p>"},{"location":"api/#file_keeper.Storage.hidden","title":"<code>hidden = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Flag that marks unsafe/experimental storages.</p>"},{"location":"api/#file_keeper.Storage.analyze","title":"<code>analyze(location, /, **kwargs)</code>","text":"<p>Return file details.</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the file to analyze.</p> <p> TYPE: <code>Location</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.ANALYZE)\ndef analyze(self, location: types.Location, /, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Return file details.\n\n    Args:\n        location: The location of the file to analyze.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return self.manager.analyze(location, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.append","title":"<code>append(data, upload, /, **kwargs)</code>","text":"<p>Append content to existing file.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to append to.</p> <p> TYPE: <code>FileData</code> </p> <code>upload</code> <p>The Upload object containing the content to append.</p> <p> TYPE: <code>Upload</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.APPEND)\ndef append(self, data: data.FileData, upload: Upload, /, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Append content to existing file.\n\n    Args:\n        data: The FileData object representing the file to append to.\n        upload: The Upload object containing the content to append.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return self.manager.append(data, upload, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.compose","title":"<code>compose(location, /, *files, **kwargs)</code>","text":"<p>Combine multiple files into a new file.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the composed file.</p> <p> TYPE: <code>Location</code> </p> <code>*files</code> <p>FileData objects representing the files to combine.</p> <p> TYPE: <code>FileData</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.COMPOSE)\ndef compose(self, location: types.Location, /, *files: data.FileData, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Combine multiple files into a new file.\n\n    Args:\n        location: The destination location for the composed file.\n        *files: FileData objects representing the files to combine.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return self.manager.compose(location, files, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.compose_synthetic","title":"<code>compose_synthetic(location, dest_storage, /, *files, **kwargs)</code>","text":"<p>Generic composition that relies on APPEND.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the composed file.</p> <p> TYPE: <code>Location</code> </p> <code>dest_storage</code> <p>The destination storage</p> <p> TYPE: <code>Storage</code> </p> <code>*files</code> <p>FileData objects representing the files to combine.</p> <p> TYPE: <code>FileData</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def compose_synthetic(\n    self, location: types.Location, dest_storage: Storage, /, *files: data.FileData, **kwargs: Any\n) -&gt; data.FileData:\n    \"\"\"Generic composition that relies on [APPEND][file_keeper.Capability.APPEND].\n\n    Args:\n        location: The destination location for the composed file.\n        dest_storage: The destination storage\n        *files: FileData objects representing the files to combine.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    result = dest_storage.upload(location, make_upload(b\"\"), **kwargs)\n\n    # when first append succeeded with the fragment of the file added\n    # in the storage, and the following append failed, this incomplete\n    # fragment must be removed.\n    #\n    # Expected reasons of failure are:\n    #\n    # * one of the source fiels is missing\n    # * file will go over the size limit after the following append\n    try:\n        for item in files:\n            result = dest_storage.append(\n                result,\n                self.file_as_upload(item, **kwargs),\n                **kwargs,\n            )\n    except (exceptions.MissingFileError, exceptions.UploadError):\n        self.remove(result, **kwargs)\n        raise\n\n    return result\n</code></pre>"},{"location":"api/#file_keeper.Storage.compute_capabilities","title":"<code>compute_capabilities()</code>","text":"<p>Computes the capabilities of the storage based on its services.</p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def compute_capabilities(self) -&gt; Capability:\n    \"\"\"Computes the capabilities of the storage based on its services.\"\"\"\n    result = self.uploader.capabilities | self.manager.capabilities | self.reader.capabilities\n\n    for name in self.settings.disabled_capabilities:\n        result = result.exclude(Capability[name])\n\n    return result\n</code></pre>"},{"location":"api/#file_keeper.Storage.configure","title":"<code>configure(settings)</code>  <code>classmethod</code>","text":"<p>Initialize storage configuration.</p> <p>This method is responsible for transforming mapping with options into storage's settings. It also can initialize additional services and perform extra work, like verifying that storage is ready to accept uploads.</p> PARAMETER DESCRIPTION <code>settings</code> <p>mapping with storage configuration</p> <p> TYPE: <code>Mapping[str, Any] | Settings</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@classmethod\ndef configure(cls, settings: Mapping[str, Any] | Settings) -&gt; Settings:\n    \"\"\"Initialize storage configuration.\n\n    This method is responsible for transforming mapping with options into\n    storage's settings. It also can initialize additional services and\n    perform extra work, like verifying that storage is ready to accept\n    uploads.\n\n    Args:\n        settings: mapping with storage configuration\n\n    \"\"\"\n    if isinstance(settings, Settings):\n        return settings\n\n    return cls.SettingsFactory.from_dict(settings)\n</code></pre>"},{"location":"api/#file_keeper.Storage.content","title":"<code>content(data, /, **kwargs)</code>","text":"<p>Return file content as a single byte object.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.STREAM)\ndef content(self, data: data.FileData, /, **kwargs: Any) -&gt; bytes:\n    \"\"\"Return file content as a single byte object.\n\n    Args:\n        data: The FileData object representing the file to read.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return self.reader.content(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.copy","title":"<code>copy(location, data, /, **kwargs)</code>","text":"<p>Copy file inside the storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the copied file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to copy.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.COPY)\ndef copy(self, location: types.Location, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Copy file inside the storage.\n\n    Args:\n        location: The destination location for the copied file.\n        data: The FileData object representing the file to copy.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return self.manager.copy(location, data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.copy_synthetic","title":"<code>copy_synthetic(location, data, dest_storage, /, **kwargs)</code>","text":"<p>Generic implementation of the copy operation that relies on CREATE.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the copied file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to copy.</p> <p> TYPE: <code>FileData</code> </p> <code>dest_storage</code> <p>The destination storage</p> <p> TYPE: <code>Storage</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def copy_synthetic(\n    self, location: types.Location, data: data.FileData, dest_storage: Storage, /, **kwargs: Any\n) -&gt; data.FileData:\n    \"\"\"Generic implementation of the copy operation that relies on [CREATE][file_keeper.Capability.CREATE].\n\n    Args:\n        location: The destination location for the copied file.\n        data: The FileData object representing the file to copy.\n        dest_storage: The destination storage\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return dest_storage.upload(\n        location,\n        self.file_as_upload(data, **kwargs),\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#file_keeper.Storage.exists","title":"<code>exists(data, /, **kwargs)</code>","text":"<p>Check if file exists in the storage.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to check.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.EXISTS)\ndef exists(self, data: data.FileData, /, **kwargs: Any) -&gt; bool:\n    \"\"\"Check if file exists in the storage.\n\n    Args:\n        data: The FileData object representing the file to check.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return self.manager.exists(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.file_as_upload","title":"<code>file_as_upload(data, **kwargs)</code>","text":"<p>Make an Upload with file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object to wrap into Upload</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def file_as_upload(self, data: data.FileData, **kwargs: Any) -&gt; Upload:\n    \"\"\"Make an [Upload][file_keeper.Upload] with file content.\n\n    Args:\n        data: The FileData object to wrap into Upload\n        **kwargs: Additional metadata for the upload.\n    \"\"\"\n    stream = self.stream(data, **kwargs)\n    stream = cast(types.PStream, stream) if hasattr(stream, \"read\") else utils.IterableBytesReader(stream)\n\n    return Upload(\n        stream,\n        data.location,\n        data.size,\n        data.content_type,\n    )\n</code></pre>"},{"location":"api/#file_keeper.Storage.full_path","title":"<code>full_path(location, /, **kwargs)</code>","text":"<p>Compute path to the file from the storage's root.</p> PARAMETER DESCRIPTION <code>location</code> <p>location of the file object</p> <p> TYPE: <code>Location</code> </p> <code>**kwargs</code> <p>exra parameters for custom storages</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>full path required to access location</p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def full_path(self, location: types.Location, /, **kwargs: Any) -&gt; str:\n    \"\"\"Compute path to the file from the storage's root.\n\n    Args:\n        location: location of the file object\n        **kwargs: exra parameters for custom storages\n\n    Returns:\n        full path required to access location\n    \"\"\"\n    return os.path.join(self.settings.path, location)\n</code></pre>"},{"location":"api/#file_keeper.Storage.make_manager","title":"<code>make_manager()</code>","text":"<p>Initialize manager service.</p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def make_manager(self):\n    \"\"\"Initialize [manager][file_keeper.Manager] service.\"\"\"\n    return self.ManagerFactory(self)\n</code></pre>"},{"location":"api/#file_keeper.Storage.make_reader","title":"<code>make_reader()</code>","text":"<p>Initialize reader service.</p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def make_reader(self):\n    \"\"\"Initialize [reader][file_keeper.Reader] service.\"\"\"\n    return self.ReaderFactory(self)\n</code></pre>"},{"location":"api/#file_keeper.Storage.make_uploader","title":"<code>make_uploader()</code>","text":"<p>Initialize uploader service.</p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def make_uploader(self):\n    \"\"\"Initialize [uploader][file_keeper.Uploader] service.\"\"\"\n    return self.UploaderFactory(self)\n</code></pre>"},{"location":"api/#file_keeper.Storage.move","title":"<code>move(location, data, /, **kwargs)</code>","text":"<p>Move file to a different location inside the storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the moved file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to move.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.MOVE)\ndef move(self, location: types.Location, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Move file to a different location inside the storage.\n\n    Args:\n        location: The destination location for the moved file.\n        data: The FileData object representing the file to move.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return self.manager.move(location, data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.move_synthetic","title":"<code>move_synthetic(location, data, dest_storage, /, **kwargs)</code>","text":"<p>Generic implementation of move operation.</p> <p>Relies on CREATE and REMOVE.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the moved file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to move.</p> <p> TYPE: <code>FileData</code> </p> <code>dest_storage</code> <p>The destination storage</p> <p> TYPE: <code>Storage</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def move_synthetic(\n    self, location: types.Location, data: data.FileData, dest_storage: Storage, /, **kwargs: Any\n) -&gt; data.FileData:\n    \"\"\"Generic implementation of move operation.\n\n    Relies on [CREATE][file_keeper.Capability.CREATE] and\n    [REMOVE][file_keeper.Capability.REMOVE].\n\n    Args:\n        location: The destination location for the moved file.\n        data: The FileData object representing the file to move.\n        dest_storage: The destination storage\n        **kwargs: Additional metadata for the operation.\n\n    \"\"\"\n    result = dest_storage.upload(location, self.file_as_upload(data, **kwargs), **kwargs)\n    self.remove(data)\n    return result\n</code></pre>"},{"location":"api/#file_keeper.Storage.multipart_complete","title":"<code>multipart_complete(data, /, **kwargs)</code>","text":"<p>Verify file integrity and finalize incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.MULTIPART)\ndef multipart_complete(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Verify file integrity and finalize incomplete upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        **kwargs: Additional metadata for the upload.\n    \"\"\"\n    return self.uploader.multipart_complete(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.multipart_refresh","title":"<code>multipart_refresh(data, /, **kwargs)</code>","text":"<p>Show details of the incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.MULTIPART)\ndef multipart_refresh(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Show details of the incomplete upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        **kwargs: Additional metadata for the upload.\n    \"\"\"\n    return self.uploader.multipart_refresh(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.multipart_remove","title":"<code>multipart_remove(data, /, **kwargs)</code>","text":"<p>Interrupt and remove incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.MULTIPART)\ndef multipart_remove(self, data: data.FileData, /, **kwargs: Any) -&gt; bool:\n    \"\"\"Interrupt and remove incomplete upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        **kwargs: Additional metadata for the upload.\n    \"\"\"\n    return self.uploader.multipart_remove(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.multipart_start","title":"<code>multipart_start(data, /, **kwargs)</code>","text":"<p>Prepare everything for multipart upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.MULTIPART)\ndef multipart_start(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Prepare everything for multipart upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        **kwargs: Additional metadata for the upload.\n    \"\"\"\n    return self.uploader.multipart_start(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.multipart_update","title":"<code>multipart_update(data, /, **kwargs)</code>","text":"<p>Add data to the incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.MULTIPART)\ndef multipart_update(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Add data to the incomplete upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        **kwargs: Additional metadata for the upload.\n    \"\"\"\n    return self.uploader.multipart_update(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.one_time_link","title":"<code>one_time_link(data, /, **kwargs)</code>","text":"<p>Return one-time download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def one_time_link(self, data: data.FileData, /, **kwargs: Any) -&gt; str | None:\n    \"\"\"Return one-time download link.\n\n    Args:\n        data: The FileData object representing the file.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    if self.supports(Capability.LINK_ONE_TIME):\n        return self.reader.one_time_link(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.permanent_link","title":"<code>permanent_link(data, /, **kwargs)</code>","text":"<p>Return permanent download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def permanent_link(self, data: data.FileData, /, **kwargs: Any) -&gt; str | None:\n    \"\"\"Return permanent download link.\n\n    Args:\n        data: The FileData object representing the file.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    if self.supports(Capability.LINK_PERMANENT):\n        return self.reader.permanent_link(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.prepare_location","title":"<code>prepare_location(location, upload_or_data=None, /, **kwargs)</code>","text":"<p>Transform and sanitize location using configured functions.</p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def prepare_location(\n    self, location: str, upload_or_data: data.BaseData | Upload | None = None, /, **kwargs: Any\n) -&gt; types.Location:\n    \"\"\"Transform and sanitize location using configured functions.\"\"\"\n    for name in self.settings.location_transformers:\n        if transformer := location_transformers.get(name):\n            location = transformer(location, upload_or_data, kwargs)\n        else:\n            raise exceptions.LocationTransformerError(name)\n\n    return types.Location(location)\n</code></pre>"},{"location":"api/#file_keeper.Storage.range","title":"<code>range(data, start=0, end=None, /, **kwargs)</code>","text":"<p>Return slice of the file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>start</code> <p>The starting byte offset.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>end</code> <p>The ending byte offset (inclusive).</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.RANGE)\ndef range(self, data: data.FileData, start: int = 0, end: int | None = None, /, **kwargs: Any) -&gt; Iterable[bytes]:\n    \"\"\"Return slice of the file content.\n\n    Args:\n        data: The FileData object representing the file to read.\n        start: The starting byte offset.\n        end: The ending byte offset (inclusive).\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return self.reader.range(data, start, end, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.range_synthetic","title":"<code>range_synthetic(data, start=0, end=None, /, **kwargs)</code>","text":"<p>Generic implementation of range operation that relies on STREAM.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>start</code> <p>The starting byte offset.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>end</code> <p>The ending byte offset (inclusive).</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def range_synthetic(\n    self, data: data.FileData, start: int = 0, end: int | None = None, /, **kwargs: Any\n) -&gt; Iterable[bytes]:\n    \"\"\"Generic implementation of range operation that relies on [STREAM][file_keeper.Capability.STREAM].\n\n    Args:\n        data: The FileData object representing the file to read.\n        start: The starting byte offset.\n        end: The ending byte offset (inclusive).\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    if end is None:\n        end = cast(int, float(\"inf\"))\n\n    end -= start\n    if end &lt;= 0:\n        return\n\n    for chunk in self.stream(data, **kwargs):\n        if start &gt; 0:\n            start -= len(chunk)\n            if start &lt; 0:\n                chunk = chunk[start:]  # noqa: PLW2901\n            else:\n                continue\n\n        yield chunk[: end and None]\n        end -= len(chunk)\n        if end &lt;= 0:\n            break\n</code></pre>"},{"location":"api/#file_keeper.Storage.remove","title":"<code>remove(data, /, **kwargs)</code>","text":"<p>Remove file from the storage.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to remove.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.REMOVE)\ndef remove(self, data: data.FileData, /, **kwargs: Any) -&gt; bool:\n    \"\"\"Remove file from the storage.\n\n    Args:\n        data: The FileData object representing the file to remove.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return self.manager.remove(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.resumable_refresh","title":"<code>resumable_refresh(data, /, **kwargs)</code>","text":"<p>Show details of the incomplete resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.RESUMABLE)\ndef resumable_refresh(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Show details of the incomplete resumable upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        **kwargs: Additional metadata for the upload.\n    \"\"\"\n    return self.uploader.resumable_refresh(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.resumable_remove","title":"<code>resumable_remove(data, /, **kwargs)</code>","text":"<p>Remove incomplete resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.RESUMABLE)\ndef resumable_remove(self, data: data.FileData, /, **kwargs: Any) -&gt; bool:\n    \"\"\"Remove incomplete resumable upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        **kwargs: Additional metadata for the upload.\n    \"\"\"\n    return self.uploader.resumable_remove(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.resumable_resume","title":"<code>resumable_resume(data, /, **kwargs)</code>","text":"<p>Resume the interrupted resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.RESUMABLE)\ndef resumable_resume(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Resume the interrupted resumable upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        **kwargs: Additional metadata for the upload.\n    \"\"\"\n    return self.uploader.resumable_resume(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.resumable_start","title":"<code>resumable_start(data, /, **kwargs)</code>","text":"<p>Prepare everything for resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.RESUMABLE)\ndef resumable_start(self, data: data.FileData, /, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Prepare everything for resumable upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        **kwargs: Additional metadata for the upload.\n    \"\"\"\n    return self.uploader.resumable_start(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.scan","title":"<code>scan(**kwargs)</code>","text":"<p>List all locations(filenames) in storage.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.SCAN)\ndef scan(self, **kwargs: Any) -&gt; Iterable[str]:\n    \"\"\"List all locations(filenames) in storage.\n\n    Args:\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return self.manager.scan(kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.signed","title":"<code>signed(action, duration, location, **kwargs)</code>","text":"<p>Make an URL for signed action.</p> PARAMETER DESCRIPTION <code>action</code> <p>The action to sign (e.g., \"upload\", \"download\").</p> <p> TYPE: <code>SignedAction</code> </p> <code>duration</code> <p>The duration for which the signed URL is valid.</p> <p> TYPE: <code>int</code> </p> <code>location</code> <p>The location of the file to sign.</p> <p> TYPE: <code>Location</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.SIGNED)\ndef signed(\n    self,\n    action: types.SignedAction,\n    duration: int,\n    location: types.Location,\n    **kwargs: Any,\n) -&gt; str:\n    \"\"\"Make an URL for signed action.\n\n    Args:\n        action: The action to sign (e.g., \"upload\", \"download\").\n        duration: The duration for which the signed URL is valid.\n        location: The location of the file to sign.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return self.manager.signed(action, duration, location, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.stream","title":"<code>stream(data, /, **kwargs)</code>","text":"<p>Return byte-stream of the file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to stream.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.STREAM)\ndef stream(self, data: data.FileData, /, **kwargs: Any) -&gt; Iterable[bytes]:\n    \"\"\"Return byte-stream of the file content.\n\n    Args:\n        data: The FileData object representing the file to stream.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    return self.reader.stream(data, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.supports","title":"<code>supports(operation)</code>","text":"<p>Check whether the storage supports operation.</p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def supports(self, operation: Capability) -&gt; bool:\n    \"\"\"Check whether the storage supports operation.\"\"\"\n    return self.capabilities.can(operation)\n</code></pre>"},{"location":"api/#file_keeper.Storage.supports_synthetic","title":"<code>supports_synthetic(operation, dest)</code>","text":"<p>Check if the storage can emulate operation using other operations.</p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def supports_synthetic(self, operation: Capability, dest: Storage) -&gt; bool:\n    \"\"\"Check if the storage can emulate operation using other operations.\"\"\"\n    if operation is Capability.RANGE:\n        return self.supports(Capability.STREAM)\n\n    if operation is Capability.COPY:\n        return self.supports(Capability.STREAM) and dest.supports(\n            Capability.CREATE,\n        )\n\n    if operation is Capability.MOVE:\n        return self.supports(\n            Capability.STREAM | Capability.REMOVE,\n        ) and dest.supports(Capability.CREATE)\n\n    if operation is Capability.COMPOSE:\n        return self.supports(Capability.STREAM) and dest.supports(\n            Capability.CREATE | Capability.APPEND | Capability.REMOVE\n        )\n\n    return False\n</code></pre>"},{"location":"api/#file_keeper.Storage.temporal_link","title":"<code>temporal_link(data, duration, /, **kwargs)</code>","text":"<p>Return temporal download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>duration</code> <p>The duration for which the link is valid.</p> <p> TYPE: <code>int</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def temporal_link(self, data: data.FileData, duration: int, /, **kwargs: Any) -&gt; str | None:\n    \"\"\"Return temporal download link.\n\n    Args:\n        data: The FileData object representing the file.\n        duration: The duration for which the link is valid.\n        **kwargs: Additional metadata for the operation.\n    \"\"\"\n    if self.supports(Capability.LINK_TEMPORAL):\n        return self.reader.temporal_link(data, duration, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Storage.upload","title":"<code>upload(location, upload, /, **kwargs)</code>","text":"<p>Upload file using single stream.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the upload.</p> <p> TYPE: <code>Location</code> </p> <code>upload</code> <p>The Upload object containing the file data.</p> <p> TYPE: <code>Upload</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@requires_capability(Capability.CREATE)\ndef upload(self, location: types.Location, upload: Upload, /, **kwargs: Any) -&gt; data.FileData:\n    \"\"\"Upload file using single stream.\n\n    Args:\n        location: The destination location for the upload.\n        upload: The Upload object containing the file data.\n        **kwargs: Additional metadata for the upload.\n    \"\"\"\n    return self.uploader.upload(location, upload, kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Capability","title":"<code>Capability</code>","text":"<p>               Bases: <code>Flag</code></p> <p>Enumeration of operations supported by the storage.</p> Example <pre><code>read_and_write = Capability.STREAM | Capability.CREATE\nif storage.supports(read_and_write)\n    ...\n</code></pre> Source code in <code>src/file_keeper/core/utils.py</code> <pre><code>class Capability(enum.Flag):\n    \"\"\"Enumeration of operations supported by the storage.\n\n    Example:\n        ```python\n        read_and_write = Capability.STREAM | Capability.CREATE\n        if storage.supports(read_and_write)\n            ...\n        ```\n    \"\"\"\n\n    NONE = 0\n\n    ANALYZE = enum.auto()\n    \"\"\"Return file details from the storage.\"\"\"\n\n    APPEND = enum.auto()\n    \"\"\"Add content to the existing file.\"\"\"\n\n    COMPOSE = enum.auto()\n    \"\"\"Combine multiple files into a new one in the same storage.\"\"\"\n\n    COPY = enum.auto()\n    \"\"\"Make a copy of the file inside the same storage.\"\"\"\n\n    CREATE = enum.auto()\n    \"\"\"Create a file as an atomic object.\"\"\"\n\n    EXISTS = enum.auto()\n    \"\"\"Check if file exists.\"\"\"\n\n    LINK_PERMANENT = enum.auto()\n    \"\"\"Make permanent download link.\"\"\"\n\n    LINK_TEMPORAL = enum.auto()\n    \"\"\"Make expiring download link.\"\"\"\n\n    LINK_ONE_TIME = enum.auto()\n    \"\"\"Make one-time download link.\"\"\"\n\n    MOVE = enum.auto()\n    \"\"\"Move file to a different location inside the same storage.\"\"\"\n\n    MULTIPART = enum.auto()\n    \"\"\"Create file in 3 stages: initialize, upload(repeatable), complete.\"\"\"\n\n    RANGE = enum.auto()\n    \"\"\"Return specific range of bytes from the file.\"\"\"\n\n    REMOVE = enum.auto()\n    \"\"\"Remove file from the storage.\"\"\"\n\n    RESUMABLE = enum.auto()\n    \"\"\"Perform resumable uploads that can be continued after interruption.\"\"\"\n\n    SCAN = enum.auto()\n    \"\"\"Iterate over all files in the storage.\"\"\"\n\n    SIGNED = enum.auto()\n    \"\"\"Generate signed URL for specific operation.\"\"\"\n\n    STREAM = enum.auto()\n    \"\"\"Return file content as stream of bytes.\"\"\"\n\n    MANAGER_CAPABILITIES = ANALYZE | SCAN | COPY | MOVE | APPEND | COMPOSE | EXISTS | REMOVE | SIGNED\n    READER_CAPABILITIES = RANGE | STREAM | LINK_PERMANENT | LINK_TEMPORAL | LINK_ONE_TIME\n    UPLOADER_CAPABILITIES = CREATE | MULTIPART | RESUMABLE\n\n    def exclude(self, *capabilities: Capability):\n        \"\"\"Remove capabilities from the cluster.\n\n        Other Args:\n            capabilities: removed capabilities\n\n        Example:\n            ```python\n            cluster = cluster.exclude(Capability.REMOVE)\n            ```\n        \"\"\"\n        result = Capability(self)\n        for capability in capabilities:\n            result = result &amp; ~capability\n        return result\n\n    def can(self, operation: Capability) -&gt; bool:\n        \"\"\"Check whether the cluster supports given operation.\"\"\"\n        return (self &amp; operation) == operation\n</code></pre>"},{"location":"api/#file_keeper.Capability.ANALYZE","title":"<code>ANALYZE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Return file details from the storage.</p>"},{"location":"api/#file_keeper.Capability.APPEND","title":"<code>APPEND = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Add content to the existing file.</p>"},{"location":"api/#file_keeper.Capability.COMPOSE","title":"<code>COMPOSE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Combine multiple files into a new one in the same storage.</p>"},{"location":"api/#file_keeper.Capability.COPY","title":"<code>COPY = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Make a copy of the file inside the same storage.</p>"},{"location":"api/#file_keeper.Capability.CREATE","title":"<code>CREATE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Create a file as an atomic object.</p>"},{"location":"api/#file_keeper.Capability.EXISTS","title":"<code>EXISTS = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Check if file exists.</p>"},{"location":"api/#file_keeper.Capability.LINK_ONE_TIME","title":"<code>LINK_ONE_TIME = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Make one-time download link.</p>"},{"location":"api/#file_keeper.Capability.LINK_PERMANENT","title":"<code>LINK_PERMANENT = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Make permanent download link.</p>"},{"location":"api/#file_keeper.Capability.LINK_TEMPORAL","title":"<code>LINK_TEMPORAL = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Make expiring download link.</p>"},{"location":"api/#file_keeper.Capability.MOVE","title":"<code>MOVE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Move file to a different location inside the same storage.</p>"},{"location":"api/#file_keeper.Capability.MULTIPART","title":"<code>MULTIPART = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Create file in 3 stages: initialize, upload(repeatable), complete.</p>"},{"location":"api/#file_keeper.Capability.RANGE","title":"<code>RANGE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Return specific range of bytes from the file.</p>"},{"location":"api/#file_keeper.Capability.REMOVE","title":"<code>REMOVE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Remove file from the storage.</p>"},{"location":"api/#file_keeper.Capability.RESUMABLE","title":"<code>RESUMABLE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Perform resumable uploads that can be continued after interruption.</p>"},{"location":"api/#file_keeper.Capability.SCAN","title":"<code>SCAN = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Iterate over all files in the storage.</p>"},{"location":"api/#file_keeper.Capability.SIGNED","title":"<code>SIGNED = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Generate signed URL for specific operation.</p>"},{"location":"api/#file_keeper.Capability.STREAM","title":"<code>STREAM = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Return file content as stream of bytes.</p>"},{"location":"api/#file_keeper.Capability.can","title":"<code>can(operation)</code>","text":"<p>Check whether the cluster supports given operation.</p> Source code in <code>src/file_keeper/core/utils.py</code> <pre><code>def can(self, operation: Capability) -&gt; bool:\n    \"\"\"Check whether the cluster supports given operation.\"\"\"\n    return (self &amp; operation) == operation\n</code></pre>"},{"location":"api/#file_keeper.Capability.exclude","title":"<code>exclude(*capabilities)</code>","text":"<p>Remove capabilities from the cluster.</p> PARAMETER DESCRIPTION <code>capabilities</code> <p>removed capabilities</p> <p> TYPE: <code>Capability</code> </p> Example <pre><code>cluster = cluster.exclude(Capability.REMOVE)\n</code></pre> Source code in <code>src/file_keeper/core/utils.py</code> <pre><code>def exclude(self, *capabilities: Capability):\n    \"\"\"Remove capabilities from the cluster.\n\n    Other Args:\n        capabilities: removed capabilities\n\n    Example:\n        ```python\n        cluster = cluster.exclude(Capability.REMOVE)\n        ```\n    \"\"\"\n    result = Capability(self)\n    for capability in capabilities:\n        result = result &amp; ~capability\n    return result\n</code></pre>"},{"location":"api/#file_keeper.Settings","title":"<code>Settings</code>  <code>dataclass</code>","text":"<p>Settings for the storage adapter.</p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@dataclasses.dataclass()\nclass Settings:\n    \"\"\"Settings for the storage adapter.\"\"\"\n\n    name: str = \"unknown\"\n    \"\"\"Descriptive name of the storage used for debugging.\"\"\"\n\n    override_existing: bool = False\n    \"\"\"If file already exists, replace it with new content.\"\"\"\n\n    path: str = \"\"\n    \"\"\"Prefix for the file's location.\"\"\"\n\n    location_transformers: list[str] = cast(\"list[str]\", dataclasses.field(default_factory=list))\n    \"\"\"List of transformations applied to the file location.\"\"\"\n\n    disabled_capabilities: list[str] = cast(\"list[str]\", dataclasses.field(default_factory=list))\n    \"\"\"Capabilities that are not supported even if implemented.\"\"\"\n\n    initialize: bool = False\n    \"\"\"Prepare storage backend for uploads(create path, bucket, DB)\"\"\"\n\n    _required_options: ClassVar[list[str]] = []\n    _extra_settings: dict[str, Any] = cast(\"dict[str, Any]\", dataclasses.field(default_factory=dict))\n\n    def __post_init__(self, **kwargs: Any):\n        for attr in self._required_options:\n            if not getattr(self, attr):\n                raise exceptions.MissingStorageConfigurationError(self.name, attr)\n\n    @classmethod\n    def from_dict(cls, data: Mapping[str, Any]) -&gt; Settings:\n        \"\"\"Make settings object using dictionary as a source.\n\n        Any unexpected options are extracted from the `data` to avoid\n        initialization errors from dataclass constructor.\n\n        Args:\n            data: mapping with settings\n\n        Returns:\n            settings object built from data\n\n        \"\"\"\n        # try:\n        #     return cls(**settings)\n        # except TypeError as err:\n        #     raise exceptions.InvalidStorageConfigurationError(\n        #         settings.get(\"name\") or cls, str(err)\n        #     ) from err\n\n        sig = inspect.signature(cls)\n        names = set(sig.parameters)\n\n        valid = {}\n        invalid = {}\n        for k, v in data.items():\n            if k in names:\n                valid[k] = v\n            else:\n                invalid[k] = v\n\n        valid.setdefault(\"_extra_settings\", {}).update(invalid)\n        cfg = cls(**valid)\n        if invalid:\n            log.warning(\n                \"Storage %s received unknow settings: %s\",\n                cfg.name,\n                invalid,\n            )\n        return cfg\n</code></pre>"},{"location":"api/#file_keeper.Settings.disabled_capabilities","title":"<code>disabled_capabilities = cast('list[str]', dataclasses.field(default_factory=list))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Capabilities that are not supported even if implemented.</p>"},{"location":"api/#file_keeper.Settings.initialize","title":"<code>initialize = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prepare storage backend for uploads(create path, bucket, DB)</p>"},{"location":"api/#file_keeper.Settings.location_transformers","title":"<code>location_transformers = cast('list[str]', dataclasses.field(default_factory=list))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of transformations applied to the file location.</p>"},{"location":"api/#file_keeper.Settings.name","title":"<code>name = 'unknown'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Descriptive name of the storage used for debugging.</p>"},{"location":"api/#file_keeper.Settings.override_existing","title":"<code>override_existing = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If file already exists, replace it with new content.</p>"},{"location":"api/#file_keeper.Settings.path","title":"<code>path = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prefix for the file's location.</p>"},{"location":"api/#file_keeper.Settings.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Make settings object using dictionary as a source.</p> <p>Any unexpected options are extracted from the <code>data</code> to avoid initialization errors from dataclass constructor.</p> PARAMETER DESCRIPTION <code>data</code> <p>mapping with settings</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Settings</code> <p>settings object built from data</p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Mapping[str, Any]) -&gt; Settings:\n    \"\"\"Make settings object using dictionary as a source.\n\n    Any unexpected options are extracted from the `data` to avoid\n    initialization errors from dataclass constructor.\n\n    Args:\n        data: mapping with settings\n\n    Returns:\n        settings object built from data\n\n    \"\"\"\n    # try:\n    #     return cls(**settings)\n    # except TypeError as err:\n    #     raise exceptions.InvalidStorageConfigurationError(\n    #         settings.get(\"name\") or cls, str(err)\n    #     ) from err\n\n    sig = inspect.signature(cls)\n    names = set(sig.parameters)\n\n    valid = {}\n    invalid = {}\n    for k, v in data.items():\n        if k in names:\n            valid[k] = v\n        else:\n            invalid[k] = v\n\n    valid.setdefault(\"_extra_settings\", {}).update(invalid)\n    cfg = cls(**valid)\n    if invalid:\n        log.warning(\n            \"Storage %s received unknow settings: %s\",\n            cfg.name,\n            invalid,\n        )\n    return cfg\n</code></pre>"},{"location":"api/#file_keeper.Uploader","title":"<code>Uploader</code>","text":"<p>               Bases: <code>StorageService</code></p> <p>Service responsible for writing data into a storage.</p> <p><code>Storage</code> internally calls methods of this service. For example, <code>Storage.upload(location, upload, **kwargs)</code> results in <code>Uploader.upload(location, upload, kwargs)</code>.</p> Example <pre><code>class MyUploader(Uploader):\n    capabilities = Capability.CREATE\n\n    def upload(\n        self, location: types.Location, upload: Upload, extras: dict[str, Any]\n    ) -&gt; FileData:\n        reader = upload.hashing_reader()\n\n        with open(location, \"wb\") as dest:\n            dest.write(reader.read())\n\n        return FileData(\n            location, upload.size,\n            upload.content_type,\n            reader.get_hash()\n        )\n</code></pre> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>class Uploader(StorageService):\n    \"\"\"Service responsible for writing data into a storage.\n\n    `Storage` internally calls methods of this service. For example,\n    `Storage.upload(location, upload, **kwargs)` results in\n    `Uploader.upload(location, upload, kwargs)`.\n\n    Example:\n        ```python\n        class MyUploader(Uploader):\n            capabilities = Capability.CREATE\n\n            def upload(\n                self, location: types.Location, upload: Upload, extras: dict[str, Any]\n            ) -&gt; FileData:\n                reader = upload.hashing_reader()\n\n                with open(location, \"wb\") as dest:\n                    dest.write(reader.read())\n\n                return FileData(\n                    location, upload.size,\n                    upload.content_type,\n                    reader.get_hash()\n                )\n        ```\n    \"\"\"\n\n    def upload(self, location: types.Location, upload: Upload, extras: dict[str, Any]) -&gt; data.FileData:\n        \"\"\"Upload file using single stream.\n\n        Args:\n            location: The destination location for the upload.\n            upload: The Upload object containing the file data.\n            extras: Additional metadata for the upload.\n        \"\"\"\n        raise NotImplementedError\n\n    def resumable_start(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n        \"\"\"Prepare everything for resumable upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            extras: Additional metadata for the upload.\n        \"\"\"\n        raise NotImplementedError\n\n    def resumable_refresh(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n        \"\"\"Show details of the incomplete resumable upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            extras: Additional metadata for the upload.\n        \"\"\"\n        raise NotImplementedError\n\n    def resumable_resume(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n        \"\"\"Resume the interrupted resumable upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            extras: Additional metadata for the upload.\n        \"\"\"\n        raise NotImplementedError\n\n    def resumable_remove(self, data: data.FileData, extras: dict[str, Any]) -&gt; bool:\n        \"\"\"Remove incomplete resumable upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            extras: Additional metadata for the upload.\n        \"\"\"\n        raise NotImplementedError\n\n    def multipart_start(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n        \"\"\"Prepare everything for multipart(resumable) upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            extras: Additional metadata for the upload.\n        \"\"\"\n        raise NotImplementedError\n\n    def multipart_refresh(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n        \"\"\"Show details of the incomplete upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            extras: Additional metadata for the upload.\n        \"\"\"\n        raise NotImplementedError\n\n    def multipart_update(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n        \"\"\"Add data to the incomplete upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            extras: Additional metadata for the upload.\n        \"\"\"\n        raise NotImplementedError\n\n    def multipart_complete(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n        \"\"\"Verify file integrity and finalize incomplete upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            extras: Additional metadata for the upload.\n        \"\"\"\n        raise NotImplementedError\n\n    def multipart_remove(self, data: data.FileData, extras: dict[str, Any]) -&gt; bool:\n        \"\"\"Interrupt and remove incomplete upload.\n\n        Args:\n            data: The FileData object containing the upload metadata.\n            extras: Additional metadata for the upload.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Uploader.multipart_complete","title":"<code>multipart_complete(data, extras)</code>","text":"<p>Verify file integrity and finalize incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def multipart_complete(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n    \"\"\"Verify file integrity and finalize incomplete upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        extras: Additional metadata for the upload.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Uploader.multipart_refresh","title":"<code>multipart_refresh(data, extras)</code>","text":"<p>Show details of the incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def multipart_refresh(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n    \"\"\"Show details of the incomplete upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        extras: Additional metadata for the upload.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Uploader.multipart_remove","title":"<code>multipart_remove(data, extras)</code>","text":"<p>Interrupt and remove incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def multipart_remove(self, data: data.FileData, extras: dict[str, Any]) -&gt; bool:\n    \"\"\"Interrupt and remove incomplete upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        extras: Additional metadata for the upload.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Uploader.multipart_start","title":"<code>multipart_start(data, extras)</code>","text":"<p>Prepare everything for multipart(resumable) upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def multipart_start(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n    \"\"\"Prepare everything for multipart(resumable) upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        extras: Additional metadata for the upload.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Uploader.multipart_update","title":"<code>multipart_update(data, extras)</code>","text":"<p>Add data to the incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def multipart_update(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n    \"\"\"Add data to the incomplete upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        extras: Additional metadata for the upload.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Uploader.resumable_refresh","title":"<code>resumable_refresh(data, extras)</code>","text":"<p>Show details of the incomplete resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def resumable_refresh(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n    \"\"\"Show details of the incomplete resumable upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        extras: Additional metadata for the upload.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Uploader.resumable_remove","title":"<code>resumable_remove(data, extras)</code>","text":"<p>Remove incomplete resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def resumable_remove(self, data: data.FileData, extras: dict[str, Any]) -&gt; bool:\n    \"\"\"Remove incomplete resumable upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        extras: Additional metadata for the upload.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Uploader.resumable_resume","title":"<code>resumable_resume(data, extras)</code>","text":"<p>Resume the interrupted resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def resumable_resume(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n    \"\"\"Resume the interrupted resumable upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        extras: Additional metadata for the upload.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Uploader.resumable_start","title":"<code>resumable_start(data, extras)</code>","text":"<p>Prepare everything for resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def resumable_start(self, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n    \"\"\"Prepare everything for resumable upload.\n\n    Args:\n        data: The FileData object containing the upload metadata.\n        extras: Additional metadata for the upload.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Uploader.upload","title":"<code>upload(location, upload, extras)</code>","text":"<p>Upload file using single stream.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the upload.</p> <p> TYPE: <code>Location</code> </p> <code>upload</code> <p>The Upload object containing the file data.</p> <p> TYPE: <code>Upload</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def upload(self, location: types.Location, upload: Upload, extras: dict[str, Any]) -&gt; data.FileData:\n    \"\"\"Upload file using single stream.\n\n    Args:\n        location: The destination location for the upload.\n        upload: The Upload object containing the file data.\n        extras: Additional metadata for the upload.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Manager","title":"<code>Manager</code>","text":"<p>               Bases: <code>StorageService</code></p> <p>Service responsible for maintenance file operations.</p> <p><code>Storage</code> internally calls methods of this service. For example, <code>Storage.remove(data, **kwargs)</code> results in <code>Manager.remove(data, kwargs)</code>.</p> Example <pre><code>class MyManager(Manager):\n    capabilities = Capability.REMOVE\n    def remove(\n        self, data: FileData|FileData, extras: dict[str, Any]\n    ) -&gt; bool:\n        os.remove(data.location)\n        return True\n</code></pre> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>class Manager(StorageService):\n    \"\"\"Service responsible for maintenance file operations.\n\n    `Storage` internally calls methods of this service. For example,\n    `Storage.remove(data, **kwargs)` results in `Manager.remove(data, kwargs)`.\n\n    Example:\n        ```python\n        class MyManager(Manager):\n            capabilities = Capability.REMOVE\n            def remove(\n                self, data: FileData|FileData, extras: dict[str, Any]\n            ) -&gt; bool:\n                os.remove(data.location)\n                return True\n        ```\n    \"\"\"\n\n    def remove(self, data: data.FileData, extras: dict[str, Any]) -&gt; bool:\n        \"\"\"Remove file from the storage.\n\n        Args:\n            data: The FileData object representing the file to remove.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n\n    def exists(self, data: data.FileData, extras: dict[str, Any]) -&gt; bool:\n        \"\"\"Check if file exists in the storage.\n\n        Args:\n            data: The FileData object representing the file to check.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n\n    def compose(\n        self, location: types.Location, datas: Iterable[data.FileData], extras: dict[str, Any]\n    ) -&gt; data.FileData:\n        \"\"\"Combine multipe file inside the storage into a new one.\n\n        Args:\n            location: The destination location for the composed file.\n            datas: An iterable of FileData objects representing the files to combine.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n\n    def append(self, data: data.FileData, upload: Upload, extras: dict[str, Any]) -&gt; data.FileData:\n        \"\"\"Append content to existing file.\n\n        Args:\n            data: The FileData object representing the file to append to.\n            upload: The Upload object containing the content to append.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n\n    def copy(self, location: types.Location, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n        \"\"\"Copy file inside the storage.\n\n        Args:\n            location: The destination location for the copied file.\n            data: The FileData object representing the file to copy.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n\n    def move(self, location: types.Location, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n        \"\"\"Move file to a different location inside the storage.\n\n        Args:\n            location: The destination location for the moved file.\n            data: The FileData object representing the file to move.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n\n    def scan(self, extras: dict[str, Any]) -&gt; Iterable[str]:\n        \"\"\"List all locations(filenames) in storage.\n\n        Args:\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n\n    def analyze(self, location: types.Location, extras: dict[str, Any]) -&gt; data.FileData:\n        \"\"\"Return details about location.\n\n        Args:\n            location: The location of the file to analyze.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n\n    def signed(\n        self, action: types.SignedAction, duration: int, location: types.Location, extras: dict[str, Any]\n    ) -&gt; str:\n        \"\"\"Make an URL for signed action.\n\n        Args:\n            action: The action to sign (e.g., \"upload\", \"download\").\n            duration: The duration for which the signed URL is valid.\n            location: The location of the file to sign.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Manager.analyze","title":"<code>analyze(location, extras)</code>","text":"<p>Return details about location.</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the file to analyze.</p> <p> TYPE: <code>Location</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def analyze(self, location: types.Location, extras: dict[str, Any]) -&gt; data.FileData:\n    \"\"\"Return details about location.\n\n    Args:\n        location: The location of the file to analyze.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Manager.append","title":"<code>append(data, upload, extras)</code>","text":"<p>Append content to existing file.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to append to.</p> <p> TYPE: <code>FileData</code> </p> <code>upload</code> <p>The Upload object containing the content to append.</p> <p> TYPE: <code>Upload</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def append(self, data: data.FileData, upload: Upload, extras: dict[str, Any]) -&gt; data.FileData:\n    \"\"\"Append content to existing file.\n\n    Args:\n        data: The FileData object representing the file to append to.\n        upload: The Upload object containing the content to append.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Manager.compose","title":"<code>compose(location, datas, extras)</code>","text":"<p>Combine multipe file inside the storage into a new one.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the composed file.</p> <p> TYPE: <code>Location</code> </p> <code>datas</code> <p>An iterable of FileData objects representing the files to combine.</p> <p> TYPE: <code>Iterable[FileData]</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def compose(\n    self, location: types.Location, datas: Iterable[data.FileData], extras: dict[str, Any]\n) -&gt; data.FileData:\n    \"\"\"Combine multipe file inside the storage into a new one.\n\n    Args:\n        location: The destination location for the composed file.\n        datas: An iterable of FileData objects representing the files to combine.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Manager.copy","title":"<code>copy(location, data, extras)</code>","text":"<p>Copy file inside the storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the copied file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to copy.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def copy(self, location: types.Location, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n    \"\"\"Copy file inside the storage.\n\n    Args:\n        location: The destination location for the copied file.\n        data: The FileData object representing the file to copy.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Manager.exists","title":"<code>exists(data, extras)</code>","text":"<p>Check if file exists in the storage.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to check.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def exists(self, data: data.FileData, extras: dict[str, Any]) -&gt; bool:\n    \"\"\"Check if file exists in the storage.\n\n    Args:\n        data: The FileData object representing the file to check.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Manager.move","title":"<code>move(location, data, extras)</code>","text":"<p>Move file to a different location inside the storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the moved file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to move.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def move(self, location: types.Location, data: data.FileData, extras: dict[str, Any]) -&gt; data.FileData:\n    \"\"\"Move file to a different location inside the storage.\n\n    Args:\n        location: The destination location for the moved file.\n        data: The FileData object representing the file to move.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Manager.remove","title":"<code>remove(data, extras)</code>","text":"<p>Remove file from the storage.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to remove.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def remove(self, data: data.FileData, extras: dict[str, Any]) -&gt; bool:\n    \"\"\"Remove file from the storage.\n\n    Args:\n        data: The FileData object representing the file to remove.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Manager.scan","title":"<code>scan(extras)</code>","text":"<p>List all locations(filenames) in storage.</p> PARAMETER DESCRIPTION <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def scan(self, extras: dict[str, Any]) -&gt; Iterable[str]:\n    \"\"\"List all locations(filenames) in storage.\n\n    Args:\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Manager.signed","title":"<code>signed(action, duration, location, extras)</code>","text":"<p>Make an URL for signed action.</p> PARAMETER DESCRIPTION <code>action</code> <p>The action to sign (e.g., \"upload\", \"download\").</p> <p> TYPE: <code>SignedAction</code> </p> <code>duration</code> <p>The duration for which the signed URL is valid.</p> <p> TYPE: <code>int</code> </p> <code>location</code> <p>The location of the file to sign.</p> <p> TYPE: <code>Location</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def signed(\n    self, action: types.SignedAction, duration: int, location: types.Location, extras: dict[str, Any]\n) -&gt; str:\n    \"\"\"Make an URL for signed action.\n\n    Args:\n        action: The action to sign (e.g., \"upload\", \"download\").\n        duration: The duration for which the signed URL is valid.\n        location: The location of the file to sign.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Reader","title":"<code>Reader</code>","text":"<p>               Bases: <code>StorageService</code></p> <p>Service responsible for reading data from the storage.</p> <p><code>Storage</code> internally calls methods of this service. For example, <code>Storage.stream(data, **kwargs)</code> results in <code>Reader.stream(data, kwargs)</code>.</p> Example <pre><code>class MyReader(Reader):\n    capabilities = Capability.STREAM\n\n    def stream(\n        self, data: data.FileData, extras: dict[str, Any]\n    ) -&gt; Iterable[bytes]:\n        return open(data.location, \"rb\")\n</code></pre> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>class Reader(StorageService):\n    \"\"\"Service responsible for reading data from the storage.\n\n    `Storage` internally calls methods of this service. For example,\n    `Storage.stream(data, **kwargs)` results in `Reader.stream(data, kwargs)`.\n\n    Example:\n        ```python\n        class MyReader(Reader):\n            capabilities = Capability.STREAM\n\n            def stream(\n                self, data: data.FileData, extras: dict[str, Any]\n            ) -&gt; Iterable[bytes]:\n                return open(data.location, \"rb\")\n        ```\n    \"\"\"\n\n    def stream(self, data: data.FileData, extras: dict[str, Any]) -&gt; Iterable[bytes]:\n        \"\"\"Return byte-stream of the file content.\n\n        Args:\n            data: The FileData object representing the file to stream.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n\n    def content(self, data: data.FileData, extras: dict[str, Any]) -&gt; bytes:\n        \"\"\"Return file content as a single byte object.\n\n        Args:\n            data: The FileData object representing the file to read.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        return b\"\".join(self.stream(data, extras))\n\n    def range(self, data: data.FileData, start: int, end: int | None, extras: dict[str, Any]) -&gt; Iterable[bytes]:\n        \"\"\"Return slice of the file content.\n\n        Args:\n            data: The FileData object representing the file to read.\n            start: The starting byte offset.\n            end: The ending byte offset (inclusive).\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n\n    def permanent_link(self, data: data.FileData, extras: dict[str, Any]) -&gt; str:\n        \"\"\"Return permanent download link.\n\n        Args:\n            data: The FileData object representing the file.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n\n    def temporal_link(self, data: data.FileData, duration: int, extras: dict[str, Any]) -&gt; str:\n        \"\"\"Return temporal download link.\n\n        Args:\n            data: The FileData object representing the file.\n            duration: The duration for which the link is valid.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n\n    def one_time_link(self, data: data.FileData, extras: dict[str, Any]) -&gt; str:\n        \"\"\"Return one-time download link.\n\n        Args:\n            data: The FileData object representing the file.\n            extras: Additional metadata for the operation.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Reader.content","title":"<code>content(data, extras)</code>","text":"<p>Return file content as a single byte object.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def content(self, data: data.FileData, extras: dict[str, Any]) -&gt; bytes:\n    \"\"\"Return file content as a single byte object.\n\n    Args:\n        data: The FileData object representing the file to read.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    return b\"\".join(self.stream(data, extras))\n</code></pre>"},{"location":"api/#file_keeper.Reader.one_time_link","title":"<code>one_time_link(data, extras)</code>","text":"<p>Return one-time download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def one_time_link(self, data: data.FileData, extras: dict[str, Any]) -&gt; str:\n    \"\"\"Return one-time download link.\n\n    Args:\n        data: The FileData object representing the file.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Reader.permanent_link","title":"<code>permanent_link(data, extras)</code>","text":"<p>Return permanent download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def permanent_link(self, data: data.FileData, extras: dict[str, Any]) -&gt; str:\n    \"\"\"Return permanent download link.\n\n    Args:\n        data: The FileData object representing the file.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Reader.range","title":"<code>range(data, start, end, extras)</code>","text":"<p>Return slice of the file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>start</code> <p>The starting byte offset.</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>The ending byte offset (inclusive).</p> <p> TYPE: <code>int | None</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def range(self, data: data.FileData, start: int, end: int | None, extras: dict[str, Any]) -&gt; Iterable[bytes]:\n    \"\"\"Return slice of the file content.\n\n    Args:\n        data: The FileData object representing the file to read.\n        start: The starting byte offset.\n        end: The ending byte offset (inclusive).\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Reader.stream","title":"<code>stream(data, extras)</code>","text":"<p>Return byte-stream of the file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to stream.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def stream(self, data: data.FileData, extras: dict[str, Any]) -&gt; Iterable[bytes]:\n    \"\"\"Return byte-stream of the file content.\n\n    Args:\n        data: The FileData object representing the file to stream.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.Reader.temporal_link","title":"<code>temporal_link(data, duration, extras)</code>","text":"<p>Return temporal download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>duration</code> <p>The duration for which the link is valid.</p> <p> TYPE: <code>int</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p> Source code in <code>src/file_keeper/core/storage.py</code> <pre><code>def temporal_link(self, data: data.FileData, duration: int, extras: dict[str, Any]) -&gt; str:\n    \"\"\"Return temporal download link.\n\n    Args:\n        data: The FileData object representing the file.\n        duration: The duration for which the link is valid.\n        extras: Additional metadata for the operation.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/#file_keeper.FileData","title":"<code>FileData</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseData[TData]</code></p> <p>Information required by storage to operate the file.</p> PARAMETER DESCRIPTION <code>location</code> <p>filepath, filename or any other type of unique identifier</p> <p> TYPE: <code>Location</code> </p> <code>size</code> <p>size of the file in bytes</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>content_type</code> <p>MIMEtype of the file</p> <p> TYPE: <code>str</code> DEFAULT: <code>'application/octet-stream'</code> </p> <code>hash</code> <p>checksum of the file</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>storage_data</code> <p>additional details set by storage adapter</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>cast('dict[str, Any]', field(default_factory=dict))</code> </p> Example <pre><code>FileData(\n    \"local/path.txt\",\n    123,\n    \"text/plain\",\n    md5_of_content,\n)\n</code></pre> Source code in <code>src/file_keeper/core/data.py</code> <pre><code>@dataclasses.dataclass(frozen=True)\nclass FileData(BaseData[TData]):\n    \"\"\"Information required by storage to operate the file.\n\n    Args:\n        location: filepath, filename or any other type of unique identifier\n        size: size of the file in bytes\n        content_type: MIMEtype of the file\n        hash: checksum of the file\n        storage_data: additional details set by storage adapter\n\n    Example:\n        ```\n        FileData(\n            \"local/path.txt\",\n            123,\n            \"text/plain\",\n            md5_of_content,\n        )\n        ```\n    \"\"\"\n\n    content_type: str = \"application/octet-stream\"\n</code></pre>"},{"location":"api/#file_keeper.Upload","title":"<code>Upload</code>  <code>dataclass</code>","text":"<p>Standard upload details.</p> PARAMETER DESCRIPTION <code>stream</code> <p>iterable of bytes or file-like object</p> <p> TYPE: <code>PStream</code> </p> <code>filename</code> <p>name of the file</p> <p> TYPE: <code>str</code> </p> <code>size</code> <p>size of the file in bytes</p> <p> TYPE: <code>int</code> </p> <code>content_type</code> <p>MIMEtype of the file</p> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Upload(\n&gt;&gt;&gt;     BytesIO(b\"hello world\"),\n&gt;&gt;&gt;     \"file.txt\",\n&gt;&gt;&gt;     11,\n&gt;&gt;&gt;     \"text/plain\",\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>src/file_keeper/core/upload.py</code> <pre><code>@dataclasses.dataclass\nclass Upload:\n    \"\"\"Standard upload details.\n\n    Args:\n        stream: iterable of bytes or file-like object\n        filename: name of the file\n        size: size of the file in bytes\n        content_type: MIMEtype of the file\n\n    Examples:\n        &gt;&gt;&gt; Upload(\n        &gt;&gt;&gt;     BytesIO(b\"hello world\"),\n        &gt;&gt;&gt;     \"file.txt\",\n        &gt;&gt;&gt;     11,\n        &gt;&gt;&gt;     \"text/plain\",\n        &gt;&gt;&gt; )\n    \"\"\"\n\n    stream: types.PStream\n    \"\"\"Content as iterable of bytes\"\"\"\n    filename: str\n    \"\"\"Name of the file\"\"\"\n    size: int\n    \"\"\"Size of the file\"\"\"\n    content_type: str\n    \"\"\"MIME Type of the file\"\"\"\n\n    @property\n    def seekable_stream(self) -&gt; types.PSeekableStream | None:\n        \"\"\"Return stream that can be rewinded after reading.\n\n        If internal stream does not support file-like `seek`, nothing is\n        returned from this property.\n\n        Use this property if you want to read the file ahead, to get CSV column\n        names, list of files inside ZIP, EXIF metadata. If you get `None` from\n        it, stream does not support seeking and you won't be able to rewind\n        cursor to the beginning of the file after reading something.\n\n        Example:\n            ```python\n            upload = make_upload(...)\n            if fd := upload.seekable_stream():\n                # read fragment of the file\n                chunk = fd.read(1024)\n                # move cursor to the end of the stream\n                fd.seek(0, 2)\n                # position of the cursor is the same as number of bytes in stream\n                size = fd.tell()\n                # move cursor back, because you don't want to accidentally loose\n                # any bites from the beginning of stream when uploader reads from it\n                fd.seek(0)\n            ```\n\n        Returns:\n            file-like stream or nothing\n\n        \"\"\"\n        if hasattr(self.stream, \"tell\") and hasattr(self.stream, \"seek\"):\n            return cast(types.PSeekableStream, self.stream)\n\n        return None\n\n    def hashing_reader(self, **kwargs: Any) -&gt; utils.HashingReader:\n        \"\"\"Get reader for the upload that computes hash while reading content.\"\"\"\n        return utils.HashingReader(self.stream, **kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Upload.content_type","title":"<code>content_type</code>  <code>instance-attribute</code>","text":"<p>MIME Type of the file</p>"},{"location":"api/#file_keeper.Upload.filename","title":"<code>filename</code>  <code>instance-attribute</code>","text":"<p>Name of the file</p>"},{"location":"api/#file_keeper.Upload.seekable_stream","title":"<code>seekable_stream</code>  <code>property</code>","text":"<p>Return stream that can be rewinded after reading.</p> <p>If internal stream does not support file-like <code>seek</code>, nothing is returned from this property.</p> <p>Use this property if you want to read the file ahead, to get CSV column names, list of files inside ZIP, EXIF metadata. If you get <code>None</code> from it, stream does not support seeking and you won't be able to rewind cursor to the beginning of the file after reading something.</p> Example <pre><code>upload = make_upload(...)\nif fd := upload.seekable_stream():\n    # read fragment of the file\n    chunk = fd.read(1024)\n    # move cursor to the end of the stream\n    fd.seek(0, 2)\n    # position of the cursor is the same as number of bytes in stream\n    size = fd.tell()\n    # move cursor back, because you don't want to accidentally loose\n    # any bites from the beginning of stream when uploader reads from it\n    fd.seek(0)\n</code></pre> RETURNS DESCRIPTION <code>PSeekableStream | None</code> <p>file-like stream or nothing</p>"},{"location":"api/#file_keeper.Upload.size","title":"<code>size</code>  <code>instance-attribute</code>","text":"<p>Size of the file</p>"},{"location":"api/#file_keeper.Upload.stream","title":"<code>stream</code>  <code>instance-attribute</code>","text":"<p>Content as iterable of bytes</p>"},{"location":"api/#file_keeper.Upload.hashing_reader","title":"<code>hashing_reader(**kwargs)</code>","text":"<p>Get reader for the upload that computes hash while reading content.</p> Source code in <code>src/file_keeper/core/upload.py</code> <pre><code>def hashing_reader(self, **kwargs: Any) -&gt; utils.HashingReader:\n    \"\"\"Get reader for the upload that computes hash while reading content.\"\"\"\n    return utils.HashingReader(self.stream, **kwargs)\n</code></pre>"},{"location":"api/#file_keeper.Location","title":"<code>Location = NewType('Location', str)</code>  <code>module-attribute</code>","text":""},{"location":"api/#file_keeper.SignedAction","title":"<code>SignedAction = Literal['upload', 'download', 'delete']</code>  <code>module-attribute</code>","text":""},{"location":"api/#file_keeper.HashingReader","title":"<code>HashingReader</code>","text":"<p>IO stream wrapper that computes content hash while stream is consumed.</p> PARAMETER DESCRIPTION <code>stream</code> <p>iterable of bytes or file-like object</p> <p> TYPE: <code>PStream</code> </p> <code>chunk_size</code> <p>max number of bytes read at once</p> <p> TYPE: <code>int</code> DEFAULT: <code>CHUNK_SIZE</code> </p> <code>algorithm</code> <p>hashing algorithm</p> <p> TYPE: <code>str</code> DEFAULT: <code>'md5'</code> </p> Example <pre><code>reader = HashingReader(readable_stream)\nfor chunk in reader:\n    ...\nprint(f\"Hash: {reader.get_hash()}\")\n</code></pre> Source code in <code>src/file_keeper/core/utils.py</code> <pre><code>class HashingReader:\n    \"\"\"IO stream wrapper that computes content hash while stream is consumed.\n\n    Args:\n        stream: iterable of bytes or file-like object\n        chunk_size: max number of bytes read at once\n        algorithm: hashing algorithm\n\n    Example:\n        ```\n        reader = HashingReader(readable_stream)\n        for chunk in reader:\n            ...\n        print(f\"Hash: {reader.get_hash()}\")\n        ```\n    \"\"\"\n\n    stream: types.PStream\n    chunk_size: int\n    algorithm: str\n    hashsum: Any\n    position: int\n\n    def __init__(\n        self,\n        stream: types.PStream,\n        chunk_size: int = CHUNK_SIZE,\n        algorithm: str = \"md5\",\n    ):\n        self.stream = stream\n        self.chunk_size = chunk_size\n        self.algorithm = algorithm\n        self.hashsum = hashlib.new(algorithm)\n        self.position = 0\n\n    def __iter__(self) -&gt; Iterator[bytes]:\n        return self\n\n    def __next__(self) -&gt; bytes:\n        chunk = self.stream.read(self.chunk_size)\n        if not chunk:\n            raise StopIteration\n\n        self.position += len(chunk)\n        self.hashsum.update(chunk)\n        return chunk\n\n    next: Callable[..., bytes] = __next__\n\n    def read(self) -&gt; bytes:\n        \"\"\"Read and return all bytes from stream at once.\"\"\"\n        return b\"\".join(self)\n\n    def get_hash(self):\n        \"\"\"Get current content hash as a string.\"\"\"\n        return self.hashsum.hexdigest()\n\n    def exhaust(self):\n        \"\"\"Exhaust internal stream to compute final version of content hash.\n\n        Note, this method does not returns data from the stream. The content\n        will be irreversibly lost after method execution.\n        \"\"\"\n        for _ in self:\n            pass\n</code></pre>"},{"location":"api/#file_keeper.HashingReader.exhaust","title":"<code>exhaust()</code>","text":"<p>Exhaust internal stream to compute final version of content hash.</p> <p>Note, this method does not returns data from the stream. The content will be irreversibly lost after method execution.</p> Source code in <code>src/file_keeper/core/utils.py</code> <pre><code>def exhaust(self):\n    \"\"\"Exhaust internal stream to compute final version of content hash.\n\n    Note, this method does not returns data from the stream. The content\n    will be irreversibly lost after method execution.\n    \"\"\"\n    for _ in self:\n        pass\n</code></pre>"},{"location":"api/#file_keeper.HashingReader.get_hash","title":"<code>get_hash()</code>","text":"<p>Get current content hash as a string.</p> Source code in <code>src/file_keeper/core/utils.py</code> <pre><code>def get_hash(self):\n    \"\"\"Get current content hash as a string.\"\"\"\n    return self.hashsum.hexdigest()\n</code></pre>"},{"location":"api/#file_keeper.HashingReader.read","title":"<code>read()</code>","text":"<p>Read and return all bytes from stream at once.</p> Source code in <code>src/file_keeper/core/utils.py</code> <pre><code>def read(self) -&gt; bytes:\n    \"\"\"Read and return all bytes from stream at once.\"\"\"\n    return b\"\".join(self)\n</code></pre>"},{"location":"api/#file_keeper.Registry","title":"<code>Registry</code>","text":"<p>               Bases: <code>Generic[V, K]</code></p> <p>Mutable collection of objects.</p> Example <pre><code>col = Registry()\n\ncol.register(\"one\", 1)\nassert col.get(\"one\") == 1\n\ncol.reset()\nassert col.get(\"one\") is None\n\nassert list(col) == [\"one\"]\n</code></pre> Source code in <code>src/file_keeper/core/registry.py</code> <pre><code>class Registry(Generic[V, K]):\n    \"\"\"Mutable collection of objects.\n\n    Example:\n        ```py\n        col = Registry()\n\n        col.register(\"one\", 1)\n        assert col.get(\"one\") == 1\n\n        col.reset()\n        assert col.get(\"one\") is None\n\n        assert list(col) == [\"one\"]\n        ```\n    \"\"\"\n\n    members: MutableMapping[K, V]\n    collector: Callable[[], Mapping[K, V]] | None\n\n    def __init__(\n        self,\n        members: dict[K, V] | None = None,\n        collector: Callable[[], Mapping[K, V]] | None = None,\n    ):\n        if members is None:\n            members = {}\n        self.members = members\n        self.collector = collector\n\n    def __len__(self):\n        return len(self.members)\n\n    def __bool__(self):\n        return bool(self.members)\n\n    def __iter__(self):\n        return iter(self.members)\n\n    def __getitem__(self, key: K):\n        return self.members[key]\n\n    def __contains__(self, item: K):\n        return item in self.members\n\n    def collect(self):\n        \"\"\"Collect members of the registry.\"\"\"\n        if self.collector:\n            self.members.update(self.collector())\n\n    def reset(self):\n        \"\"\"Remove all members from registry.\"\"\"\n        self.members.clear()\n\n    def register(self, key: K, member: V):\n        \"\"\"Add a member to registry.\"\"\"\n        self.members[key] = member\n\n    def get(self, key: K) -&gt; V | None:\n        \"\"\"Get the optional member from registry.\"\"\"\n        return self.members.get(key)\n\n    def pop(self, key: K) -&gt; V | None:\n        \"\"\"Remove the member from registry.\"\"\"\n        return self.members.pop(key, None)\n\n    def decorated(self, key: K):\n        \"\"\"Collect member via decorator.\"\"\"\n\n        def decorator(value: V):\n            self.register(key, value)\n            return value\n\n        return decorator\n</code></pre>"},{"location":"api/#file_keeper.Registry.collect","title":"<code>collect()</code>","text":"<p>Collect members of the registry.</p> Source code in <code>src/file_keeper/core/registry.py</code> <pre><code>def collect(self):\n    \"\"\"Collect members of the registry.\"\"\"\n    if self.collector:\n        self.members.update(self.collector())\n</code></pre>"},{"location":"api/#file_keeper.Registry.decorated","title":"<code>decorated(key)</code>","text":"<p>Collect member via decorator.</p> Source code in <code>src/file_keeper/core/registry.py</code> <pre><code>def decorated(self, key: K):\n    \"\"\"Collect member via decorator.\"\"\"\n\n    def decorator(value: V):\n        self.register(key, value)\n        return value\n\n    return decorator\n</code></pre>"},{"location":"api/#file_keeper.Registry.get","title":"<code>get(key)</code>","text":"<p>Get the optional member from registry.</p> Source code in <code>src/file_keeper/core/registry.py</code> <pre><code>def get(self, key: K) -&gt; V | None:\n    \"\"\"Get the optional member from registry.\"\"\"\n    return self.members.get(key)\n</code></pre>"},{"location":"api/#file_keeper.Registry.pop","title":"<code>pop(key)</code>","text":"<p>Remove the member from registry.</p> Source code in <code>src/file_keeper/core/registry.py</code> <pre><code>def pop(self, key: K) -&gt; V | None:\n    \"\"\"Remove the member from registry.\"\"\"\n    return self.members.pop(key, None)\n</code></pre>"},{"location":"api/#file_keeper.Registry.register","title":"<code>register(key, member)</code>","text":"<p>Add a member to registry.</p> Source code in <code>src/file_keeper/core/registry.py</code> <pre><code>def register(self, key: K, member: V):\n    \"\"\"Add a member to registry.\"\"\"\n    self.members[key] = member\n</code></pre>"},{"location":"api/#file_keeper.Registry.reset","title":"<code>reset()</code>","text":"<p>Remove all members from registry.</p> Source code in <code>src/file_keeper/core/registry.py</code> <pre><code>def reset(self):\n    \"\"\"Remove all members from registry.\"\"\"\n    self.members.clear()\n</code></pre>"},{"location":"api/#file_keeper.IterableBytesReader","title":"<code>IterableBytesReader</code>","text":"<p>               Bases: <code>AbstractReader[Iterable[int]]</code></p> <p>Wrapper that transforms iterable of bytes into readable stream.</p> Example <p>The simplest iterable of bytes is a list that contains byte strings: <pre><code>parts = [b\"hello\", b\" \", b\"world\"]\nreader = IterableBytesReader(parts)\nassert reader.read() == b\"hello world\"\n</code></pre></p> <p>More realistic scenario is wrapping generator that produces byte string in order to initialize Upload: <pre><code>def data_generator():\n    yield b\"hello\"\n    yield b\" \"\n    yield b\"world\"\n\nstream = IterableBytesReader(data_generator())\nupload = Upload(stream, \"my_file.txt\", 11, \"text/plain\")\n</code></pre></p> Source code in <code>src/file_keeper/core/utils.py</code> <pre><code>class IterableBytesReader(AbstractReader[Iterable[int]]):\n    \"\"\"Wrapper that transforms iterable of bytes into readable stream.\n\n    Example:\n        The simplest iterable of bytes is a list that contains byte strings:\n        ```py\n        parts = [b\"hello\", b\" \", b\"world\"]\n        reader = IterableBytesReader(parts)\n        assert reader.read() == b\"hello world\"\n        ```\n\n        More realistic scenario is wrapping generator that produces byte string\n        in order to initialize [Upload][file_keeper.Upload]:\n        ```py\n        def data_generator():\n            yield b\"hello\"\n            yield b\" \"\n            yield b\"world\"\n\n        stream = IterableBytesReader(data_generator())\n        upload = Upload(stream, \"my_file.txt\", 11, \"text/plain\")\n        ```\n\n    \"\"\"\n\n    def __init__(self, source: Iterable[bytes], chunk_size: int = CHUNK_SIZE):\n        super().__init__(itertools.chain.from_iterable(source), chunk_size)\n\n    @override\n    def read(self, size: int | None = None):\n        return bytes(itertools.islice(self.source, 0, size))\n</code></pre>"},{"location":"api/#file_keeper.adapters","title":"<code>adapters = Registry['type[Storage]']()</code>  <code>module-attribute</code>","text":""},{"location":"api/#file_keeper.parse_filesize","title":"<code>parse_filesize(value)</code>","text":"<p>Transform human-readable filesize into an integer.</p> PARAMETER DESCRIPTION <code>value</code> <p>human-readable filesize</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>size cannot be parsed or uses unknown units</p> Example <pre><code>size = parse_filesize(\"10GiB\")\nassert size == 10_737_418_240\n</code></pre> Source code in <code>src/file_keeper/core/utils.py</code> <pre><code>def parse_filesize(value: str) -&gt; int:\n    \"\"\"Transform human-readable filesize into an integer.\n\n    Args:\n        value: human-readable filesize\n\n    Raises:\n        ValueError: size cannot be parsed or uses unknown units\n\n    Example:\n        ```python\n        size = parse_filesize(\"10GiB\")\n        assert size == 10_737_418_240\n        ```\n    \"\"\"\n    result = RE_FILESIZE.match(value.strip())\n    if not result:\n        raise ValueError(value)\n\n    size, unit = result.groups()\n\n    multiplier = UNITS.get(unit.lower())\n    if not multiplier:\n        raise ValueError(value)\n\n    return int(float(size) * multiplier)\n</code></pre>"},{"location":"api/#file_keeper.humanize_filesize","title":"<code>humanize_filesize(value, base=SI_BASE)</code>","text":"<p>Transform an integer into human-readable filesize.</p> PARAMETER DESCRIPTION <code>value</code> <p>size in bytes</p> <p> TYPE: <code>int | float</code> </p> <code>base</code> <p>1000 for SI(KB, MB) or 1024 for binary(KiB, MiB)</p> <p> TYPE: <code>int</code> DEFAULT: <code>SI_BASE</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>base is not recognized</p> Example <pre><code>size = humanize_filesize(10_737_418_240, base=1024)\nassert size == \"10GiB\"\nsize = humanize_filesize(10_418_240, base=1024)\nassert size == \"9.9MiB\"\n</code></pre> Source code in <code>src/file_keeper/core/utils.py</code> <pre><code>def humanize_filesize(value: int | float, base: int = SI_BASE) -&gt; str:\n    \"\"\"Transform an integer into human-readable filesize.\n\n    Args:\n        value: size in bytes\n        base: 1000 for SI(KB, MB) or 1024 for binary(KiB, MiB)\n\n    Raises:\n        ValueError: base is not recognized\n\n    Example:\n        ```python\n        size = humanize_filesize(10_737_418_240, base=1024)\n        assert size == \"10GiB\"\n        size = humanize_filesize(10_418_240, base=1024)\n        assert size == \"9.9MiB\"\n        ```\n\n    \"\"\"\n    if base == SI_BASE:\n        suffixes = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\"]\n    elif base == BINARY_BASE:\n        suffixes = [\"B\", \"KiB\", \"MiB\", \"GiB\", \"TiB\", \"PiB\", \"EiB\"]\n    else:\n        raise ValueError(base)\n\n    iteration = 0\n\n    while value &gt;= base:\n        iteration += 1\n        value /= base\n\n    value = int(value * 100) / 100\n\n    num_format = \".2f\" if iteration and not value.is_integer() else \".0f\"\n\n    return f\"{value:{num_format}}{suffixes[iteration]}\"\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#unreleased","title":"[unreleased]","text":"<p>Compare with v0.0.10</p>"},{"location":"changelog/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>[breaking] frozen FileData and MultipartData (cb9dbf8)</li> <li>define <code>RESUMABLE</code> capability (bab84fa)</li> <li>complete GCS adapter (db97e20)</li> <li>Azure Blob storage adapter (fb491b2)</li> <li>zip adapter (3b3f978)</li> <li>add SIGNED capability (fc5fcbb)</li> <li>Storage.full_path (7974aca)</li> <li>add generic disabled_capabilities option (c335071)</li> <li>add EXISTS, SCAN, MOVE and COPY to s3 adapter (2cb762e)</li> <li>add EXISTS and ANALYZE to libcloud adapter (12349aa)</li> <li>less strict typing rules for storage settings (247d1c6)</li> <li>remove str from exceptions (2ecd8a2)</li> <li>add memory storage (3abc218)</li> <li>storage.ext.register accepts optional <code>reset</code> parameter (c5edce8)</li> <li>Settings log extra options with debug level instead of raising an exception (7308578)</li> <li>add null storage (c1f8476)</li> </ul>"},{"location":"changelog/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>s3 unconditionally overrides files (c059f2b)</li> <li>storage settings keep a lot of intermediate parameters (cf69cf2)</li> <li>libcloud silently overrides existing objects (599f099)</li> </ul>"},{"location":"changelog/#refactor","title":"\ud83d\ude9c Refactor","text":"<ul> <li>[breaking] remove <code>location</code> from arguments of <code>multipart_start</code> (876ce46)</li> <li>[breaking] drop <code>MultipartData</code> and use <code>FileData</code> instead everywhere (c1c01c3)</li> <li>[breaking] <code>Storage.remove</code> does not accept <code>MultipartData</code>. Use <code>Storage.multipart_remove</code> instead (ce3e522)</li> <li>[breaking] <code>create_path</code> option for fs renamed to <code>initialize</code> (1329997)</li> <li>[breaking] Storage.temporal_link requires <code>duration</code> parameter (0d92777)</li> <li>[breaking] Storage.stream_as_upload renamed to file_as_upload (29ec68b)</li> <li>[breaking] fs and opendal settings no longer have recursive flag (3f6e29b)</li> <li>redis uses <code>bucket</code> option instead of <code>path</code> (966241f)</li> <li>remove pytz dependency (43079ea)</li> <li>rename redis_url to url in redis settings (2c998f4)</li> </ul>"},{"location":"changelog/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Add overview page (6ad5c89)</li> </ul>"},{"location":"changelog/#v0010-2025-07-13","title":"v0.0.10 - 2025-07-13","text":"<p>Compare with v0.0.9</p>"},{"location":"changelog/#features_1","title":"\ud83d\ude80 Features","text":"<ul> <li>add public_prefix(and permanent_link) to libcloud (3bc7591)</li> <li>static_uuid transformer (88383e0)</li> <li>location transformers receive optional upload-or-data second argument (8e6a6dc)</li> </ul>"},{"location":"changelog/#bug-fixes_1","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>fix_extension transformer raises an error when upload is missing (a827df5)</li> </ul>"},{"location":"changelog/#removal","title":"\u274c Removal","text":"<ul> <li>delete Storage.public_link (da08744)</li> </ul>"},{"location":"changelog/#v009-2025-07-02","title":"v0.0.9 - 2025-07-02","text":"<p>Compare with v0.0.8</p>"},{"location":"changelog/#features_2","title":"\ud83d\ude80 Features","text":"<ul> <li>add fix_extension transformer (1345915)</li> <li>opendal got path option (d044ade)</li> </ul>"},{"location":"changelog/#bug-fixes_2","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>cast fs:multipart:position to int (dc4d768)</li> </ul>"},{"location":"changelog/#v008-2025-04-23","title":"v0.0.8 - 2025-04-23","text":"<p>Compare with v0.0.7</p>"},{"location":"changelog/#features_3","title":"\ud83d\ude80 Features","text":"<ul> <li>libcloud storage got path option (555036c)</li> </ul>"},{"location":"changelog/#bug-fixes_3","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>fs storage reports relative location of the missing file (cef9589)</li> </ul>"},{"location":"changelog/#v007-2025-03-28","title":"v0.0.7 - 2025-03-28","text":"<p>Compare with v0.0.6</p>"},{"location":"changelog/#features_4","title":"\ud83d\ude80 Features","text":"<ul> <li>storage upload and append requires Upload (ebf5fef)</li> </ul>"},{"location":"changelog/#bug-fixes_4","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>fs storage checks permission when creating folders (1791d68)</li> </ul>"},{"location":"changelog/#v006-2025-03-23","title":"v0.0.6 - 2025-03-23","text":"<p>Compare with v0.0.4</p>"},{"location":"changelog/#features_5","title":"\ud83d\ude80 Features","text":"<ul> <li>add *_synthetic methods (27c4164)</li> </ul>"},{"location":"changelog/#bug-fixes_5","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>multipart update of s3 and gcs work with arbitrary upload (902c9cb)</li> </ul>"},{"location":"changelog/#v004-2025-03-22","title":"v0.0.4 - 2025-03-22","text":"<p>Compare with v0.0.3</p>"},{"location":"changelog/#features_6","title":"\ud83d\ude80 Features","text":"<ul> <li>storages accept any type of upload (a64ee3d)</li> </ul>"},{"location":"changelog/#refactor_1","title":"\ud83d\ude9c Refactor","text":"<ul> <li>remove validation from storage (93392f9)</li> <li>remove type and size validation from append and compose (890c89a)</li> <li>remove public link method and capability (cb39151)</li> </ul>"},{"location":"changelog/#removal_1","title":"\u274c Removal","text":"<ul> <li>drop link storage (3328eb2)</li> </ul>"},{"location":"changelog/#v002-2025-03-17","title":"v0.0.2 - 2025-03-17","text":"<p>Compare with v0.0.1</p>"},{"location":"changelog/#features_7","title":"\ud83d\ude80 Features","text":"<ul> <li>stream-based composite implementation of range (7d47bd8)</li> <li>add Location wrapper around unsafe path parameters (b99f155)</li> <li>file_keeper:opendal adapter (214fb6c)</li> <li>file_keeper:redis adapter (8c7da94)</li> </ul>"},{"location":"changelog/#bug-fixes_6","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>map error during settings initialization into custom exception (f596037)</li> <li>fs adapter: infer <code>uploaded</code> size if it is not specified in <code>multipart_update</code> (620ec3a)</li> </ul>"},{"location":"changelog/#refactor_2","title":"\ud83d\ude9c Refactor","text":"<ul> <li><code>location_strategy: str</code> become <code>location_transformers: list[str]</code> (daf2dc6)</li> <li>remove default range implementation from reader (36f5f31)</li> </ul>"},{"location":"changelog/#testing","title":"\ud83e\uddea Testing","text":"<ul> <li>add standard adapter tests (d961682)</li> </ul>"},{"location":"changelog/#v001-2025-03-13","title":"v0.0.1 - 2025-03-13","text":""},{"location":"configuration/","title":"Configuration","text":"<p>Behavior of every storage is configurable through the Settings class. This page details the available settings and how to use them to customize your storage setup.</p>"},{"location":"configuration/#the-settings-class","title":"The Settings Class","text":"<p>The Settings class is the central point for configuring each storage adapter. It defines the options that control how the adapter interacts with the underlying storage.  Each adapter has its own subclass of Settings that adds adapter-specific options.</p> <p>While you can directly instantiate a Settings subclass with its arguments, the most common approach is to pass a dictionary of options to the storage adapter constructor. file-keeper automatically handles the transformation of this dictionary into a Settings object. This provides a more flexible and user-friendly configuration experience.</p> <p>Example</p> <p>Let's say you want to configure the S3 adapter. Instead of creating a Settings object directly, you can simply pass a dictionary like this:</p> <pre><code>from file_keeper import make_storage\n\ns3_settings = {\n    \"type\": \"file_keeper:s3\",\n    \"bucket\": \"my-s3-bucket\",\n    \"key\": \"YOUR_AWS_ACCESS_KEY_ID\",\n    \"secret\": \"YOUR_AWS_SECRET_ACCESS_KEY\",\n    \"region\": \"us-east-1\",\n}\n\nstorage = make_storage(\"my_s3_storage\", s3_settings)\n\n# Accessing the settings (for demonstration)\nprint(storage.settings.bucket) # (1)!\n</code></pre> <ol> <li>Output: my-s3-bucket</li> </ol> <p>In this example, <code>make_storage</code> automatically creates a <code>Settings</code> object from the <code>s3_settings</code> dictionary.</p>"},{"location":"configuration/#common-settings","title":"Common Settings","text":"<p>These settings are available for most storage adapters:</p> Setting Type Default Description <code>name</code> str <code>\"unknown\"</code> Descriptive name of the storage used for debugging. <code>override_existing</code> bool <code>False</code> If <code>True</code>, existing files will be overwritten during upload. If <code>False</code>, an <code>ExistingFileError</code> will be raised if a file with the same location already exists. <code>path</code> str <code>\"\"</code> The base path or directory where files are stored. The exact meaning depends on the adapter (e.g., a path in the filesystem for the FS adapter, a prefix-path inside the bucket for S3).  Required for most adapters. <code>location_transformers</code> list[str] <code>[]</code> A list of names of location transformers to apply to file locations. These transformers can be used to sanitize or modify file paths before they are used to store files. See the Extending file-keeper documentation for details. <code>disabled_capabilities</code> list[str] <code>[]</code> A list of capabilities to disable for the storage adapter. This can be useful for limiting the functionality of an adapter or for testing purposes. <code>initialize</code> bool <code>False</code> Prepare storage backend for uploads. The exact meaning depends on the adapter. Filesystem adapter created the upload folder if it's missing; cloud adapters create a bucket/container if it does not exists. <p>Important Considerations</p> Security Be careful when storing sensitive information like AWS access keys and secret keys in your configuration.  Consider using environment variables or a secure configuration management system. Validation file-keeper performs some basic validation of the configuration settings, but it's important to ensure that your settings are correct for your specific storage adapter. Error Handling Be prepared to handle <code>InvalidStorageConfigurationError</code> exceptions if your configuration is invalid."},{"location":"configuration/#adapter-specific-settings","title":"Adapter-Specific Settings","text":"<p>In addition to the common settings, adapters have their own specific settings:</p>"},{"location":"configuration/#file_keeperazure_blob","title":"<code>file_keeper:azure_blob</code>","text":"Setting Type Default Description <code>account_name</code> str <code>\"\"</code> Name of the account. <code>account_key</code> str <code>\"\"</code> Key for the account. <code>container_name</code> str <code>\"\"</code> Name of the storage container. <code>account_url</code> str <code>\"https://{account_name}.blob.core.windows.net\"</code> Custom resource URL. <code>client</code> BlobServiceClient <code>None</code> Existing storage client. <code>container</code> ContainerClient <code>None</code> Existing container client. <code>path</code> str <code>\"\"</code> Prefix for the file location. <code>initialize</code> bool <code>False</code> Create <code>container_name</code> if it does not exist."},{"location":"configuration/#file_keeperfilebin","title":"<code>file_keeper:filebin</code>","text":"Setting Type Default Description <code>bin</code> str <code>\"\"</code> The name of the bin. <code>timeout</code> int <code>10</code> Request timeout in seconds."},{"location":"configuration/#file_keeperfs","title":"<code>file_keeper:fs</code>","text":"Setting Type Default Description <code>path</code> str <code>\"\"</code> The base directory where files are stored. <code>initialize</code> bool <code>False</code> Create <code>path</code> if it does not exist."},{"location":"configuration/#file_keepergcs","title":"<code>file_keeper:gcs</code>","text":"Setting Type Default Description <code>bucket_name</code> str <code>\"\"</code> Name of the storage bucket. <code>client</code> Client <code>None</code> Existing storage client. <code>bucket</code> Bucket <code>None</code> Existing storage bucket. <code>credentials</code> Credentials <code>None</code> Existing cloud credentials. <code>credentials_file</code> str <code>\"\"</code> Path to the JSON with cloud credentials. <code>project_id</code> str <code>\"\"</code> The project which the client acts on behalf of. <code>client_options</code> dict <code>None</code> Client options for storage client. <code>path</code> str <code>\"\"</code> Prefix for the file location. <code>initialize</code> bool <code>False</code> Create <code>bucket_name</code> if it does not exist."},{"location":"configuration/#file_keeperlibcloud","title":"<code>file_keeper:libcloud</code>","text":"Setting Type Default Description <code>provider</code> str <code>\"\"</code> Name of the Libcloud provider <code>key</code> str <code>\"\"</code> Access key of the cloud account. <code>secret</code> str or None <code>None</code> Secret key of the cloud account. <code>params</code> dict <code>{}</code> Additional parameters for cloud provider. <code>container_name</code> str <code>\"\"</code> Name of the cloud container. <code>public_prefix</code> str <code>\"\"</code> Root URL for containers with public access. This URL will be used as a prefix for the file object location when building permanent links. <code>driver</code> StorageDriver <code>None</code> Existing storage driver. <code>container</code> Container <code>None</code> Existing container object. <code>path</code> str <code>\"\"</code> Prefix for the file location. <code>initialize</code> bool <code>False</code> Create <code>container_name</code> if it does not exist."},{"location":"configuration/#file_keepermemory","title":"<code>file_keeper:memory</code>","text":"Setting Type Default Description <code>bucket</code> MutableMapping[str, bytes] <code>{}</code> Container for uploaded objects."},{"location":"configuration/#file_keepernull","title":"<code>file_keeper:null</code>","text":"<p>No specific settings</p>"},{"location":"configuration/#file_keeperopendal","title":"<code>file_keeper:opendal</code>","text":"Setting Type Default Description <code>operator</code> opendal.Operator <code>None</code> Existing OpenDAL operator <code>scheme</code> str <code>\"\"</code> Name of OpenDAL operator's scheme. <code>params</code> dict <code>{}</code> Parameters for OpenDAL operator initialization. <code>path</code> str <code>\"\"</code> Prefix for the file location."},{"location":"configuration/#file_keeperredis","title":"<code>file_keeper:redis</code>","text":"Setting Type Default Description <code>redis</code> redis.Redis <code>None</code> Optional existing connection to Redis DB <code>url</code> str <code>\"\"</code> URL of the Redis DB. <code>bucket</code> str <code>\"\"</code> Key of the Redis HASH for uploaded objects."},{"location":"configuration/#file_keepers3","title":"<code>file_keeper:s3</code>","text":"Setting Type Default Description <code>bucket</code> str <code>\"\"</code> Name of the storage bucket. <code>client</code> S3Client <code>None\"</code> Existing S3 client. <code>key</code> str <code>None</code> The AWS Access Key. <code>secret</code> str <code>None</code> The AWS Secret Key. <code>region</code> str <code>None</code> The AWS Region of the bucket. <code>endpoint</code> str <code>None</code> Custom AWS endpoint. <code>path</code> str <code>\"\"</code> Prefix for the file location. <code>initialize</code> bool <code>False</code> Create <code>bucket</code> if it does not exist."},{"location":"configuration/#file_keepersqlalchemy","title":"<code>file_keeper:sqlalchemy</code>","text":"Setting Type Default Description <code>db_url</code> str <code>\"\"</code> URL of the storage DB. <code>table_name</code> str <code>\"\"</code> Name of the storage table. <code>location_column</code> str <code>\"\"</code> Name of the column that contains file location. <code>content_column</code> str <code>\"\"</code> Name of the column that contains file content. <code>engine</code> Engine <code>None</code> Existing DB engine. <code>table</code> Engine <code>None</code> Existing DB table. <code>location</code> Engine <code>None</code> Existing column for location. <code>content</code> Engine <code>None</code> Existing column for content. <code>initialize</code> bool <code>False</code> Create <code>table_name</code> if it does not exist."},{"location":"configuration/#file_keeperzip","title":"<code>file_keeper:zip</code>","text":"Setting Type Default Description <code>path</code> str <code>\"\"</code> Path of the ZIP archive for uploaded objects."},{"location":"error_handling/","title":"Error handling","text":"<p>file-keeper provides a comprehensive set of exceptions to help you handle errors gracefully. This page documents the available exceptions and provides guidance on how to handle them.</p>"},{"location":"error_handling/#general-exception-hierarchy","title":"General exception hierarchy","text":"<p>All exceptions in file-keeper inherit from the base <code>FilesError</code> exception.  This allows you to catch all file-keeper related errors with a single <code>except</code> block.  More specific exceptions are derived from FilesError to provide more detailed error information.</p> <p>Note</p> <p>file-keeper's exceptions can be imported from <code>file_keeper.core.exceptions</code></p> <pre><code>from file_keeper.core.exceptions import FilesError\n\ntry:\n    ...\nexcept FilesError:\n    ...\n</code></pre> <p>As a shortcut, they can be accessed from the <code>exc</code> object available at the root of file-keeper module</p> <pre><code>from file_keeper import exc\n\ntry:\n    ...\nexcept exc.FilesError:\n    ...\n</code></pre>"},{"location":"error_handling/#example-error-handling","title":"Example error handling","text":"<pre><code>from file_keeper import make_storage, make_upload, exc\n\ntry:\n    storage = make_storage(\"my_storage\", {\"type\": \"file_keeper:fs\", \"path\": \"/nonexistent/path\"})\nexcept exc.InvalidStorageConfigurationError as e:\n    print(f\"Error configuring storage: {e}\")\n\nupload = make_upload(b\"Hello, file-keeper!\")\n\ntry:\n    storage.upload(\"my_file.txt\", upload)\nexcept exc.ExistingFileError as e:\n    print(f\"File already exists: {e}\")\n</code></pre>"},{"location":"error_handling/#file_keeper.exc","title":"<code>exc</code>","text":"<p>Exception definitions for the extension.</p> <p>Hierarchy:</p> <ul> <li>Exception<ul> <li>FilesError<ul> <li>StorageError<ul> <li>UnknownAdapterError</li> <li>UnknownStorageError</li> <li>UnsupportedOperationError</li> <li>PermissionError</li> <li>LocationError<ul> <li>MissingFileError</li> <li>ExistingFileError</li> </ul> </li> <li>ExtrasError<ul> <li>MissingExtrasError</li> </ul> </li> <li>InvalidStorageConfigurationError<ul> <li>MissingStorageConfigurationError</li> </ul> </li> <li>UploadError<ul> <li>WrongUploadTypeError</li> <li>LocationTransformerError</li> <li>ContentError</li> <li>LargeUploadError<ul> <li>UploadOutOfBoundError</li> </ul> </li> <li>UploadMismatchError<ul> <li>UploadTypeMismatchError</li> <li>UploadHashMismatchError</li> <li>UploadSizeMismatchError</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"error_handling/#file_keeper.exc.ContentError","title":"<code>ContentError(storage, msg)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Storage cannot accept uploaded content.</p>"},{"location":"error_handling/#file_keeper.exc.ExistingFileError","title":"<code>ExistingFileError(storage, location)</code>","text":"<p>               Bases: <code>LocationError</code></p> <p>File already exists.</p>"},{"location":"error_handling/#file_keeper.exc.ExtrasError","title":"<code>ExtrasError(problem)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Wrong extras passed during upload.</p>"},{"location":"error_handling/#file_keeper.exc.FilesError","title":"<code>FilesError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base error for catch-all scenario.</p>"},{"location":"error_handling/#file_keeper.exc.InvalidStorageConfigurationError","title":"<code>InvalidStorageConfigurationError(adapter_or_storage, problem)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Storage cannot be initialized with given configuration.</p>"},{"location":"error_handling/#file_keeper.exc.LargeUploadError","title":"<code>LargeUploadError(actual_size, max_size)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Storage cannot be initialized due to missing option.</p>"},{"location":"error_handling/#file_keeper.exc.LocationError","title":"<code>LocationError(storage, location)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Storage cannot use given location.</p>"},{"location":"error_handling/#file_keeper.exc.LocationTransformerError","title":"<code>LocationTransformerError(transformer)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Undefined location transformer.</p>"},{"location":"error_handling/#file_keeper.exc.MissingExtrasError","title":"<code>MissingExtrasError(key)</code>","text":"<p>               Bases: <code>ExtrasError</code></p> <p>Wrong extras passed to storage method.</p>"},{"location":"error_handling/#file_keeper.exc.MissingFileError","title":"<code>MissingFileError(storage, location)</code>","text":"<p>               Bases: <code>LocationError</code></p> <p>File does not exist.</p>"},{"location":"error_handling/#file_keeper.exc.MissingStorageConfigurationError","title":"<code>MissingStorageConfigurationError(adapter_or_storage, option)</code>","text":"<p>               Bases: <code>InvalidStorageConfigurationError</code></p> <p>Storage cannot be initialized due to missing option.</p>"},{"location":"error_handling/#file_keeper.exc.PermissionError","title":"<code>PermissionError(storage, operation, problem)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Storage client does not have required permissions.</p>"},{"location":"error_handling/#file_keeper.exc.StorageError","title":"<code>StorageError</code>","text":"<p>               Bases: <code>FilesError</code></p> <p>Error related to storage.</p>"},{"location":"error_handling/#file_keeper.exc.UnknownAdapterError","title":"<code>UnknownAdapterError(adapter)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Specified storage adapter is not registered.</p>"},{"location":"error_handling/#file_keeper.exc.UnknownStorageError","title":"<code>UnknownStorageError(storage)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Storage with the given name is not configured.</p>"},{"location":"error_handling/#file_keeper.exc.UnsupportedOperationError","title":"<code>UnsupportedOperationError(operation, storage)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Requested operation is not supported by storage.</p>"},{"location":"error_handling/#file_keeper.exc.UploadError","title":"<code>UploadError</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Error related to file upload process.</p>"},{"location":"error_handling/#file_keeper.exc.UploadHashMismatchError","title":"<code>UploadHashMismatchError(actual, expected)</code>","text":"<p>               Bases: <code>UploadMismatchError</code></p> <p>Expected value of hash match the actual value.</p>"},{"location":"error_handling/#file_keeper.exc.UploadMismatchError","title":"<code>UploadMismatchError(attribute, actual, expected)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Expected value of file attribute doesn't match the actual value.</p>"},{"location":"error_handling/#file_keeper.exc.UploadOutOfBoundError","title":"<code>UploadOutOfBoundError(actual_size, max_size)</code>","text":"<p>               Bases: <code>LargeUploadError</code></p> <p>Ongoing upload exceeds expected size.</p>"},{"location":"error_handling/#file_keeper.exc.UploadSizeMismatchError","title":"<code>UploadSizeMismatchError(actual, expected)</code>","text":"<p>               Bases: <code>UploadMismatchError</code></p> <p>Expected value of upload size doesn't match the actual value.</p>"},{"location":"error_handling/#file_keeper.exc.UploadTypeMismatchError","title":"<code>UploadTypeMismatchError(actual, expected)</code>","text":"<p>               Bases: <code>UploadMismatchError</code></p> <p>Expected value of content type doesn't match the actual value.</p>"},{"location":"error_handling/#file_keeper.exc.WrongUploadTypeError","title":"<code>WrongUploadTypeError(content_type)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Storage does not support given MIMEType.</p>"},{"location":"adapters/azure_blob/","title":"Azure Blob Storage","text":"<p>The <code>file_keeper:azure_blob</code> adapter allows you to use Microsoft Azure Blob Storage for storing and retrieving files. This adapter leverages the <code>azure-storage-blob</code> Python library.</p>"},{"location":"adapters/azure_blob/#overview","title":"Overview","text":"<p>This adapter provides a convenient way to integrate Azure Blob Storage with file-keeper. You'll need to have the <code>azure-storage-blob</code> library installed (<code>pip install azure-storage-blob</code>) and configure it with the appropriate credentials for your Azure account.</p>"},{"location":"adapters/azure_blob/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the Azure Blob Storage adapter:</p> <pre><code>from file_keeper import make_storage\n\nstorage = make_storage(\"my_azure_storage\", {\n    \"type\": \"file_keeper:azure_blob\",\n    \"account_name\": \"your_account_name\",  # Replace with your Azure account name\n    \"account_key\": \"your_account_key\",  # Replace with your Azure account key\n    \"container_name\": \"file-keeper-container\",  # Replace with your container name\n    \"endpoint\": \"https://your_account_name.blob.core.windows.net/\", # Optional: Specify the endpoint\n})\n\n# Now you can use the 'storage' object to upload, download, and manage files.\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values (account_name, account_key, container_name, endpoint) with your actual Azure account credentials and configuration.</li> <li>Ensure that you have created a container in your Azure Blob Storage account to store the files.</li> <li>For enhanced security, consider using Azure Active Directory (Azure AD) authentication instead of account keys.  Refer to the <code>azure-storage-blob</code> documentation for details on Azure AD authentication.</li> <li>Refer to the Azure Blob Storage documentation for more information about Azure Blob Storage.</li> </ul>"},{"location":"adapters/emulate/","title":"Emulate cloud providers with Docker","text":"<p>For local development and testing, you can emulate cloud providers using Docker containers. This allows you to test your file-keeper integrations without incurring costs or requiring access to real cloud resources.</p> <p>Remember to adjust the port mappings and environment variables as needed for your specific setup.</p> MinIO (S3-compatible)Azurite (Azure Blob Storage)Fake GCS Server (Google Cloud Storage) <pre><code>docker run -d -p 9000:9000 -p 9001:9001 \\\n    --name minio \\\n    -e MINIO_PUBLIC_ADDRESS=0.0.0.0:9000 \\\n    quay.io/minio/minio server /data --console-address \":9001\"\n</code></pre> Attribute Value Endpoint http://127.0.0.1:9000 Key <code>minioadmin</code> Secret <code>minioadmin</code> <pre><code>docker run -d -p 10000:10000 \\\n    --name azurite-blob \\\n    mcr.microsoft.com/azure-storage/azurite azurite-blob --blobHost 0.0.0.0\n</code></pre> Attribute Value Endpoint http://127.0.0.1:10000 Key <code>devstoreaccount1</code> Secret <code>Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==</code> <pre><code>docker run -d -p 4443:4443 \\\n     --name gcs \\\n     fsouza/fake-gcs-server -scheme http\n</code></pre> Attribute Value Endpoint http://127.0.0.1:4443 Key Not requred Secret Not required"},{"location":"adapters/filebin/","title":"Filebin Adapter","text":"<p>The <code>file_keeper:filebin</code> adapter allows you to use Filebin for storing and retrieving files. Filebin is a simple, self-hosted file storage service. This adapter leverages the <code>filebin</code> Python library.</p>"},{"location":"adapters/filebin/#overview","title":"Overview","text":"<p>This adapter provides a convenient way to integrate Filebin with file-keeper. You'll need to have the <code>filebin</code> library installed (<code>pip install filebin</code>) and a running Filebin instance.</p>"},{"location":"adapters/filebin/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the Filebin adapter:</p> <pre><code>from file_keeper import make_storage\n\nstorage = make_storage(\"my_filebin_storage\", {\n    \"type\": \"file_keeper:filebin\",\n    \"url\": \"http://your_filebin_instance:8000\",  # Replace with your Filebin instance URL\n    \"api_key\": \"YOUR_API_KEY\",  # Replace with your Filebin API key\n})\n\n# Now you can use the 'storage' object to upload, download, and manage files.\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values (url, api_key) with your actual Filebin instance URL and API key.</li> <li>Ensure that your Filebin instance is running and accessible.</li> <li>Refer to the Filebin documentation for more information about Filebin.</li> </ul>"},{"location":"adapters/fs/","title":"Local Filesystem Adapter","text":"<p>The <code>file_keeper:fs</code> adapter allows you to use your local filesystem for storing and retrieving files. This adapter is useful for testing, development, or scenarios where you need to store files locally.</p>"},{"location":"adapters/fs/#overview","title":"Overview","text":"<p>This adapter provides a simple way to interact with the local filesystem. You'll need to specify the base path where files will be stored.</p>"},{"location":"adapters/fs/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the local filesystem adapter:</p> <pre><code>from file_keeper import make_storage\n\nstorage = make_storage(\"my_local_storage\", {\n    \"type\": \"file_keeper:fs\",\n    \"path\": \"/tmp/file-keeper\",  # Replace with your desired base path\n    \"initialize\": True,  # Optional: Create the directory if it doesn't exist\n})\n\n# Now you can use the 'storage' object to upload, download, and manage files.\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace <code>/tmp/file-keeper</code> with the desired base path on your system.</li> <li>The <code>initialize</code> option (defaulting to <code>False</code>) determines whether the adapter should attempt to create the specified directory if it doesn't exist. If <code>initialize</code> is <code>True</code> and the directory cannot be created (e.g., due to permissions issues), an error will be raised.</li> <li>Ensure that the process running file-keeper has the necessary permissions to read and write to the specified directory.</li> </ul>"},{"location":"adapters/gcs/","title":"Google Cloud Storage","text":"<p>The <code>file_keeper:gcs</code> adapter allows you to use Google Cloud Storage for storing and retrieving files. This adapter leverages the <code>google-cloud-storage</code> Python library.</p>"},{"location":"adapters/gcs/#overview","title":"Overview","text":"<p>This adapter provides a convenient way to integrate Google Cloud Storage with file-keeper. You'll need to have the <code>google-cloud-storage</code> library installed (<code>pip install google-cloud-storage</code>) and configure it with the appropriate credentials for your Google Cloud project.</p>"},{"location":"adapters/gcs/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the Google Cloud Storage adapter:</p> <pre><code>from file_keeper import make_storage\n\nstorage = make_storage(\"my_gcs_storage\", {\n    \"type\": \"file_keeper:gcs\",\n    \"project\": \"your-gcp-project-id\",  # Replace with your Google Cloud project ID\n    \"bucket_name\": \"your-gcs-bucket-name\",  # Replace with your GCS bucket name\n    \"credentials_path\": \"/path/to/your/credentials.json\",  # Replace with the path to your service account key file\n})\n\n# Now you can use the 'storage' object to upload, download, and manage files.\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values (project, bucket_name, credentials_path) with your actual Google Cloud project ID, GCS bucket name, and the path to your service account key file.</li> <li>Ensure that you have created a bucket in your Google Cloud Storage account to store the files.</li> <li>For enhanced security, it's recommended to use a service account with limited permissions.</li> <li>Refer to the Google Cloud Storage documentation for more information about Google Cloud Storage.</li> </ul>"},{"location":"adapters/libcloud/","title":"Apache Libcloud Adapter","text":"<p>The <code>file_keeper:libcloud</code> adapter allows you to use Apache Libcloud to connect to a wide range of cloud storage providers. Libcloud provides a unified interface for interacting with various storage services, simplifying integration and reducing vendor lock-in.</p>"},{"location":"adapters/libcloud/#overview","title":"Overview","text":"<p>This adapter leverages the Libcloud library to provide storage functionality. You'll need to have Libcloud installed and configure it with the appropriate credentials for your chosen provider.</p> <pre><code>pip install 'file-keeper[libcloud]'\n\n## or\n\npip install apache-libcloud\n</code></pre>"},{"location":"adapters/libcloud/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the Libcloud adapter:</p> AWS S3MinIOAzurite <pre><code>storage = make_storage(\"my_libcloud_storage\", {\n    \"type\": \"file_keeper:libcloud\",\n    \"provider\": \"S3\",\n    \"key\": \"***\",\n    \"secret\": \"***\",\n    \"container_name\": \"file-keeper\",\n})\n</code></pre> <pre><code>storage = make_storage(\"my_libcloud_storage\", {\n    \"type\": \"file_keeper:libcloud\",\n    \"provider\": \"MINIO\",\n    \"key\": \"minioadmin\",\n    \"secret\": \"minioadmin\",\n    \"container_name\": \"file-keeper\",\n    \"params\": {\n        \"host\": \"127.0.0.1\",\n        \"port\": 9000,\n        \"secure\": False,\n    },\n})\n</code></pre> <pre><code>storage = make_storage(\"my_libcloud_storage\", {\n    \"type\": \"file_keeper:libcloud\",\n    \"provider\": \"AZURE_BLOBS\",\n    \"key\": \"devstoreaccount1\",\n    \"secret\": \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n    \"container_name\": \"file-keeper\",\n    \"params\": {\n        \"host\": \"127.0.0.1\",\n        \"port\": 10000,\n        \"secure\": False,\n    },\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values (provider, host, port, secure, key, secret,     container_name) with your actual provider credentials and configuration.</li> <li>Refer to the Apache Libcloud     documentation for a complete list of     supported providers and their specific configuration options.</li> <li>Ensure that the necessary Libcloud drivers are installed for your chosen     provider.</li> </ul>"},{"location":"adapters/memory/","title":"In-Memory Storage Adapter","text":"<p>The <code>file_keeper:memory</code> adapter provides a simple in-memory storage solution. This adapter is primarily intended for testing and development purposes, as data is not persisted to disk.</p>"},{"location":"adapters/memory/#overview","title":"Overview","text":"<p>This adapter stores files entirely in memory, making it very fast but also volatile. Data is lost when the application exits. It's a convenient way to simulate a storage backend without requiring external dependencies.</p>"},{"location":"adapters/memory/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the in-memory storage adapter:</p> <pre><code>from file_keeper import make_storage\n\nstorage = make_storage(\"my_memory_storage\", {\n    \"type\": \"file_keeper:memory\",\n})\n\n# Now you can use the 'storage' object to upload, download, and manage files.\n</code></pre> <p>Important Notes:</p> <ul> <li>No configuration options are required for this adapter.</li> <li>All data is stored in memory and will be lost when the application terminates.</li> <li>This adapter is not suitable for production environments where data persistence is required.</li> </ul>"},{"location":"adapters/null/","title":"Null Adapter","text":"<p>The <code>file_keeper:null</code> adapter provides a no-op storage solution. It does not actually store any files and is primarily intended for testing or situations where you want to disable storage functionality.</p>"},{"location":"adapters/null/#overview","title":"Overview","text":"<p>This adapter implements all the required storage interfaces but performs no real operations. Any attempt to upload, download, or manage files will be silently ignored. It's a useful tool for isolating issues or running tests without interacting with external storage.</p>"},{"location":"adapters/null/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the null adapter:</p> <pre><code>from file_keeper import make_storage\n\nstorage = make_storage(\"my_null_storage\", {\n    \"type\": \"file_keeper:null\",\n})\n\n# Now you can use the 'storage' object, but all operations will be no-ops.\n</code></pre> <p>Important Notes:</p> <ul> <li>No configuration options are required for this adapter.</li> <li>All operations are silently ignored.</li> <li>This adapter is not suitable for production environments where data persistence is required.</li> </ul>"},{"location":"adapters/opendal/","title":"Apache OpenDAL","text":"<p>The <code>file_keeper:opendal</code> adapter allows you to use Apache OpenDAL for storing and retrieving files. OpenDAL provides a unified interface for interacting with various object storage systems and local filesystems.</p>"},{"location":"adapters/opendal/#overview","title":"Overview","text":"<p>This adapter leverages the OpenDAL library to provide storage functionality. You'll need to have OpenDAL installed and configure it with the appropriate credentials and configuration for your chosen storage provider.</p> <pre><code>pip install 'file-keeper[opendal]'\n\n## or\n\npip install opendal\n</code></pre>"},{"location":"adapters/opendal/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the OpenDAL adapter:</p> MinIOAzuriteFake GCS <pre><code>storage = make_storage(\"my_opendal_storage\", {\n     \"type\": \"file_keeper:opendal\",\n     \"scheme\": \"s3\",\n     \"params\": {\n         \"bucket\": \"file-keeper\",\n         \"access_key_id\": \"minioadmin\",\n         \"secret_access_key\": \"minioadmin\",\n         \"endpoint\": \"http://127.0.0.1:9000\",\n         \"region\": \"auto\"\n     },\n })\n</code></pre> <pre><code>storage = make_storage(\"my_opendal_storage\", {\n     \"type\": \"file_keeper:opendal\",\n     \"scheme\": \"azblob\",\n     \"params\": {\n         \"container\": \"file-keeper\",\n         \"account_name\": \"devstoreaccount1\",\n         \"account_key\": \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n         \"endpoint\": \"http://127.0.0.1:10000/devstoreaccount1\",\n     },\n })\n</code></pre> <pre><code>storage = make_storage(\"my_opendal_storage\", {\n     \"type\": \"file_keeper:opendal\",\n     \"scheme\": \"gcs\",\n     \"params\": {\n         \"bucket\": \"file-keeper\",\n         \"endpoint\": \"http://127.0.0.1:4443\",\n     },\n })\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the params with your actual OpenDAL location and credentials.</li> <li>Refer to the Apache OpenDAL     documentation for a complete list of     supported storage systems and their specific configuration options.</li> <li>Ensure that you have the necessary credentials and permissions to access     the specified storage location.</li> </ul>"},{"location":"adapters/redis/","title":"Redis Adapter","text":"<p>The <code>file_keeper:redis</code> adapter allows you to use Redis as a storage backend. This adapter stores files as binary data within Redis keys.</p>"},{"location":"adapters/redis/#overview","title":"Overview","text":"<p>This adapter provides a simple way to integrate Redis with file-keeper. You'll need to have a running Redis instance and the <code>redis</code> Python library installed (<code>pip install redis</code>).  This adapter is suitable for smaller files and scenarios where fast access is critical.</p>"},{"location":"adapters/redis/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the Redis adapter:</p> <pre><code>from file_keeper import make_storage\n\nstorage = make_storage(\"my_redis_storage\", {\n    \"type\": \"file_keeper:redis\",\n    \"host\": \"localhost\",  # Replace with your Redis host\n    \"port\": 6379,          # Replace with your Redis port\n    \"db\": 0,               # Replace with your Redis database number\n    \"password\": \"\",        # Replace with your Redis password (if any)\n    \"bucket\": \"file-keeper\",  # Replace with your desired bucket name\n})\n\n# Now you can use the 'storage' object to upload, download, and manage files.\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values (host, port, db, password, bucket) with your actual Redis configuration.</li> <li>Ensure that your Redis instance is running and accessible.</li> <li>Consider the limitations of storing large files in Redis, as it is an in-memory data store.</li> <li>The <code>bucket</code> parameter is used as a prefix for all keys in Redis, helping to organize your files.</li> </ul>"},{"location":"adapters/s3/","title":"AWS S3 Adapter","text":"<p>The <code>file_keeper:s3</code> adapter allows you to use Amazon S3 for storing and retrieving files. This adapter leverages the <code>boto3</code> Python library.</p>"},{"location":"adapters/s3/#overview","title":"Overview","text":"<p>This adapter provides a convenient way to integrate AWS S3 with file-keeper. You'll need to have the <code>boto3</code> library installed and configure it with the appropriate credentials for your AWS account.</p> <pre><code>pip install 'file-keeper[s3]'\n\n## or\n\npip install boto3\n</code></pre>"},{"location":"adapters/s3/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the S3 adapter:</p> AWS S3MinIO <pre><code>storage = make_storage(\"my_s3_storage\", {\n    \"type\": \"file_keeper:s3\",\n    \"key\": \"***\",\n    \"secret\": \"***\",\n    \"bucket\": \"file-keeper\",\n})\n</code></pre> <pre><code>storage = make_storage(\"my_s3_storage\", {\n    \"type\": \"file_keeper:s3\",\n    \"key\": \"minioadmin\",\n    \"secret\": \"minioadmin\",\n    \"bucket\": \"file-keeper\",\n    \"endpoint\": \"http://127.0.0.1:9000/\",\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values (bucket, region, access_key_id,     secret_access_key) with your actual AWS account credentials and     configuration.</li> <li>Ensure that you have created a bucket in your AWS S3 account to store the     files.</li> <li>For enhanced security, consider using IAM roles instead of hardcoding     access keys.</li> <li>Refer to the AWS S3 documentation for more     information about AWS S3.</li> </ul>"},{"location":"adapters/sqlalchemy/","title":"SQLAlchemy Adapter","text":"<p>The <code>file_keeper:sqlalchemy</code> adapter allows you to use a relational database managed by SQLAlchemy for storing and retrieving file metadata and content. This adapter stores file content as binary data within the database.</p>"},{"location":"adapters/sqlalchemy/#overview","title":"Overview","text":"<p>This adapter provides a way to integrate a relational database with file-keeper. You'll need to have SQLAlchemy installed (<code>pip install sqlalchemy</code>) and a configured database connection string.  This adapter is suitable for scenarios where you need to store file metadata alongside the file content in a structured manner.</p>"},{"location":"adapters/sqlalchemy/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the SQLAlchemy adapter:</p> <pre><code>from file_keeper import make_storage\n\nstorage = make_storage(\"my_sqlalchemy_storage\", {\n    \"type\": \"file_keeper:sqlalchemy\",\n    \"url\": \"postgresql://user:password@host:port/database\",  # Replace with your database connection string\n    \"table_name\": \"files\",  # Replace with the name of the table to store files\n})\n\n# Now you can use the 'storage' object to upload, download, and manage files.\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values (url, table_name) with your actual database connection string and table name.</li> <li>Ensure that the specified table exists in your database. The adapter will create the table if it doesn't exist, but you may need to adjust the schema based on your specific requirements.</li> <li>The database connection string should be in a format supported by SQLAlchemy.  Refer to the SQLAlchemy documentation for details.</li> <li>Consider the performance implications of storing large files directly in the database.</li> </ul>"},{"location":"adapters/zip/","title":"ZIP Archive Adapter","text":"<p>The <code>file_keeper:zip</code> adapter allows you to use a ZIP archive as a storage backend. This adapter is useful for packaging and distributing files in a single archive.</p>"},{"location":"adapters/zip/#overview","title":"Overview","text":"<p>This adapter stores files within a ZIP archive. You'll need to provide the path to the ZIP archive.  The adapter can create the ZIP archive if it doesn't exist.</p>"},{"location":"adapters/zip/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the ZIP archive adapter:</p> <pre><code>from file_keeper import make_storage\n\nstorage = make_storage(\"my_zip_storage\", {\n    \"type\": \"file_keeper:zip\",\n    \"path\": \"/tmp/my_archive.zip\",  # Replace with your desired ZIP archive path\n    \"create\": True,  # Optional: Create the ZIP archive if it doesn't exist\n})\n\n# Now you can use the 'storage' object to upload, download, and manage files.\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace <code>/tmp/my_archive.zip</code> with the desired path to your ZIP archive.</li> <li>The <code>create</code> option (defaulting to <code>False</code>) determines whether the adapter should attempt to create the ZIP archive if it doesn't exist. If <code>create</code> is <code>True</code> and the ZIP archive cannot be created (e.g., due to permissions issues), an error will be raised.</li> <li>This adapter is suitable for smaller files, as ZIP archives can become slow to process with a large number of files.</li> </ul>"},{"location":"core_concepts/capabilities/","title":"Capabilities","text":"<p>The Capability enum defines the features supported by different storage backends. It's a fundamental concept in file-keeper, enabling the system to adapt to the limitations and strengths of each storage provider. This document explains the Capability enum, how to check for support, and provides practical examples.</p>"},{"location":"core_concepts/capabilities/#what-are-capabilities","title":"What are Capabilities?","text":"<p>Not all storage systems are created equal. Some support resumable uploads, while others don't. Some offer efficient copy and move operations, while others require you to handle this type of tasks yourself. Capabilities provide a standardized way to describe what a particular storage backend can do.</p> <p>The Capability enum is a set of flags, each representing a specific feature.  By checking which capabilities a Storage object possesses, file-keeper can dynamically adjust its behavior to ensure compatibility and optimal performance.</p>"},{"location":"core_concepts/capabilities/#the-capability-enum","title":"The Capability Enum","text":"<p>Here's a breakdown of the key capabilities:</p> Capability Description ANALYZE Return file details from the storage. APPEND Add content to the existing file. COMPOSE Combine multiple files into a new one in the same storage. COPY Make a copy of the file inside the same storage. CREATE Create a file as an atomic object. EXISTS Check if file exists. LINK_PERMANENT Make permanent download link. LINK_TEMPORAL Make expiring download link. LINK_ONE_TIME Make one-time download link. MOVE Move file to a different location inside the same storage. MULTIPART Create file in 3 stages: initialize, upload(repeatable), complete. RANGE Return specific range of bytes from the file. REMOVE Remove file from the storage. RESUMABLE Perform resumable uploads that can be continued after interruption. SCAN Iterate over all files in the storage. SIGNED Generate signed URL for specific operation. STREAM Return file content as stream of bytes."},{"location":"core_concepts/capabilities/#checking-for-capability-support","title":"Checking for Capability Support","text":"<p>You can determine if a Storage object supports a specific capability using the supports() method:</p> <pre><code>from file_keeper import Capability, make_storage, make_upload\n\nstorage = make_storage(\"my_storage\", {\"adapter\": \"s3\", \"region\": \"us-east-1\"})\n\nif storage.supports(Capability.REMOVE | Capability.CREATE):\n    upload = make_upload(b\"hello world\")\n    info = storage.upload(\"hello.txt\", upload)\n    storage.remove(info)\n\nelse:\n    raise TypeError(\"File creation and removal is not supported by this storage.\")\n\n\nif storage.supports(Capability.EXISTS):\n    assert not storage.exists(info)\n\nelse:\n    print(\"File existence cannot be checked by this storage.\")\n</code></pre> <p>The <code>supports()</code> method returns <code>True</code> if the storage backend has the specified capability, and <code>False</code> otherwise.</p>"},{"location":"core_concepts/data_model/","title":"Data model","text":"<p>This document details the core data models used within the file-keeper system: Storage, FileData, and Location. It explains the why behind these concepts and how they work together to provide a flexible and reliable file storage solution.</p>"},{"location":"core_concepts/data_model/#storage","title":"Storage","text":"<p>The Storage class is the central point of interaction with any underlying storage system \u2013 whether that's a cloud provider like AWS S3 or Google Cloud Storage, a local filesystem, or something else entirely. Think of it as an adapter that translates file-keeper's generic requests into the specific language of the storage backend.</p> <p>Instead of directly dealing with the complexities of each storage system, you interact with Storage objects.  file-keeper handles the details of connecting, authenticating, and performing operations.  Each Storage instance is equipped with specialized services \u2013 Uploader, Reader, and Manager \u2013 to handle different types of tasks.  This separation of concerns makes the system more modular and easier to extend.</p>"},{"location":"core_concepts/data_model/#filedata","title":"FileData","text":"<p>FileData represents a file as known to file-keeper. It's a metadata record that contains everything we need to identify and manage a file, regardless of where it's physically stored.</p> <p>Crucially, FileData is independent of the underlying storage. It's a consistent representation that allows file-keeper to work with different storage backends seamlessly.  It tracks essential information like the file's name, size, content type, and content hash.  It can also store additional, storage-specific metadata in the storage_data field.</p> <p>FileData is the key to operations like tracking progress during uploads, managing file versions, and providing consistent access to file information across different storage systems.</p>"},{"location":"core_concepts/data_model/#location","title":"Location","text":"<p>Location represents the address of a file within a specific storage system.  It's a simple string, but it's given a specific meaning within file-keeper.</p> <p>Think of it as a path or key that uniquely identifies a file within its storage backend.  The format of a Location will vary depending on the storage system (e.g., an S3 key, a filesystem path).</p> <p>file-keeper uses Location objects to tell the Storage adapter where to find a file.  It also provides a mechanism for transforming Location objects to different formats if needed, allowing for flexibility and compatibility with different storage systems.</p>"},{"location":"core_concepts/uploads/","title":"Upload","text":"<p>The Upload class represents the data you want to store in a storage backend. It's a key component in file-keeper, encapsulating the file's content, metadata, and instructions for how to transfer it to storage. This document explains how to create and use Upload objects, covering streaming uploads and hashing for data integrity.</p>"},{"location":"core_concepts/uploads/#what-is-an-upload","title":"What is an Upload?","text":"<p>Think of an Upload as a package containing everything needed to send a file to storage. It's more than just the raw data; it includes information about the file itself, like its name, size, and content type. This metadata is essential for proper storage and retrieval.</p> <p>The Upload object decouples the source of the data from the transfer process. This allows file-keeper to handle various data sources \u2013 files on disk, in-memory buffers, network streams \u2013 without changing the core storage logic.</p>"},{"location":"core_concepts/uploads/#creating-an-upload-object","title":"Creating an Upload Object","text":"<p>You can create an Upload object in several ways, depending on the source of your data. The recommended way is using make_upload helper:</p> File-like objectByte stringWerkzeug's FileStorageManually <p>If you have an open file, you can directly pass it to the make_upload function. file-keeper will handle reading the data from the file.</p> <pre><code>src = open(\"my_image.jpg\", \"rb\")\nupload = make_upload(src)\n</code></pre> <p>If your data is already in memory as a byte string, you can pass it directly.</p> <pre><code>data = b\"This is the content of my file.\"\nupload = make_upload(data)\n</code></pre> <p>When writing an application using werkzeug-based framework you can handle uploaded files in this way.</p> <pre><code>from werkzeug.datastructures import FileStorage\n\ndata = FileStorage(..., \"my_data.txt\")\nupload = make_upload(data)\n</code></pre> <p>This is useful for large files that don't fit in memory. You need to provide an object that has methods <code>read</code> and <code>__iter__</code> producing byte string as a first argument to Upload class. If you have a generator that yields data, wrap it into IterableBytesReader instead of manually implementing class with required methods:</p> <pre><code>from file_keeper import Upload, IterableBytesReader\n\ndef data_generator():\n    yield b\"hello\"\n    yield b\" \"\n    yield b\"world\"\n\nstream = IterableBytesReader(data_generator())\nupload = Upload(stream, \"my_file.txt\", 11, \"text/plain\")\n</code></pre> <p>The make_upload function automatically determines the file size and content type for supported source types.</p>"},{"location":"core_concepts/uploads/#streaming-uploads","title":"Streaming Uploads","text":"<p>For very large files, loading the entire content into memory is impractical. Streaming uploads allow you to send the data in chunks, reducing memory usage and improving performance.</p> <p>As shown in the example above, you can create an Upload object from an iterable of bytes. file-keeper will then stream the data to the storage backend as it becomes available. This is the preferred method for handling large files, but it expects that you compute the size and content type of the upload in advance and provide these details to the Upload constructor.</p>"},{"location":"core_concepts/uploads/#hashing-for-data-integrity","title":"Hashing for Data Integrity","text":"<p>Data integrity is crucial when transferring files. Hashing ensures that the data you upload is exactly the same as the data stored in the backend. file-keeper automatically calculates a hash of the upload data during the transfer process.</p> <p>The calculated hash is stored as part of the FileData metadata. When you retrieve the file, file-keeper can recalculate the hash via Storage.analyze method to verify data integrity. If the hashes don't match, it indicates that the file has been corrupted or tampered with.</p> <p>Hashing is performed transparently during the upload process, so you don't need to worry about implementing it yourself. It provides an extra layer of assurance that your data is stored reliably.</p>"},{"location":"extending/","title":"Extending file-keeper","text":"<p>file-keeper is designed to be highly extensible, allowing you to add new storage backends, upload factories, and location transformers without modifying the core library. This is achieved through the use of the Pluggy framework.</p>"},{"location":"extending/#overview","title":"Overview","text":"<p>file-keeper uses <code>pluggy</code> to discover and register extensions. To extend file-keeper's capabilities, you need to create a Python package that provides entry points for the <code>file_keeper_ext</code> hook specification.  Within your extension package, you can define functions decorated with <code>@file_keeper.hookimpl</code> to override or augment existing functionality.</p>"},{"location":"extending/#available-extension-points","title":"Available extension points","text":"<p>The following extension points are currently available:</p> Hook Description <code>register_adapters</code> Use this function to register new storage adapters. The <code>registry</code> object allows you to add your <code>Storage</code> class to the list of available storage options. <code>register_upload_factories</code> Use this function to register new upload factories. The <code>registry</code> object allows you to add your <code>UploadFactory</code> class to the list of available upload factories. <code>register_location_transformers</code> Use this function to register new location transformers. The <code>registry</code> object allows you to add your <code>LocationTransformer</code> function to the list of available location transformers."},{"location":"extending/#example-registering-a-custom-storage-adapter","title":"Example: registering a custom storage adapter","text":"<p>Let's say you want to add a new storage adapter that stores files in a local directory.  Here's how you would do it:</p> <ol> <li> <p>Create a new Python package (<code>my_storage_extension</code>)</p> </li> <li> <p>Inside your package, create a module ( <code>my_storage.py</code>) with the following content:</p> <pre><code>import file_keeper as fk\n\nclass MyLocalStorage(fk.Storage):\n    ...\n\n@fk.hookimpl\ndef register_adapters(registry):\n    registry.register(\"my_local\", MyLocalStorage)\n</code></pre> </li> <li> <p>Add an entry point to your <code>setup.py</code> or <code>pyproject.toml</code> file:</p> setup.pypyproject.toml <pre><code>from setuptools import setup\n\nsetup(\n    ...,\n    entry_points={\n        \"file_keeper_ext\": [\"my_storage_extension = my_storage\"],\n    },\n)\n</code></pre> <pre><code>...\n\n[project.entry-points.file_keeper_ext]\nmy_storage_extension = \"my_storage\"\n</code></pre> </li> </ol> <p>Now, when file-keeper discovers your extension, it will register <code>MyLocalStorage</code> as a new storage option, accessible by the name \"my_local\".</p>"},{"location":"extending/#example-registering-a-custom-location-transformer","title":"Example: registering a custom location transformer","text":"<p>Let's say you want to add a location transformer that prepends a prefix to all locations.</p> <ol> <li> <p>Create a new Python package</p> </li> <li> <p>Inside your package, create a file  with the following content:</p> <pre><code>import file_keeper as fk\n\ndef my_location_transformer(location: str, upload, extras: dict[str, any]) -&gt; str:\n    return f\"prefix_{location}\"\n\n@fk.hookimpl\ndef register_location_transformers(registry: Registry[types.LocationTransformer]):\n    registry.register(\"my_prefix\", my_location_transformer)\n</code></pre> </li> <li> <p>Add an entry point to your <code>setup.py</code> or <code>pyproject.toml</code> file</p> </li> </ol> <p>Now, when file-keeper discovers your extension, it will register a new location transformer, accessible by the name \"my_prefix\".</p>"},{"location":"extending/adapters/","title":"Storage adapters","text":"<p>The core of file-keeper's flexibility lies in its storage adapters. Adapters encapsulate the logic for interacting with a specific storage system, allowing file-keeper to remain agnostic to the underlying implementation.  To create a custom adapter, you'll need to define a class that inherits from Storage and implements its services.</p>"},{"location":"extending/adapters/#steps-to-create-a-custom-adapter","title":"Steps to create a custom adapter","text":""},{"location":"extending/adapters/#define-a-settings-class","title":"Define a Settings class","text":"<p>Create a dataclass to hold the configuration options for your storage adapter. This class should inherit from Settings.  This allows file-keeper to handle validation and default values.</p> <p>Example</p> <pre><code>from dataclasses import dataclass\nimport file_keeper as fk\n\n@dataclass\nclass MyStorageSettings(fk.Settings):\n    api_key: str = \"\"\n    endpoint: str = \"\"\n</code></pre>"},{"location":"extending/adapters/#extend-the-storage-class","title":"Extend the Storage class","text":"<p>Create a class that inherits from Storage and sets the SettingsFactory class attribute to your settings class. It also sets UploaderFactory and ReaderFactory in the same way. The implementation will follow soon.</p> <p>Example</p> <pre><code>...\n\nclass MyStorage(fk.Storage):\n    settings: MyStorageSettings\n\n    SettingsFactory = MyStorageSettings\n    UploaderFactory = MyUploader\n    ReaderFactory = MyReader\n</code></pre>"},{"location":"extending/adapters/#implement-uploader-and-reader-services","title":"Implement Uploader and Reader services","text":"<p>Create classes for UploaderFactory and ReaderFactory that inherit from Uploader and Reader respectively. These classes will contain the logic for uploading and reading files to and from your storage system.</p> <p>Make sure to add CREATE capability to <code>Uploader</code> and STREAM capability to <code>Reader</code>. Otherwise storage will pretend that these services do not support these operations</p> <p>Example</p> <pre><code>class MyUploader(fk.Uploader):\n\n    capabilities = fk.Capability.CREATE\n\n    def upload(self, location: fk.Location, upload: fk.Upload, extras: dict[str, Any]) -&gt; fk.FileData:\n        # Implement your upload logic here\n        reader = upload.hashing_reader()\n        for chunk in reader:\n            # send fragment to storage\n\n        return fk.FileData(location, upload.size, upload.content_type, reader.get_hash())\n\n\nclass MyReader(fk.Reader):\n\n    capabilities = fk.Capability.STREAM\n\n    def stream(self, data: fk.FileData, extras: dict[str, Any]) -&gt; Iterable[bytes]:\n        # Implement your streaming logic here\n        for chunk in file_stream:\n            yield chunk\n</code></pre>"},{"location":"extending/adapters/#register-the-adapter","title":"Register the adapter","text":"<p>If you are going to use custom adapter only inside a single script, you can register it directly using <code>adapters</code> registry:</p> <pre><code>fk.adapters.register(\"local\", MyStorage)\n</code></pre> <p>But if you are writing a library that will be used accross multiple project it's better to register storage using entrypoints of the python package.</p> <p>First, create a function with name <code>register_adapters</code> and decorate it with <code>hookimpl</code> decorator. This function is responsible to register the adapter</p> <pre><code>@fk.hookimpl\ndef register_adapters(registry: fk.Registry[type[fk.Storage]]):\n    registry.register(\"local\", MyStorage)\n</code></pre> <p>Next, add an entry-point named <code>file_keeper_ext</code> pointing to the module with <code>register_adapters</code> to you package's definition. For example, if you are using <code>pyproject.toml</code> add following lines there:</p> <pre><code>[project.entry-points.\"file_keeper_ext\"]\nmy_extension = \"my_module.my_storage\"\n</code></pre> <p>Finally, re-install the your package, to refresh entrypoints. From now on your custom adapter will be available just as any adapter shipped inside file-keeper as soon as you install your library.</p>"},{"location":"extending/adapters/#initialize-the-adapter","title":"Initialize the adapter","text":"<p>Now you can use your custom adapter:</p> <pre><code>storage = fk.make_storage(\"local\", {\n    \"adapter\": \"local\",\n    \"api_key\": \"123\",\n    \"endpoint\": \"https://example.local\",\n})\n</code></pre> <p>This is a basic example, but it demonstrates the fundamental principles of creating a custom storage adapter.  You can extend this example to support more complex features and integrate with a wider range of storage systems.</p>"},{"location":"extending/location_transformers/","title":"Location transformers","text":"<p>Location transformers allow you to modify the Location string before it's used to access a file in the storage backend. This is useful for scenarios where you need to perform additional processing or formatting on the location, such as adding prefixes, encoding characters, or generating unique identifiers.</p>"},{"location":"extending/location_transformers/#what-are-location-transformers","title":"What are location transformers?","text":"<p>A location transformer is a callable (usually a function) that takes the original Location string, optional Upload or FileData object, and any extra data as input, and returns a modified Location string. They provide a flexible way to customize how locations are handled by file-keeper.</p> <p>Location transformers are set per-storage via location_transformers option. To apply them call prepare_location() method.</p> <p>Example</p> <pre><code>storage = make_storage(\"test\", {\n    \"type\": \"file_keeper:fs\",\n    \"location_transformers\": [\"safe_relative_path\"],\n    \"path\": \"/tmp\",\n})\n\nunsafe_location = \"../etc/passwd\"\n\nsafe_location = storage.prepare_location(unsafe_location)\n\nstorage.upload(safe_location, ...)\n</code></pre>"},{"location":"extending/location_transformers/#steps-to-create-a-custom-location-transformer","title":"Steps to create a custom location transformer","text":""},{"location":"extending/location_transformers/#define-your-transformer","title":"Define your transformer","text":"<p>Create a function that accepts the Location, optional Upload or FileData, and <code>extras</code> as input and returns the transformed Location.</p> <pre><code>def my_location_transformer(location, data, extras):\n    # Perform custom transformation here\n    return \"prefix_\" + location\n</code></pre>"},{"location":"extending/location_transformers/#register-the-transformer","title":"Register the transformer","text":"<p>Use the <code>register_location_transformers</code> hook to register your transformer. This makes it available for use when creating or accessing files.</p> <pre><code>import file_keeper as fk\n\n@fk.hookimpl\ndef register_location_transformers(registry):\n    registry.register(\"my_transformer\", my_location_transformer)\n</code></pre>"},{"location":"extending/location_transformers/#using-your-custom-transformer","title":"Using Your Custom Transformer","text":"<p>To use your custom transformer, specify its name when creating a <code>Storage</code> object in the settings.</p> <pre><code>storage = make_storage(\"my_storage\", {\n    \"adapter\": \"s3\",\n    \"location_transformers\": [\"my_transformer\"],\n})\n</code></pre> <p>And apply <code>Storage.prepare_location</code> to original location:</p> <pre><code>transformed = storage.prepare_location(\"hello.txt\")\n\nassert transformed == \"prefix_hello.txt\"\n</code></pre>"}]}