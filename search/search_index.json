{"config":{"lang":["en"],"separator":"[\\s\\-\\.\\_]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to file-keeper","text":"<p>file-keeper provides an abstraction layer for storing and retrieving files, supporting various storage backends like local filesystems, cloud storage (S3, GCS), and more. It simplifies file management by providing a consistent API regardless of the underlying storage.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you're new to file-keeper, start with our Getting Started Tutorial which covers installation, basic configuration, and fundamental operations.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Unified API: Consistent interface across multiple storage backends</li> <li>Multiple Storage Backends: Support for file system, memory, S3, GCS, Azure, Redis, and more</li> <li>Type Safety: Comprehensive type annotations for better development experience</li> <li>Security: Built-in protection against directory traversal and other attacks</li> <li>Extensible: Plugin architecture for adding custom storage adapters</li> <li>Comprehensive Testing: Extensive test coverage with security-focused tests</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>You can install file-keeper using pip:</p> <pre><code>pip install file-keeper\n</code></pre> Additional dependencies <p>To use specific storage adapters, you may need to install extra dependencies. Most standard adapters do not require extras, but some \u2013 like those interfacing with external cloud providers \u2013 do. Here's a table of available extras:</p> Storage Type Adapter Name Extras Driver AWS S3 <code>file_keeper:s3</code> <code>s3</code> boto3 Apache Libcloud <code>file_keeper:libcloud</code> <code>libcloud</code> apache-libcloud Apache OpenDAL <code>file_keeper:opendal</code> <code>opendal</code> opendal Azure Blob Storage <code>file_keeper:azure_blob</code> <code>azure</code> azure-storage-blob Google Cloud Storage <code>file_keeper:gcs</code> <code>gcs</code> google-cloud-storage Redis <code>file_keeper:redis</code> <code>redis</code> redis SQLAlchemy <code>file_keeper:sqlalchemy</code> <code>sqlalchemy</code> SQLAlchemy <p>For example, to install file-keeper with S3 support:</p> <pre><code>pip install 'file-keeper[s3]'\n</code></pre> <p>And if you are not sure which storage backend you will use, you can install all extras: <pre><code>pip install 'file-keeper[all]'\n</code></pre></p>"},{"location":"#basic-configuration-and-usage-fs-adapter","title":"Basic configuration and usage (FS adapter)","text":"<p>Example</p> <pre><code>from file_keeper import make_storage, make_upload\n\n# Create a storage instance.  The 'path' setting specifies the root directory\n# for storing files. 'initialize' will automatically create the directory\n# if it doesn't exist.\nstorage = make_storage(\n    \"my_fs_storage\",  # A name for your storage (for logging/debugging)\n    {\n        \"type\": \"file_keeper:fs\",\n        \"path\": \"/tmp/my_filekeeper_files\",\n        \"initialize\": True,\n    },\n)\n\n# Create an upload object from a byte string.\nupload = make_upload(b\"Hello, file-keeper!\")\n\n# Upload the file.  This returns a FileData object containing information\n# about the uploaded file.\nfile_data = storage.upload(\"my_file.txt\", upload)\n\n# Print the file data.\nprint(file_data)\n\n# The file is now stored in /tmp/my_filekeeper_files/my_file.txt\n\n# Get the content of file using corresponding FileData object\ncontent: bytes = storage.content(file_data)\n</code></pre> <p>Explanation:</p> <ul> <li><code>make_storage()</code>: Creates a storage instance with the specified configuration.</li> <li><code>make_upload()</code>: Creates an <code>Upload</code> object from the data you want to store.</li> <li><code>storage.upload()</code>: Uploads the data to the storage.</li> <li><code>FileData</code>:  A dataclass that contains metadata about the uploaded file, including its location, size, content type, and hash.</li> <li><code>storage.content()</code>: Locates file using <code>FileData</code> and returns byte string with its content</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>This documentation is organized to help you find what you need:</p> <ul> <li>Tutorials: Step-by-step guides for beginners</li> <li>Core Concepts: Understanding the fundamental ideas</li> <li>Reference: Technical reference materials</li> <li>Storage Adapters: Specific information for each storage backend</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Utilities available for import directly from the <code>file_keeper</code> module.</p>"},{"location":"api/#file_keeper.make_storage","title":"<code>make_storage(name, settings)</code>","text":"<p>Initialize storage instance with specified settings.</p> <p>Storage adapter is defined by <code>type</code> key of the settings. The rest of settings depends on the specific adapter.</p> Example <pre><code>storage = make_storage(\"memo\", {\"type\": \"file_keeper:memory\"})\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>name of the storage</p> <p> TYPE: <code>str</code> </p> <code>settings</code> <p>configuration for the storage</p> <p> TYPE: <code>dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>Storage</code> <p>storage instance</p> RAISES DESCRIPTION <code>UnknownAdapterError</code> <p>storage adapter is not registered</p>"},{"location":"api/#file_keeper.list_adapters","title":"<code>list_adapters()</code>","text":"<p>List all registered storage adapters.</p> RETURNS DESCRIPTION <code>list[str]</code> <p>list of registered storage adapter types</p>"},{"location":"api/#file_keeper.get_storage","title":"<code>get_storage(name, settings=None)</code>","text":"<p>Get storage from the pool.</p> <p>If storage accessed for the first time, it's initialized and added to the pool. After that the same storage is returned every time the function is called with the given name.</p> <p>Settings are required only for initialization, so you can omit them if you are sure that storage exists. Additionally, if <code>settings</code> are not specified but storage is missing from the pool, file-keeper makes an attempt to initialize storage using global configuration. Global configuration can be provided as:</p> <ul> <li><code>FILE_KEEPER_CONFIG</code> environment variable that points to a file with configuration</li> <li><code>.file-keeper.json</code> in the current directory hierarchy</li> <li><code>file-keeper/file-keeper.json</code> in the user's config directory(usually,   <code>~/.config/</code>) when platformdirs   installed in the environment, for example via <code>pip install   'file-keeper[user_config]'</code> extras.</li> </ul> <p>File must contain storage configuration provided in format</p> <pre><code>{\n    \"storages\": {\n        \"my_storage\": {  # (1)!\n            \"type\": \"file_keeper:memory\"  # (2)!\n        }\n    }\n}\n</code></pre> <ol> <li>Name of the storage</li> <li>Options for the storage</li> </ol> <p>JSON configuration is used by default, because python has built-in JSON support. Additional file extensions are checked if environment contains corresponding package:</p> Package Extension tomllib <code>.toml</code> tomli <code>.toml</code> pyyaml <code>.yaml</code>, <code>.yml</code> <p>Extensions are checked in order <code>.toml</code>, <code>.yaml</code>, <code>.yml</code>, <code>.json</code>.</p> Example <p>If storage accessed for the first time, settings are required</p> <pre><code>&gt;&gt;&gt; storage = get_storage(\"memory\", {\"type\": \"file_keeper:memory\"})\n&gt;&gt;&gt; storage\n&lt;file_keeper.default.adapters.memory.MemoryStorage object at 0x...&gt;\n</code></pre> <p>and the same storage is returned every time in subsequent calls</p> <pre><code>&gt;&gt;&gt; cached = get_storage(\"memory\")\n&gt;&gt;&gt; storage is cached\nTrue\n</code></pre> <p>but if storage does not exist and settings are omitted, exception is raised</p> <pre><code>&gt;&gt;&gt; get_storage(\"new-memory\")\nTraceback (most recent call last):\n    ...\nfile_keeper.core.exceptions.UnknownStorageError: Storage new-memory is not configured\n</code></pre> PARAMETER DESCRIPTION <code>name</code> <p>name of the storage</p> <p> TYPE: <code>str</code> </p> <code>settings</code> <p>configuration for the storage</p> <p> TYPE: <code>dict[str, Any] | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Storage</code> <p>storage instance</p> RAISES DESCRIPTION <code>UnknownStorageError</code> <p>storage with the given name is not configured</p>"},{"location":"api/#file_keeper.make_upload","title":"<code>make_upload(value)</code>","text":"<p>Convert value into Upload object.</p> <p>Use this function for simple and reliable initialization of Upload object. Avoid creating Upload manually, unless you are 100% sure you can provide correct MIMEtype, size and stream.</p> Example <p>Bytes, binary streams, file objects, and a number of other types can be converted into Upload object.</p> <pre><code>&gt;&gt;&gt; upload = make_upload(b\"hello world\")\n</code></pre> <p>Upload object contains generic information about the file.</p> <pre><code>&gt;&gt;&gt; upload.size\n11\n&gt;&gt;&gt; upload.content_type\n'text/plain'\n</code></pre> <p>Unsupported types will raise TypeError.</p> <pre><code>&gt;&gt;&gt; make_upload(\"unicode string\")\nTraceback (most recent call last):\n  ...\nTypeError: &lt;class 'str'&gt; cannot be converted into Upload\n</code></pre> PARAMETER DESCRIPTION <code>value</code> <p>content of the file</p> <p> TYPE: <code>Any</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>if value cannot be converted into Upload object</p> RETURNS DESCRIPTION <code>Upload</code> <p>upload object with specified content</p>"},{"location":"api/#file_keeper.Storage","title":"<code>Storage</code>","text":"<p>Base class for storage implementation.</p> PARAMETER DESCRIPTION <code>settings</code> <p>storage configuration</p> <p> TYPE: <code>Mapping[str, Any] | Settings</code> </p> Example <p>Extend base class to implement custom storage <pre><code>class MyStorage(Storage):\n    SettingsFactory = Settings\n    UploaderFactory = Uploader\n    ManagerFactory = Manager\n    ReaderFactory = Reader\n</code></pre> then initialize it using required settings <pre><code>my_storage = MyStorage({\"option\": \"value\"})\n</code></pre></p>"},{"location":"api/#file_keeper.Storage.SettingsFactory","title":"<code>SettingsFactory = Settings</code>  <code>class-attribute</code>","text":"<p>Factory class for storage settings.</p>"},{"location":"api/#file_keeper.Storage.UploaderFactory","title":"<code>UploaderFactory = Uploader</code>  <code>class-attribute</code>","text":"<p>Factory class for uploader service.</p>"},{"location":"api/#file_keeper.Storage.ManagerFactory","title":"<code>ManagerFactory = Manager</code>  <code>class-attribute</code>","text":"<p>Factory class for manager service.</p>"},{"location":"api/#file_keeper.Storage.ReaderFactory","title":"<code>ReaderFactory = Reader</code>  <code>class-attribute</code>","text":"<p>Factory class for reader service.</p>"},{"location":"api/#file_keeper.Storage.capabilities","title":"<code>capabilities = self.compute_capabilities()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Operations supported by storage. Computed from capabilities of services during storage initialization.</p>"},{"location":"api/#file_keeper.Storage.hidden","title":"<code>hidden = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Flag that marks unsafe/experimental storages.</p>"},{"location":"api/#file_keeper.Storage.make_uploader","title":"<code>make_uploader()</code>","text":"<p>Initialize uploader service.</p>"},{"location":"api/#file_keeper.Storage.make_manager","title":"<code>make_manager()</code>","text":"<p>Initialize manager service.</p>"},{"location":"api/#file_keeper.Storage.make_reader","title":"<code>make_reader()</code>","text":"<p>Initialize reader service.</p>"},{"location":"api/#file_keeper.Storage.configure","title":"<code>configure(settings)</code>  <code>classmethod</code>","text":"<p>Initialize storage configuration.</p> <p>This method is responsible for transforming mapping with options into storage's settings. It also can initialize additional services and perform extra work, like verifying that storage is ready to accept uploads.</p> PARAMETER DESCRIPTION <code>settings</code> <p>mapping with storage configuration</p> <p> TYPE: <code>Mapping[str, Any] | Settings</code> </p> RETURNS DESCRIPTION <code>Settings</code> <p>initialized settings object</p>"},{"location":"api/#file_keeper.Storage.compute_capabilities","title":"<code>compute_capabilities()</code>","text":"<p>Computes the capabilities of the storage based on its services.</p> <p>Combines the capabilities of the uploader, manager, and reader services, then excludes any capabilities that are listed in the storage settings as disabled.</p> RETURNS DESCRIPTION <code>Capability</code> <p>The combined capabilities of the storage.</p>"},{"location":"api/#file_keeper.Storage.supports","title":"<code>supports(operation)</code>","text":"<p>Check whether the storage supports operation.</p> PARAMETER DESCRIPTION <code>operation</code> <p>capability to check</p> <p> TYPE: <code>Capability</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if operation is supported, False otherwise</p>"},{"location":"api/#file_keeper.Storage.supports_synthetic","title":"<code>supports_synthetic(operation, dest)</code>","text":"<p>Check if the storage can emulate operation using other operations.</p> <p>This method checks if the storage can perform the specified operation using a combination of other supported operations, often in conjunction with a destination storage.</p> <p>Synthetic operations are not stable and may change in future. They are not considered when computing capabilities of the storage. There are two main reasons to use them:</p> <ul> <li>required operation involves two storage. For example, moving or   copying file from one storage to another.</li> <li>operation is not natively supported by the storage, but can be   emulated using other operations. For example,   RANGE capability means that storage   can return specified slice of the file. When this capability is not   natively supported, storage can use   STREAM capability to read the whole   file, returning only specified fragment. This is not efficient but   still can solve certain problems.</li> </ul> PARAMETER DESCRIPTION <code>operation</code> <p>capability to check</p> <p> TYPE: <code>Capability</code> </p> <code>dest</code> <p>destination storage for operations that require it</p> <p> TYPE: <code>Storage</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if operation is supported, False otherwise</p>"},{"location":"api/#file_keeper.Storage.full_path","title":"<code>full_path(location, /, **kwargs)</code>","text":"<p>Compute path to the file from the storage's root.</p> <p>This method works as a shortcut for enabling <code>path</code> option. Whenever your custom storage works with location provided by user, wrap this location into this method to get full path:</p> <pre><code>class MyCustomReader:\n    def stream(self, data: FileData, extras):\n        real_location = self.storage.full_path(data_location)\n        ...\n</code></pre> PARAMETER DESCRIPTION <code>location</code> <p>location of the file object</p> <p> TYPE: <code>Location</code> </p> <code>**kwargs</code> <p>extra parameters for custom storages</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>full path required to access location</p> RAISES DESCRIPTION <code>LocationError</code> <p>when location is outside of the storage's path</p>"},{"location":"api/#file_keeper.Storage.prepare_location","title":"<code>prepare_location(location, sample=None, /, **kwargs)</code>","text":"<p>Transform and sanitize location using configured functions.</p> <p>This method applies all transformations configured in location_transformers setting to the provided location. Each transformer is called in the order they are listed in the setting. The output of the previous transformer is passed as an input to the next one.</p> Example <pre><code>location = storage.prepare_location(untrusted_location)\n</code></pre> PARAMETER DESCRIPTION <code>location</code> <p>initial location provided by user</p> <p> TYPE: <code>str</code> </p> <code>sample</code> <p>optional Upload object that can be used by transformers.</p> <p> TYPE: <code>Upload | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>additional parameters for transformers</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Location</code> <p>transformed location</p> RAISES DESCRIPTION <code>LocationTransformerError</code> <p>when transformer is not found</p>"},{"location":"api/#file_keeper.Storage.file_as_upload","title":"<code>file_as_upload(data, **kwargs)</code>","text":"<p>Make an Upload with file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object to wrap into Upload</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Upload</code> <p>Upload object with file content</p>"},{"location":"api/#file_keeper.Storage.upload","title":"<code>upload(location, upload, /, **kwargs)</code>","text":"<p>Upload file using single stream.</p> <p>Requires CREATE capability.</p> <p>This is the simplest way to upload file into the storage. It uses single stream to transfer the whole file. If upload fails, no file is created in the storage.</p> <p>Content is not modified during upload, it is written as-is. And content can be of size 0, which results in empty file in the storage.</p> <p>When file already exists, behavior depends on override_existing setting. If it is False, ExistingFileError is raised. If it is True, existing file is replaced with new content. In this case, it is possible to lose existing file if upload fails. When adapter does not support removal, but supports overrides, this can be used to wipe the content of the file.</p> <p>Location can contain nested path as long as it does not go outside of the path from settings. For example, if <code>path</code> is set to <code>/data</code>, location can be <code>file.txt</code> or <code>2024/file.txt</code> but not <code>../file.txt</code> or <code>/etc/passwd</code>. Violating this rule leads to LocationError.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the upload.</p> <p> TYPE: <code>Location</code> </p> <code>upload</code> <p>The Upload object containing the file data.</p> <p> TYPE: <code>Upload</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>FileData object with details about the uploaded file.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support CREATE operation</p> <code>ExistingFileError</code> <p>when file already exists and override_existing is False</p> <code>LocationError</code> <p>when location is outside of the storage's path</p>"},{"location":"api/#file_keeper.Storage.resumable_start","title":"<code>resumable_start(location, size, /, **kwargs)</code>","text":"<p>Prepare everything for resumable upload.</p> <p>Warning</p> <p>This operation is not stabilized yet.</p> <p>Requires RESUMABLE capability.</p> <p><code>content_type</code> and <code>hash</code> are optional. When any of those provided, it will be used to verify the integrity of the upload. If they are missing, the upload will be accepted without verification.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the upload.</p> <p> TYPE: <code>Location</code> </p> <code>size</code> <p>The total size of the upload in bytes.</p> <p> TYPE: <code>int</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>details about the upload.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support RESUMABLE operation</p>"},{"location":"api/#file_keeper.Storage.resumable_refresh","title":"<code>resumable_refresh(data, /, **kwargs)</code>","text":"<p>Show details of the incomplete resumable upload.</p> <p>Warning</p> <p>This operation is not stabilized yet.</p> <p>Requires RESUMABLE capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>details about the upload.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support RESUMABLE operation</p>"},{"location":"api/#file_keeper.Storage.resumable_resume","title":"<code>resumable_resume(data, upload, /, **kwargs)</code>","text":"<p>Resume the interrupted resumable upload.</p> <p>Warning</p> <p>This operation is not stabilized yet.</p> <p>Requires RESUMABLE capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>upload</code> <p>The Upload object containing the content.</p> <p> TYPE: <code>Upload</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>details about the upload.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support RESUMABLE operation</p>"},{"location":"api/#file_keeper.Storage.resumable_remove","title":"<code>resumable_remove(data, /, **kwargs)</code>","text":"<p>Remove incomplete resumable upload.</p> <p>Warning</p> <p>This operation is not stabilized yet.</p> <p>Requires RESUMABLE capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if upload was removed, False otherwise.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support RESUMABLE operation</p>"},{"location":"api/#file_keeper.Storage.multipart_start","title":"<code>multipart_start(location, size, /, **kwargs)</code>","text":"<p>Prepare everything for multipart upload.</p> <p>Warning</p> <p>This operation is not stabilized yet.</p> <p>Requires MULTIPART capability.</p> <p><code>content_type</code> and <code>hash</code> are optional. When any of those provided, it will be used to verify the integrity of the upload. If they are missing, the upload will be accepted without verification.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the upload.</p> <p> TYPE: <code>Location</code> </p> <code>size</code> <p>The total size of the upload in bytes.</p> <p> TYPE: <code>int</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>details about the upload.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support MULTIPART operation</p>"},{"location":"api/#file_keeper.Storage.multipart_refresh","title":"<code>multipart_refresh(data, /, **kwargs)</code>","text":"<p>Show details of the incomplete upload.</p> <p>Warning</p> <p>This operation is not stabilized yet.</p> <p>Requires MULTIPART capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>details about the upload.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support MULTIPART operation</p>"},{"location":"api/#file_keeper.Storage.multipart_update","title":"<code>multipart_update(data, upload, part, /, **kwargs)</code>","text":"<p>Add data to the incomplete upload.</p> <p>Warning</p> <p>This operation is not stabilized yet.</p> <p>Requires MULTIPART capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>upload</code> <p>The Upload object containing the content.</p> <p> TYPE: <code>Upload</code> </p> <code>part</code> <p>Position of the given part among other parts, starting with 0.</p> <p> TYPE: <code>int</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>details about the upload.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support MULTIPART operation</p>"},{"location":"api/#file_keeper.Storage.multipart_complete","title":"<code>multipart_complete(data, /, **kwargs)</code>","text":"<p>Verify file integrity and finalize incomplete upload.</p> <p>Warning</p> <p>This operation is not stabilized yet.</p> <p>Requires MULTIPART capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>details about the upload.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support MULTIPART operation</p>"},{"location":"api/#file_keeper.Storage.multipart_remove","title":"<code>multipart_remove(data, /, **kwargs)</code>","text":"<p>Interrupt and remove incomplete upload.</p> <p>Warning</p> <p>This operation is not stabilized yet.</p> <p>Requires MULTIPART capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if upload was removed, False otherwise.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support MULTIPART operation</p>"},{"location":"api/#file_keeper.Storage.exists","title":"<code>exists(data, /, **kwargs)</code>","text":"<p>Check if file exists in the storage.</p> <p>Requires EXISTS capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to check.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if file exists, False otherwise.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support EXISTS operation</p>"},{"location":"api/#file_keeper.Storage.remove","title":"<code>remove(data, /, **kwargs)</code>","text":"<p>Remove file from the storage.</p> <p>Requires REMOVE capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to remove.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if file was removed, False otherwise.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support REMOVE operation</p>"},{"location":"api/#file_keeper.Storage.scan","title":"<code>scan(**kwargs)</code>","text":"<p>List all locations(filenames) in storage.</p> <p>Requires SCAN capability.</p> <p>This operation lists all locations (filenames) in the storage if they start with the configured path. If path is empty, all locations are listed. Locations that match path partially(e.g. location <code>nested_dir</code> overlaps with path <code>nested</code>) are not listed.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Iterable[str]</code> <p>An iterable of location strings.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support SCAN operation</p>"},{"location":"api/#file_keeper.Storage.filtered_scan","title":"<code>filtered_scan(prefix='', glob='', **kwargs)</code>","text":"<p>List all locations(filenames) in storage that match prefix and glob.</p> <p>Requires SCAN capability.</p> <p>This operation lists all locations (filenames) in the storage if they start with the specified prefix and match the provided glob pattern. If no prefix is provided, it defaults to an empty string, meaning all locations are considered. If no glob pattern is provided, it defaults to an empty string, which matches all filenames.</p> <p>Glob parameter supports standard Unix shell-style wildcards:</p> <ul> <li><code>*</code> matches everything</li> <li><code>?</code> matches any single character</li> <li><code>[seq]</code> matches any character in seq</li> <li><code>[!seq]</code> matches any character not in seq</li> </ul> <p>Adapters may choose to support different set of wildcards. Check adapter specific documentation for details.</p> PARAMETER DESCRIPTION <code>prefix</code> <p>The prefix to filter locations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>glob</code> <p>The glob pattern to filter locations.</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Iterable[str]</code> <p>An iterable of location strings.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support SCAN operation</p>"},{"location":"api/#file_keeper.Storage.analyze","title":"<code>analyze(location, /, **kwargs)</code>","text":"<p>Return file details.</p> <p>Requires ANALYZE capability.</p> <p>FileData produced by this operation is the same as data produced by the upload().</p> <p>Attempt to analyze non-existing location leads to MissingFileError</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the file to analyze.</p> <p> TYPE: <code>Location</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>details about the file.</p> RAISES DESCRIPTION <code>MissingFileError</code> <p>when location does not exist</p> <code>UnsupportedOperationError</code> <p>when storage does not support ANALYZE operation</p>"},{"location":"api/#file_keeper.Storage.size","title":"<code>size(location, /, **kwargs)</code>","text":"<p>Return the size of the file.</p> <p>Requires ANALYZE capability.</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the file to analyze.</p> <p> TYPE: <code>Location</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>int</code> <p>size of the file</p> RAISES DESCRIPTION <code>MissingFileError</code> <p>when location does not exist</p> <code>UnsupportedOperationError</code> <p>when storage does not support ANALYZE operation</p>"},{"location":"api/#file_keeper.Storage.hash","title":"<code>hash(location, /, **kwargs)</code>","text":"<p>Return the hash of the file.</p> <p>Requires ANALYZE capability.</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the file to analyze.</p> <p> TYPE: <code>Location</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>hash of the file</p> RAISES DESCRIPTION <code>MissingFileError</code> <p>when location does not exist</p> <code>UnsupportedOperationError</code> <p>when storage does not support ANALYZE operation</p>"},{"location":"api/#file_keeper.Storage.content_type","title":"<code>content_type(location, /, **kwargs)</code>","text":"<p>Return the MIME type of the file.</p> <p>Requires ANALYZE capability.</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the file to analyze.</p> <p> TYPE: <code>Location</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>MIME type of the file</p> RAISES DESCRIPTION <code>MissingFileError</code> <p>when location does not exist</p> <code>UnsupportedOperationError</code> <p>when storage does not support ANALYZE operation</p>"},{"location":"api/#file_keeper.Storage.signed","title":"<code>signed(action, duration, location, **kwargs)</code>","text":"<p>Make an URL for signed action.</p> <p>Warning</p> <p>This operation is not stabilized yet.</p> <p>Requires SIGNED capability.</p> <p>This operation creates a signed URL that allows performing the specified action (e.g., \"upload\", \"download\") on the given location for a limited duration. The signed URL is typically used to grant temporary access to a file without requiring authentication. The URL includes a signature that verifies its authenticity and validity.</p> <p>Depending on the action, user is expected to use the URL in different ways:</p> <ul> <li>upload - use HTTP PUT request to send the file content to the URL.</li> <li>download - use HTTP GET request to retrieve the file content from     the URL.</li> <li>delete - use HTTP DELETE request to remove the file at the URL.</li> </ul> <p>Check adapter specific implementation for details. Some actions may not be supported or require additional information. For example, upload with Azure Blob Storage requires header <code>x-ms-blob-type: BlockBlob</code>.</p> PARAMETER DESCRIPTION <code>action</code> <p>The action to sign (upload, download, delete).</p> <p> TYPE: <code>SignedAction</code> </p> <code>duration</code> <p>The duration for which the signed URL is valid.</p> <p> TYPE: <code>int</code> </p> <code>location</code> <p>The location of the file to sign.</p> <p> TYPE: <code>Location</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>A signed URL as a string.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support SIGNED operation</p>"},{"location":"api/#file_keeper.Storage.stream","title":"<code>stream(data, /, **kwargs)</code>","text":"<p>Return byte-stream of the file content.</p> <p>Requires STREAM capability.</p> <p>Returns iterable that yields chunks of bytes from the file. The size of each chunk depends on the storage implementation. The iterable can be used in a for loop or converted to a list to get all chunks at once.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to stream.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Iterable[bytes]</code> <p>An iterable yielding chunks of bytes from the file.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support STREAM operation</p> <code>MissingFileError</code> <p>when file does not exist</p>"},{"location":"api/#file_keeper.Storage.range","title":"<code>range(data, start=0, end=None, /, **kwargs)</code>","text":"<p>Return slice of the file content.</p> <p>Requires RANGE capability.</p> <p>This operation slices the file content from the specified start byte offset to the end byte offset (exclusive). If <code>end</code> is None, the slice extends to the end of the file. The operation returns an iterable that yields chunks of bytes from the specified range. The size of each chunk depends on the storage implementation. The iterable can be used in a for loop or converted to a list to get all chunks at once.</p> <p>Unlike python slices, this operation does not expect negative indexes, but certain adapters may support it.</p> <p>Attempt to get a range from non-existing file leads to MissingFileError.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>start</code> <p>The starting byte offset.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>end</code> <p>The ending byte offset (inclusive).</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support RANGE operation</p> <code>MissingFileError</code> <p>when file does not exist</p>"},{"location":"api/#file_keeper.Storage.range_synthetic","title":"<code>range_synthetic(data, start=0, end=None, /, **kwargs)</code>","text":"<p>Generic implementation of range operation that relies on STREAM.</p> <p>This method provides a generic implementation of the range operation using the STREAM capability. It reads the file in chunks and yields only the specified range of bytes.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>start</code> <p>The starting byte offset.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>end</code> <p>The ending byte offset (inclusive).</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support STREAM operation</p> <code>MissingFileError</code> <p>when file does not exist</p>"},{"location":"api/#file_keeper.Storage.content","title":"<code>content(data, /, **kwargs)</code>","text":"<p>Return file content as a single byte object.</p> <p>Requires STREAM capability.</p> <p>Returns complete file content as a single bytes object. This method internally uses stream() method to read the file content in chunks and then combines those chunks into a single bytes object.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>bytes</code> <p>The complete file content as a single bytes object.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support STREAM operation</p> <code>MissingFileError</code> <p>when file does not exist</p>"},{"location":"api/#file_keeper.Storage.append","title":"<code>append(data, upload, /, **kwargs)</code>","text":"<p>Append content to existing file.</p> <p>Requires APPEND capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to append to.</p> <p> TYPE: <code>FileData</code> </p> <code>upload</code> <p>The Upload object containing the content to append.</p> <p> TYPE: <code>Upload</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>Updated FileData object with details about the file after appending.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support APPEND operation</p> <code>MissingFileError</code> <p>when file does not exist</p>"},{"location":"api/#file_keeper.Storage.copy","title":"<code>copy(location, data, /, **kwargs)</code>","text":"<p>Copy file inside the storage.</p> <p>Requires COPY capability.</p> <p>This operation creates a duplicate of the specified file at a new location within the same storage. The copied file retains the same content and metadata as the original file.</p> <p>If file already exists at the destination location, behavior depends on override_existing setting. If it is False, ExistingFileError is raised. If it is True, existing file is replaced with the copied file. In this case, it is possible to lose existing file if copy fails.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the copied file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to copy.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>FileData object with details about the copied file.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support COPY operation</p> <code>MissingFileError</code> <p>when source file does not exist</p> <code>ExistingFileError</code> <p>when destination file already exists and override_existing is False</p>"},{"location":"api/#file_keeper.Storage.copy_synthetic","title":"<code>copy_synthetic(location, data, dest_storage, /, **kwargs)</code>","text":"<p>Generic implementation of the copy operation that relies on CREATE.</p> <p>This method provides a generic implementation of the copy operation using the CREATE capability of the destination storage and the STREAM capability of the source storage. It reads the file content from the source storage and uploads it to the destination storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the copied file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to copy.</p> <p> TYPE: <code>FileData</code> </p> <code>dest_storage</code> <p>The destination storage</p> <p> TYPE: <code>Storage</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support CREATE operation</p> <code>MissingFileError</code> <p>when source file does not exist</p> <code>ExistingFileError</code> <p>when destination file already exists and override_existing is False</p>"},{"location":"api/#file_keeper.Storage.move","title":"<code>move(location, data, /, **kwargs)</code>","text":"<p>Move file to a different location inside the storage.</p> <p>Requires MOVE capability.</p> <p>This operation relocates the specified file to a new location within the same storage. After the move operation, the file will no longer exist at its original location.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the moved file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to move.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>FileData object with details about the moved file.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support MOVE operation</p> <code>MissingFileError</code> <p>when source file does not exist</p> <code>ExistingFileError</code> <p>when destination file already exists and override_existing is False</p>"},{"location":"api/#file_keeper.Storage.move_synthetic","title":"<code>move_synthetic(location, data, dest_storage, /, **kwargs)</code>","text":"<p>Generic implementation of move operation.</p> <p>Relies on CREATE and REMOVE.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the moved file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to move.</p> <p> TYPE: <code>FileData</code> </p> <code>dest_storage</code> <p>The destination storage</p> <p> TYPE: <code>Storage</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>FileData object with details about the moved file.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support CREATE or REMOVE operation</p> <code>MissingFileError</code> <p>when source file does not exist</p> <code>ExistingFileError</code> <p>when destination file already exists and override_existing is False</p>"},{"location":"api/#file_keeper.Storage.compose","title":"<code>compose(location, /, *files, **kwargs)</code>","text":"<p>Combine multiple files into a new file.</p> <p>Requires COMPOSE capability.</p> <p>This operation combines multiple source files into a single destination file. The content of the source files is concatenated in the order they are provided to form the content of the new file.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the composed file.</p> <p> TYPE: <code>Location</code> </p> <code>*files</code> <p>FileData objects representing the files to combine.</p> <p> TYPE: <code>FileData</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>FileData</code> <p>FileData object with details about the composed file.</p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support COMPOSE operation</p> <code>MissingFileError</code> <p>when any of source files do not exist</p> <code>ExistingFileError</code> <p>when destination file already exists and override_existing is False</p>"},{"location":"api/#file_keeper.Storage.compose_synthetic","title":"<code>compose_synthetic(location, dest_storage, /, *files, **kwargs)</code>","text":"<p>Generic composition that relies on APPEND.</p> <p>This method provides a generic implementation of the compose operation using the APPEND capability of the destination storage. It creates an empty file at the destination location and then appends the content of each source file to it in sequence.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the composed file.</p> <p> TYPE: <code>Location</code> </p> <code>dest_storage</code> <p>The destination storage</p> <p> TYPE: <code>Storage</code> </p> <code>*files</code> <p>FileData objects representing the files to combine.</p> <p> TYPE: <code>FileData</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>UnsupportedOperationError</code> <p>when storage does not support APPEND operation</p> <code>MissingFileError</code> <p>when any of source files do not exist</p> <code>ExistingFileError</code> <p>when destination file already exists and override_existing is False</p>"},{"location":"api/#file_keeper.Storage.one_time_link","title":"<code>one_time_link(data, /, **kwargs)</code>","text":"<p>Return one-time download link.</p> <p>Requires LINK_ONE_TIME capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str | None</code> <p>A one-time download link as a string, or None if not supported.</p>"},{"location":"api/#file_keeper.Storage.temporal_link","title":"<code>temporal_link(data, duration, /, **kwargs)</code>","text":"<p>Return temporal download link.</p> <p>Requires LINK_TEMPORAL capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>duration</code> <p>The duration for which the link is valid.</p> <p> TYPE: <code>int</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str | None</code> <p>A temporal download link as a string, or None if not supported.</p>"},{"location":"api/#file_keeper.Storage.permanent_link","title":"<code>permanent_link(data, /, **kwargs)</code>","text":"<p>Return permanent download link.</p> <p>Requires LINK_PERMANENT capability.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str | None</code> <p>A permanent download link as a string, or None if not supported.</p>"},{"location":"api/#file_keeper.Capability","title":"<code>Capability</code>","text":"<p>               Bases: <code>Flag</code></p> <p>Enumeration of operations supported by the storage.</p> Example <pre><code>read_and_write = Capability.STREAM | Capability.CREATE\nif storage.supports(read_and_write)\n    ...\n</code></pre>"},{"location":"api/#file_keeper.Capability.ANALYZE","title":"<code>ANALYZE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Return file details from the storage.</p>"},{"location":"api/#file_keeper.Capability.APPEND","title":"<code>APPEND = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Add content to the existing file.</p>"},{"location":"api/#file_keeper.Capability.COMPOSE","title":"<code>COMPOSE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Combine multiple files into a new one in the same storage.</p>"},{"location":"api/#file_keeper.Capability.COPY","title":"<code>COPY = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Make a copy of the file inside the same storage.</p>"},{"location":"api/#file_keeper.Capability.CREATE","title":"<code>CREATE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Create a file as an atomic object.</p>"},{"location":"api/#file_keeper.Capability.EXISTS","title":"<code>EXISTS = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Check if file exists.</p>"},{"location":"api/#file_keeper.Capability.LINK_ONE_TIME","title":"<code>LINK_ONE_TIME = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Make one-time download link.</p>"},{"location":"api/#file_keeper.Capability.LINK_PERMANENT","title":"<code>LINK_PERMANENT = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Make permanent download link.</p>"},{"location":"api/#file_keeper.Capability.LINK_TEMPORAL","title":"<code>LINK_TEMPORAL = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Make expiring download link.</p>"},{"location":"api/#file_keeper.Capability.MOVE","title":"<code>MOVE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Move file to a different location inside the same storage.</p>"},{"location":"api/#file_keeper.Capability.MULTIPART","title":"<code>MULTIPART = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Create file in 3 stages: initialize, upload(repeatable), complete.</p>"},{"location":"api/#file_keeper.Capability.RANGE","title":"<code>RANGE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Return specific range of bytes from the file.</p>"},{"location":"api/#file_keeper.Capability.REMOVE","title":"<code>REMOVE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Remove file from the storage.</p>"},{"location":"api/#file_keeper.Capability.RESUMABLE","title":"<code>RESUMABLE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Perform resumable uploads that can be continued after interruption.</p>"},{"location":"api/#file_keeper.Capability.SCAN","title":"<code>SCAN = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Iterate over all files in the storage.</p>"},{"location":"api/#file_keeper.Capability.SIGNED","title":"<code>SIGNED = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Generate signed URL for specific operation.</p>"},{"location":"api/#file_keeper.Capability.STREAM","title":"<code>STREAM = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Return file content as stream of bytes.</p>"},{"location":"api/#file_keeper.Capability.can","title":"<code>can(operation)</code>","text":"<p>Check whether the cluster supports given operation.</p>"},{"location":"api/#file_keeper.Capability.exclude","title":"<code>exclude(*capabilities)</code>","text":"<p>Remove capabilities from the cluster.</p> PARAMETER DESCRIPTION <code>capabilities</code> <p>removed capabilities</p> <p> TYPE: <code>Capability</code> </p> Example <pre><code>cluster = cluster.exclude(Capability.REMOVE)\n</code></pre>"},{"location":"api/#file_keeper.Settings","title":"<code>Settings</code>  <code>dataclass</code>","text":"<p>Settings for the storage adapter.</p>"},{"location":"api/#file_keeper.Settings.disabled_capabilities","title":"<code>disabled_capabilities = cast('list[str]', dataclasses.field(default_factory=list))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Capabilities that are not supported even if implemented.</p>"},{"location":"api/#file_keeper.Settings.initialize","title":"<code>initialize = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prepare storage backend for uploads(create path, bucket, DB)</p>"},{"location":"api/#file_keeper.Settings.location_transformers","title":"<code>location_transformers = cast('list[str]', dataclasses.field(default_factory=list))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of transformations applied to the file location.</p>"},{"location":"api/#file_keeper.Settings.name","title":"<code>name = 'unknown'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Descriptive name of the storage used for debugging.</p>"},{"location":"api/#file_keeper.Settings.override_existing","title":"<code>override_existing = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If file already exists, replace it with new content.</p>"},{"location":"api/#file_keeper.Settings.path","title":"<code>path = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prefix for the file's location.</p>"},{"location":"api/#file_keeper.Settings.skip_in_place_copy","title":"<code>skip_in_place_copy = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Skip in-place copy operations.</p>"},{"location":"api/#file_keeper.Settings.skip_in_place_move","title":"<code>skip_in_place_move = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Skip in-place move operations.</p>"},{"location":"api/#file_keeper.Settings.type","title":"<code>type = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of the storage adapter.</p>"},{"location":"api/#file_keeper.Settings.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Make settings object using dictionary as a source.</p> <p>Any unexpected options are extracted from the <code>data</code> to avoid initialization errors from dataclass constructor.</p> PARAMETER DESCRIPTION <code>data</code> <p>mapping with settings</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Settings</code> <p>settings object built from data</p>"},{"location":"api/#file_keeper.Uploader","title":"<code>Uploader</code>","text":"<p>               Bases: <code>StorageService</code></p> <p>Service responsible for writing data into a storage.</p> <p><code>Storage</code> internally calls methods of this service. For example, <code>Storage.upload(location, upload, **kwargs)</code> results in <code>Uploader.upload(location, upload, kwargs)</code>.</p> Example <pre><code>class MyUploader(Uploader):\n    capabilities = Capability.CREATE\n\n    def upload(\n        self, location: types.Location, upload: Upload, extras: dict[str, Any]\n    ) -&gt; FileData:\n        reader = upload.hashing_reader()\n\n        with open(location, \"wb\") as dest:\n            dest.write(reader.read())\n\n        return FileData(\n            location, upload.size,\n            upload.content_type,\n            reader.get_hash()\n        )\n</code></pre>"},{"location":"api/#file_keeper.Uploader.multipart_complete","title":"<code>multipart_complete(data, extras)</code>","text":"<p>Verify file integrity and finalize incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.multipart_refresh","title":"<code>multipart_refresh(data, extras)</code>","text":"<p>Show details of the incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.multipart_remove","title":"<code>multipart_remove(data, extras)</code>","text":"<p>Interrupt and remove incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.multipart_start","title":"<code>multipart_start(location, size, extras)</code>","text":"<p>Prepare everything for multipart(resumable) upload.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the upload.</p> <p> TYPE: <code>Location</code> </p> <code>size</code> <p>Expected upload size.</p> <p> TYPE: <code>int</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.multipart_update","title":"<code>multipart_update(data, upload, part, extras)</code>","text":"<p>Add data to the incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>upload</code> <p>The Upload object containing the content.</p> <p> TYPE: <code>Upload</code> </p> <code>part</code> <p>Position of the given part among other parts, starting with 0.</p> <p> TYPE: <code>int</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.resumable_refresh","title":"<code>resumable_refresh(data, extras)</code>","text":"<p>Show details of the incomplete resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.resumable_remove","title":"<code>resumable_remove(data, extras)</code>","text":"<p>Remove incomplete resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.resumable_resume","title":"<code>resumable_resume(data, upload, extras)</code>","text":"<p>Resume the interrupted resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>upload</code> <p>The Upload object containing the content.</p> <p> TYPE: <code>Upload</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.resumable_start","title":"<code>resumable_start(location, size, extras)</code>","text":"<p>Prepare everything for resumable upload.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the upload.</p> <p> TYPE: <code>Location</code> </p> <code>size</code> <p>Expected upload size.</p> <p> TYPE: <code>int</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.upload","title":"<code>upload(location, upload, extras)</code>","text":"<p>Upload file using single stream.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the upload.</p> <p> TYPE: <code>Location</code> </p> <code>upload</code> <p>The Upload object containing the file data.</p> <p> TYPE: <code>Upload</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager","title":"<code>Manager</code>","text":"<p>               Bases: <code>StorageService</code></p> <p>Service responsible for maintenance file operations.</p> <p><code>Storage</code> internally calls methods of this service. For example, <code>Storage.remove(data, **kwargs)</code> results in <code>Manager.remove(data, kwargs)</code>.</p> Example <pre><code>class MyManager(Manager):\n    capabilities = Capability.REMOVE\n    def remove(\n        self, data: FileData|FileData, extras: dict[str, Any]\n    ) -&gt; bool:\n        os.remove(data.location)\n        return True\n</code></pre>"},{"location":"api/#file_keeper.Manager.analyze","title":"<code>analyze(location, extras)</code>","text":"<p>Return details about location.</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the file to analyze.</p> <p> TYPE: <code>Location</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.append","title":"<code>append(data, upload, extras)</code>","text":"<p>Append content to existing file.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to append to.</p> <p> TYPE: <code>FileData</code> </p> <code>upload</code> <p>The Upload object containing the content to append.</p> <p> TYPE: <code>Upload</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.compose","title":"<code>compose(location, datas, extras)</code>","text":"<p>Combine multiple file inside the storage into a new one.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the composed file.</p> <p> TYPE: <code>Location</code> </p> <code>datas</code> <p>An iterable of FileData objects representing the files to combine.</p> <p> TYPE: <code>Iterable[FileData]</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.content_type","title":"<code>content_type(location, extras)</code>","text":"<p>Return MIME type of the file.</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the file.</p> <p> TYPE: <code>Location</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.copy","title":"<code>copy(location, data, extras)</code>","text":"<p>Copy file inside the storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the copied file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to copy.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.exists","title":"<code>exists(data, extras)</code>","text":"<p>Check if file exists in the storage.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to check.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.filtered_scan","title":"<code>filtered_scan(prefix, glob, extras)</code>","text":"<p>List all locations(filenames) in storage that match prefix and glob.</p> PARAMETER DESCRIPTION <code>prefix</code> <p>The prefix to filter locations.</p> <p> TYPE: <code>str</code> </p> <code>glob</code> <p>The glob pattern to filter locations.</p> <p> TYPE: <code>str</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.hash","title":"<code>hash(location, extras)</code>","text":"<p>Return hash of the file.</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the file.</p> <p> TYPE: <code>Location</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.move","title":"<code>move(location, data, extras)</code>","text":"<p>Move file to a different location inside the storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the moved file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to move.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.remove","title":"<code>remove(data, extras)</code>","text":"<p>Remove file from the storage.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to remove.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.scan","title":"<code>scan(extras)</code>","text":"<p>List all locations(filenames) in storage.</p> PARAMETER DESCRIPTION <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.signed","title":"<code>signed(action, duration, location, extras)</code>","text":"<p>Make an URL for signed action.</p> PARAMETER DESCRIPTION <code>action</code> <p>The action to sign (e.g., \"upload\", \"download\").</p> <p> TYPE: <code>SignedAction</code> </p> <code>duration</code> <p>The duration for which the signed URL is valid.</p> <p> TYPE: <code>int</code> </p> <code>location</code> <p>The location of the file to sign.</p> <p> TYPE: <code>Location</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.size","title":"<code>size(location, extras)</code>","text":"<p>Return size of the file in bytes.</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the file.</p> <p> TYPE: <code>Location</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Reader","title":"<code>Reader</code>","text":"<p>               Bases: <code>StorageService</code></p> <p>Service responsible for reading data from the storage.</p> <p><code>Storage</code> internally calls methods of this service. For example, <code>Storage.stream(data, **kwargs)</code> results in <code>Reader.stream(data, kwargs)</code>.</p> Example <pre><code>class MyReader(Reader):\n    capabilities = Capability.STREAM\n\n    def stream(\n        self, data: data.FileData, extras: dict[str, Any]\n    ) -&gt; Iterable[bytes]:\n        return open(data.location, \"rb\")\n</code></pre>"},{"location":"api/#file_keeper.Reader.content","title":"<code>content(data, extras)</code>","text":"<p>Return file content as a single byte object.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Reader.one_time_link","title":"<code>one_time_link(data, extras)</code>","text":"<p>Return one-time download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Reader.permanent_link","title":"<code>permanent_link(data, extras)</code>","text":"<p>Return permanent download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Reader.range","title":"<code>range(data, start, end, extras)</code>","text":"<p>Return slice of the file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>start</code> <p>The starting byte offset.</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>The ending byte offset (inclusive).</p> <p> TYPE: <code>int | None</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Reader.stream","title":"<code>stream(data, extras)</code>","text":"<p>Return byte-stream of the file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to stream.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Reader.temporal_link","title":"<code>temporal_link(data, duration, extras)</code>","text":"<p>Return temporal download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>duration</code> <p>The duration for which the link is valid.</p> <p> TYPE: <code>int</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.FileData","title":"<code>FileData</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseData</code></p> <p>Information required by storage to operate the file.</p> PARAMETER DESCRIPTION <code>location</code> <p>filepath, filename or any other type of unique identifier</p> <p> TYPE: <code>Location</code> </p> <code>size</code> <p>size of the file in bytes</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>content_type</code> <p>MIMEtype of the file</p> <p> TYPE: <code>str</code> DEFAULT: <code>'application/octet-stream'</code> </p> <code>hash</code> <p>checksum of the file</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>storage_data</code> <p>additional details set by storage adapter</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>cast('dict[str, Any]', field(default_factory=dict))</code> </p> Example <pre><code>FileData(\n    \"local/path.txt\",\n    123,\n    \"text/plain\",\n    md5_of_content,\n)\n</code></pre>"},{"location":"api/#file_keeper.Upload","title":"<code>Upload</code>  <code>dataclass</code>","text":"<p>Standard upload details.</p> PARAMETER DESCRIPTION <code>stream</code> <p>iterable of bytes or file-like object</p> <p> TYPE: <code>PStream</code> </p> <code>filename</code> <p>name of the file</p> <p> TYPE: <code>str</code> </p> <code>size</code> <p>size of the file in bytes</p> <p> TYPE: <code>int</code> </p> <code>content_type</code> <p>MIMEtype of the file</p> <p> TYPE: <code>str</code> </p> Example <pre><code>&gt;&gt;&gt; upload = Upload(\n...     BytesIO(b\"hello world\"),\n...     \"file.txt\",\n...     11,\n...     \"text/plain\",\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; upload.size\n11\n&gt;&gt;&gt; upload.filename\n'file.txt'\n&gt;&gt;&gt; upload.content_type\n'text/plain'\n&gt;&gt;&gt; upload.stream.read()\nb'hello world'\n</code></pre>"},{"location":"api/#file_keeper.Upload.content_type","title":"<code>content_type</code>  <code>instance-attribute</code>","text":"<p>MIME Type of the file</p>"},{"location":"api/#file_keeper.Upload.filename","title":"<code>filename</code>  <code>instance-attribute</code>","text":"<p>Name of the file</p>"},{"location":"api/#file_keeper.Upload.seekable_stream","title":"<code>seekable_stream</code>  <code>property</code>","text":"<p>Return stream that supports file-like <code>seek</code>.</p> <p>If internal stream does not support file-like <code>seek</code>, nothing is returned from this property.</p> <p>Use this property if you want to read the file ahead, to get CSV column names, list of files inside ZIP, EXIF metadata. If you get <code>None</code> from it, stream does not support seeking and you won't be able to rewind cursor to the beginning of the file after reading something.</p> Example <pre><code>upload = make_upload(...)\nif fd := upload.seekable_stream():\n    # read fragment of the file\n    chunk = fd.read(1024)\n    # move cursor to the end of the stream\n    fd.seek(0, 2)\n    # position of the cursor is the same as number of bytes in stream\n    size = fd.tell()\n    # move cursor back, because you don't want to accidentally loose\n    # any bites from the beginning of stream when uploader reads from it\n    fd.seek(0)\n</code></pre> RETURNS DESCRIPTION <code>PSeekableStream | None</code> <p>file-like stream or nothing</p>"},{"location":"api/#file_keeper.Upload.size","title":"<code>size</code>  <code>instance-attribute</code>","text":"<p>Size of the file</p>"},{"location":"api/#file_keeper.Upload.stream","title":"<code>stream</code>  <code>instance-attribute</code>","text":"<p>Content as iterable of bytes</p>"},{"location":"api/#file_keeper.Upload.hashing_reader","title":"<code>hashing_reader(**kwargs)</code>","text":"<p>Get reader for the upload that computes hash while reading content.</p>"},{"location":"api/#file_keeper.Location","title":"<code>Location = NewType('Location', str)</code>  <code>module-attribute</code>","text":""},{"location":"api/#file_keeper.SignedAction","title":"<code>SignedAction = Literal['upload', 'download', 'delete']</code>  <code>module-attribute</code>","text":""},{"location":"api/#file_keeper.HashingReader","title":"<code>HashingReader</code>","text":"<p>IO stream wrapper that computes content hash while stream is consumed.</p> PARAMETER DESCRIPTION <code>stream</code> <p>iterable of bytes or file-like object</p> <p> TYPE: <code>PStream</code> </p> <code>chunk_size</code> <p>max number of bytes read at once</p> <p> TYPE: <code>int</code> DEFAULT: <code>CHUNK_SIZE</code> </p> <code>algorithm</code> <p>hashing algorithm</p> <p> TYPE: <code>str</code> DEFAULT: <code>'md5'</code> </p> Example <pre><code>reader = HashingReader(readable_stream)\nfor chunk in reader:\n    ...\nprint(f\"Hash: {reader.get_hash()}\")\n</code></pre>"},{"location":"api/#file_keeper.HashingReader.exhaust","title":"<code>exhaust()</code>","text":"<p>Exhaust internal stream to compute final version of content hash.</p> <p>Note, this method does not returns data from the stream. The content will be irreversibly lost after method execution.</p>"},{"location":"api/#file_keeper.HashingReader.get_hash","title":"<code>get_hash()</code>","text":"<p>Get current content hash as a string.</p>"},{"location":"api/#file_keeper.HashingReader.read","title":"<code>read()</code>","text":"<p>Read and return all bytes from stream at once.</p>"},{"location":"api/#file_keeper.Registry","title":"<code>Registry</code>","text":"<p>               Bases: <code>Generic[V, K]</code></p> <p>Mutable collection of objects.</p> Example <pre><code>col = Registry()\n\ncol.register(\"one\", 1)\nassert col.get(\"one\") == 1\n\ncol.reset()\nassert col.get(\"one\") is None\n\nassert list(col) == [\"one\"]\n</code></pre>"},{"location":"api/#file_keeper.Registry.collect","title":"<code>collect()</code>","text":"<p>Collect members of the registry.</p>"},{"location":"api/#file_keeper.Registry.decorated","title":"<code>decorated(key)</code>","text":"<p>Collect member via decorator.</p>"},{"location":"api/#file_keeper.Registry.get","title":"<code>get(key)</code>","text":"<p>Get the optional member from registry.</p>"},{"location":"api/#file_keeper.Registry.pop","title":"<code>pop(key)</code>","text":"<p>Remove the member from registry.</p>"},{"location":"api/#file_keeper.Registry.register","title":"<code>register(key, member)</code>","text":"<p>Add a member to registry.</p>"},{"location":"api/#file_keeper.Registry.reset","title":"<code>reset()</code>","text":"<p>Remove all members from registry.</p>"},{"location":"api/#file_keeper.IterableBytesReader","title":"<code>IterableBytesReader</code>","text":"<p>               Bases: <code>AbstractReader[Iterable[int]]</code></p> <p>Wrapper that transforms iterable of bytes into readable stream.</p> Example <p>The simplest iterable of bytes is a list that contains byte strings: <pre><code>parts = [b\"hello\", b\" \", b\"world\"]\nreader = IterableBytesReader(parts)\nassert reader.read() == b\"hello world\"\n</code></pre></p> <p>More realistic scenario is wrapping generator that produces byte string in order to initialize Upload: <pre><code>def data_generator():\n    yield b\"hello\"\n    yield b\" \"\n    yield b\"world\"\n\nstream = IterableBytesReader(data_generator())\nupload = Upload(stream, \"my_file.txt\", 11, \"text/plain\")\n</code></pre></p>"},{"location":"api/#file_keeper.adapters","title":"<code>adapters = Registry['type[Storage]']()</code>  <code>module-attribute</code>","text":""},{"location":"api/#file_keeper.parse_filesize","title":"<code>parse_filesize(value)</code>","text":"<p>Transform human-readable filesize into an integer.</p> PARAMETER DESCRIPTION <code>value</code> <p>human-readable filesize</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>size cannot be parsed or uses unknown units</p> Example <pre><code>size = parse_filesize(\"10GiB\")\nassert size == 10_737_418_240\n</code></pre>"},{"location":"api/#file_keeper.humanize_filesize","title":"<code>humanize_filesize(value, base=SI_BASE)</code>","text":"<p>Transform an integer into human-readable filesize.</p> PARAMETER DESCRIPTION <code>value</code> <p>size in bytes</p> <p> TYPE: <code>int | float</code> </p> <code>base</code> <p>1000 for SI(KB, MB) or 1024 for binary(KiB, MiB)</p> <p> TYPE: <code>int</code> DEFAULT: <code>SI_BASE</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>base is not recognized</p> Example <pre><code>size = humanize_filesize(10_737_418_240, base=1024)\nassert size == \"10GiB\"\nsize = humanize_filesize(10_418_240, base=1024)\nassert size == \"9.93MiB\"\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v010-2026-01-28","title":"v0.1.0 - 2026-01-28","text":"<p>Compare with v0.1.0a7</p>"},{"location":"changelog/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>add type to settings (40435f6)</li> </ul>"},{"location":"changelog/#refactor","title":"\ud83d\ude9c Refactor","text":"<ul> <li>gcs users AnonymousCredentials when no alternative provided (667bc12)</li> </ul>"},{"location":"changelog/#v010a7-2026-01-08","title":"v0.1.0a7 - 2026-01-08","text":"<p>Compare with v0.1.0a6</p>"},{"location":"changelog/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>fsspec does not createds intermediate folders (7b6ebb0)</li> <li>copy and move fail if directory does not exist (a3da0d1)</li> </ul>"},{"location":"changelog/#v010a6-2026-01-07","title":"v0.1.0a6 - 2026-01-07","text":"<p>Compare with v0.1.0a4</p>"},{"location":"changelog/#features_1","title":"\ud83d\ude80 Features","text":"<ul> <li>skip_in_place_copy/move (b1d252a)</li> <li>obstore adapter (ea89faf)</li> <li>add filtered_scan operation (b4f08f6)</li> <li>add fsstore adapter (fd95821)</li> <li>analyzer got separate size/content_type/hash methods (3f8457a)</li> </ul>"},{"location":"changelog/#bug-fixes_1","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>range_synthetic fails when end is not specified (41738b0)</li> </ul>"},{"location":"changelog/#v010a4-2025-09-01","title":"v0.1.0a4 - 2025-09-01","text":"<p>Compare with v0.0.10</p>"},{"location":"changelog/#features_2","title":"\ud83d\ude80 Features","text":"<ul> <li>[breaking] frozen FileData and MultipartData (cb9dbf8)</li> <li>enable multipart for S3 adapter(not stable) (c8bb37b)</li> <li>enable resumable for GCS adapter(not stable) (d00f9bd)</li> <li>global storage configuration via file-keeper.json (58939eb)</li> <li>define <code>RESUMABLE</code> capability (bab84fa)</li> <li>complete GCS adapter (db97e20)</li> <li>Azure Blob storage adapter (fb491b2)</li> <li>zip adapter (3b3f978)</li> <li>add SIGNED capability (fc5fcbb)</li> <li>Storage.full_path (7974aca)</li> <li>add generic disabled_capabilities option (c335071)</li> <li>add EXISTS, SCAN, MOVE and COPY to s3 adapter (2cb762e)</li> <li>add EXISTS and ANALYZE to libcloud adapter (12349aa)</li> <li>less strict typing rules for storage settings (247d1c6)</li> <li>remove str from exceptions (2ecd8a2)</li> <li>add memory storage (3abc218)</li> <li>storage.ext.register accepts optional <code>reset</code> parameter (c5edce8)</li> <li>Settings log extra options with debug level instead of raising an exception (7308578)</li> <li>add null storage (c1f8476)</li> </ul>"},{"location":"changelog/#bug-fixes_2","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li><code>FileData.from_*</code> ignore overrides (60e0564)</li> <li>Storage.full_path allows <code>..</code> (3c65a8e)</li> <li>s3 unconditionally overrides files (c059f2b)</li> <li>storage settings keep a lot of intermediate parameters (cf69cf2)</li> <li>libcloud silently overrides existing objects (599f099)</li> </ul>"},{"location":"changelog/#refactor_1","title":"\ud83d\ude9c Refactor","text":"<ul> <li>[breaking] multipart_start requires size; multipart_update required upload and part (02c15cb)</li> <li>[breaking] resumable_start requires size; resumable_resume required upload (3bb15a1)</li> <li>[breaking] multipart_start/resumable_start expect location instead of data (d169bef)</li> <li>[breaking] disable MULTIPART capabilities on s3, fs and memory storages (50afc52)</li> <li>[breaking] remove <code>location</code> from arguments of <code>multipart_start</code> (876ce46)</li> <li>[breaking] drop <code>MultipartData</code> and use <code>FileData</code> instead everywhere (c1c01c3)</li> <li>[breaking] <code>Storage.remove</code> does not accept <code>MultipartData</code>. Use <code>Storage.multipart_remove</code> instead (ce3e522)</li> <li>[breaking] <code>create_path</code> option for fs renamed to <code>initialize</code> (1329997)</li> <li>[breaking] Storage.temporal_link requires <code>duration</code> parameter (0d92777)</li> <li>[breaking] Storage.stream_as_upload renamed to file_as_upload (29ec68b)</li> <li>[breaking] fs and opendal settings no longer have recursive flag (3f6e29b)</li> <li>add LINK_PERMANENT to s3 (d64e8be)</li> <li>add LINK_PERMANENT to gcs (66cbcf3)</li> <li>location transformers and <code>Storage.prepare_location</code> do not accept FileData anymore` (60fc015)</li> <li>redis uses <code>bucket</code> option instead of <code>path</code> (966241f)</li> <li>remove pytz dependency (43079ea)</li> <li>rename redis_url to url in redis settings (2c998f4)</li> </ul>"},{"location":"changelog/#removal","title":"\u274c Removal","text":"<ul> <li>[breaking] drop python v3.9 support (d499876)</li> </ul>"},{"location":"changelog/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Add overview page (6ad5c89)</li> </ul>"},{"location":"changelog/#v0010-2025-07-13","title":"v0.0.10 - 2025-07-13","text":"<p>Compare with v0.0.9</p>"},{"location":"changelog/#features_3","title":"\ud83d\ude80 Features","text":"<ul> <li>add public_prefix(and permanent_link) to libcloud (3bc7591)</li> <li>static_uuid transformer (88383e0)</li> <li>location transformers receive optional upload-or-data second argument (8e6a6dc)</li> </ul>"},{"location":"changelog/#bug-fixes_3","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>fix_extension transformer raises an error when upload is missing (a827df5)</li> </ul>"},{"location":"changelog/#removal_1","title":"\u274c Removal","text":"<ul> <li>delete Storage.public_link (da08744)</li> </ul>"},{"location":"changelog/#v009-2025-07-02","title":"v0.0.9 - 2025-07-02","text":"<p>Compare with v0.0.8</p>"},{"location":"changelog/#features_4","title":"\ud83d\ude80 Features","text":"<ul> <li>add fix_extension transformer (1345915)</li> <li>opendal got path option (d044ade)</li> </ul>"},{"location":"changelog/#bug-fixes_4","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>cast fs:multipart:position to int (dc4d768)</li> </ul>"},{"location":"changelog/#v008-2025-04-23","title":"v0.0.8 - 2025-04-23","text":"<p>Compare with v0.0.7</p>"},{"location":"changelog/#features_5","title":"\ud83d\ude80 Features","text":"<ul> <li>libcloud storage got path option (555036c)</li> </ul>"},{"location":"changelog/#bug-fixes_5","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>fs storage reports relative location of the missing file (cef9589)</li> </ul>"},{"location":"changelog/#v007-2025-03-28","title":"v0.0.7 - 2025-03-28","text":"<p>Compare with v0.0.6</p>"},{"location":"changelog/#features_6","title":"\ud83d\ude80 Features","text":"<ul> <li>storage upload and append requires Upload (ebf5fef)</li> </ul>"},{"location":"changelog/#bug-fixes_6","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>fs storage checks permission when creating folders (1791d68)</li> </ul>"},{"location":"changelog/#v006-2025-03-23","title":"v0.0.6 - 2025-03-23","text":"<p>Compare with v0.0.4</p>"},{"location":"changelog/#features_7","title":"\ud83d\ude80 Features","text":"<ul> <li>add *_synthetic methods (27c4164)</li> </ul>"},{"location":"changelog/#bug-fixes_7","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>multipart update of s3 and gcs work with arbitrary upload (902c9cb)</li> </ul>"},{"location":"changelog/#v004-2025-03-22","title":"v0.0.4 - 2025-03-22","text":"<p>Compare with v0.0.3</p>"},{"location":"changelog/#features_8","title":"\ud83d\ude80 Features","text":"<ul> <li>storages accept any type of upload (a64ee3d)</li> </ul>"},{"location":"changelog/#refactor_2","title":"\ud83d\ude9c Refactor","text":"<ul> <li>remove validation from storage (93392f9)</li> <li>remove type and size validation from append and compose (890c89a)</li> <li>remove public link method and capability (cb39151)</li> </ul>"},{"location":"changelog/#removal_2","title":"\u274c Removal","text":"<ul> <li>drop link storage (3328eb2)</li> </ul>"},{"location":"changelog/#v002-2025-03-17","title":"v0.0.2 - 2025-03-17","text":"<p>Compare with v0.0.1</p>"},{"location":"changelog/#features_9","title":"\ud83d\ude80 Features","text":"<ul> <li>stream-based composite implementation of range (7d47bd8)</li> <li>add Location wrapper around unsafe path parameters (b99f155)</li> <li>file_keeper:opendal adapter (214fb6c)</li> <li>file_keeper:redis adapter (8c7da94)</li> </ul>"},{"location":"changelog/#bug-fixes_8","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>map error during settings initialization into custom exception (f596037)</li> <li>fs adapter: infer <code>uploaded</code> size if it is not specified in <code>multipart_update</code> (620ec3a)</li> </ul>"},{"location":"changelog/#refactor_3","title":"\ud83d\ude9c Refactor","text":"<ul> <li><code>location_strategy: str</code> become <code>location_transformers: list[str]</code> (daf2dc6)</li> <li>remove default range implementation from reader (36f5f31)</li> </ul>"},{"location":"changelog/#testing","title":"\ud83e\uddea Testing","text":"<ul> <li>add standard adapter tests (d961682)</li> </ul>"},{"location":"changelog/#v001-2025-03-13","title":"v0.0.1 - 2025-03-13","text":""},{"location":"configuration/","title":"Configuration","text":"<p>Behavior of every storage is configurable through the Settings class. This page details the available settings and how to use them to customize your storage setup.</p>"},{"location":"configuration/#the-settings-class","title":"The Settings Class","text":"<p>The Settings class is the central point for configuring each storage adapter. It defines the options that control how the adapter interacts with the underlying storage.  Each adapter has its own subclass of Settings that adds adapter-specific options.</p> <p>While you can directly instantiate a Settings subclass with its arguments, the most common approach is to pass a dictionary of options to the storage adapter constructor. file-keeper automatically handles the transformation of this dictionary into a Settings object. This provides a more flexible and user-friendly configuration experience.</p> <p>Example</p> <p>Let's say you want to configure the S3 adapter. Instead of creating a Settings object directly, you can simply pass a dictionary like this:</p> <pre><code>from file_keeper import make_storage\n\ns3_settings = {\n    \"type\": \"file_keeper:s3\",\n    \"bucket\": \"my-s3-bucket\",\n    \"key\": \"YOUR_AWS_ACCESS_KEY_ID\",\n    \"secret\": \"YOUR_AWS_SECRET_ACCESS_KEY\",\n    \"region\": \"us-east-1\",\n}\n\nstorage = make_storage(\"my_s3_storage\", s3_settings)\n\n# Accessing the settings (for demonstration)\nprint(storage.settings.bucket) # (1)!\n</code></pre> <ol> <li>Output: my-s3-bucket</li> </ol> <p>In this example, <code>make_storage</code> automatically creates a <code>Settings</code> object from the <code>s3_settings</code> dictionary.</p>"},{"location":"configuration/#common-settings","title":"Common Settings","text":"<p>These settings are available for most storage adapters:</p> Setting Type Default Description <code>name</code> str <code>\"unknown\"</code> Descriptive name of the storage used for debugging. <code>override_existing</code> bool <code>False</code> If <code>True</code>, existing files will be overwritten during upload. If <code>False</code>, an <code>ExistingFileError</code> will be raised if a file with the same location already exists. <code>path</code> str <code>\"\"</code> The base path or directory where files are stored. The exact meaning depends on the adapter (e.g., a path in the filesystem for the FS adapter, a prefix-path inside the bucket for S3).  Required for most adapters. <code>location_transformers</code> list[str] <code>[]</code> A list of names of location transformers to apply to file locations. These transformers can be used to sanitize or modify file paths before they are used to store files. See the Extending file-keeper documentation for details. <code>disabled_capabilities</code> list[str] <code>[]</code> A list of capabilities to disable for the storage adapter. This can be useful for limiting the functionality of an adapter or for testing purposes. <code>initialize</code> bool <code>False</code> Prepare storage backend for uploads. The exact meaning depends on the adapter. Filesystem adapter created the upload folder if it's missing; cloud adapters create a bucket/container if it does not exists. <code>skip_in_place_copy</code> bool <code>True</code> Skip in-place copy operations. <code>skip_in_place_move</code> bool <code>True</code> Skip in-place move operations. <p>Important Considerations</p> Security Be careful when storing sensitive information like AWS access keys and secret keys in your configuration.  Consider using environment variables or a secure configuration management system. Validation file-keeper performs some basic validation of the configuration settings, but it's important to ensure that your settings are correct for your specific storage adapter. Error Handling Be prepared to handle <code>InvalidStorageConfigurationError</code> exceptions if your configuration is invalid."},{"location":"configuration/#adapter-specific-settings","title":"Adapter-Specific Settings","text":"<p>In addition to the common settings, adapters have their own specific settings. Certain options are interchangeable</p>"},{"location":"configuration/#file_keeperazure_blob","title":"<code>file_keeper:azure_blob</code>","text":"Setting Type Default Description <code>account_name</code> str <code>\"\"</code> Name of the account. <code>account_key</code> str <code>\"\"</code> Key for the account. <code>container_name</code> str <code>\"\"</code> Name of the storage container. <code>account_url</code> str <code>\"https://{account_name}.blob.core.windows.net\"</code> Custom resource URL. <code>client</code> BlobServiceClient <code>None</code> Existing storage client. <code>container</code> ContainerClient <code>None</code> Existing container client. <code>path</code> str <code>\"\"</code> Prefix for the file location. <code>initialize</code> bool <code>False</code> Create <code>container_name</code> if it does not exist."},{"location":"configuration/#file_keeperfs","title":"<code>file_keeper:fs</code>","text":"Setting Type Default Description <code>path</code> str <code>\"\"</code> The base directory where files are stored. <code>initialize</code> bool <code>False</code> Create <code>path</code> if it does not exist."},{"location":"configuration/#file_keepergcs","title":"<code>file_keeper:gcs</code>","text":"Setting Type Default Description <code>bucket_name</code> str <code>\"\"</code> Name of the storage bucket. <code>client</code> Client <code>None</code> Existing storage client. <code>bucket</code> Bucket <code>None</code> Existing storage bucket. <code>credentials</code> Credentials <code>None</code> Existing cloud credentials. <code>credentials_file</code> str <code>\"\"</code> Path to the JSON with cloud credentials. <code>project_id</code> str <code>\"\"</code> The project which the client acts on behalf of. <code>client_options</code> dict <code>None</code> Client options for storage client. <code>path</code> str <code>\"\"</code> Prefix for the file location. <code>initialize</code> bool <code>False</code> Create <code>bucket_name</code> if it does not exist."},{"location":"configuration/#file_keeperlibcloud","title":"<code>file_keeper:libcloud</code>","text":"Setting Type Default Description <code>provider</code> str <code>\"\"</code> Name of the Libcloud provider <code>key</code> str <code>\"\"</code> Access key of the cloud account. <code>secret</code> str or None <code>None</code> Secret key of the cloud account. <code>params</code> dict <code>{}</code> Additional parameters for cloud provider. <code>container_name</code> str <code>\"\"</code> Name of the cloud container. <code>public_prefix</code> str <code>\"\"</code> Root URL for containers with public access. This URL will be used as a prefix for the file object location when building permanent links. <code>driver</code> StorageDriver <code>None</code> Existing storage driver. <code>container</code> Container <code>None</code> Existing container object. <code>path</code> str <code>\"\"</code> Prefix for the file location. <code>initialize</code> bool <code>False</code> Create <code>container_name</code> if it does not exist."},{"location":"configuration/#file_keepermemory","title":"<code>file_keeper:memory</code>","text":"Setting Type Default Description <code>bucket</code> MutableMapping[str, bytes] <code>{}</code> Container for uploaded objects."},{"location":"configuration/#file_keepernull","title":"<code>file_keeper:null</code>","text":"<p>No specific settings</p>"},{"location":"configuration/#file_keeperopendal","title":"<code>file_keeper:opendal</code>","text":"Setting Type Default Description <code>operator</code> opendal.Operator <code>None</code> Existing OpenDAL operator <code>scheme</code> str <code>\"\"</code> Name of OpenDAL operator's scheme. <code>params</code> dict <code>{}</code> Parameters for OpenDAL operator initialization. <code>path</code> str <code>\"\"</code> Prefix for the file location."},{"location":"configuration/#file_keeperfsspec","title":"<code>file_keeper:fsspec</code>","text":"Setting Type Default Description <code>fs</code> fsspec.AbstractFileSystem <code>None</code> Existing fsspec filesystem. <code>protocol</code> str <code>\"\"</code> Name of fsspec operator. <code>params</code> dict <code>{}</code> Parameters for fsspec filesystem initialization."},{"location":"configuration/#file_keeperobstore","title":"<code>file_keeper:obstore</code>","text":"Setting Type Default Description <code>store</code> fsspec.AbstractFileSystem <code>None</code> Existing obstore store. <code>url</code> str <code>\"\"</code> URL of obstore store. <code>params</code> dict <code>{}</code> Parameters for obstore store operatorinitialization."},{"location":"configuration/#file_keeperredis","title":"<code>file_keeper:redis</code>","text":"Setting Type Default Description <code>redis</code> redis.Redis <code>None</code> Optional existing connection to Redis DB <code>url</code> str <code>\"\"</code> URL of the Redis DB. <code>bucket</code> str <code>\"\"</code> Key of the Redis HASH for uploaded objects."},{"location":"configuration/#file_keepers3","title":"<code>file_keeper:s3</code>","text":"Setting Type Default Description <code>bucket</code> str <code>\"\"</code> Name of the storage bucket. <code>client</code> S3Client <code>None\"</code> Existing S3 client. <code>key</code> str <code>None</code> The AWS Access Key. <code>secret</code> str <code>None</code> The AWS Secret Key. <code>region</code> str <code>None</code> The AWS Region of the bucket. <code>endpoint</code> str <code>None</code> Custom AWS endpoint. <code>path</code> str <code>\"\"</code> Prefix for the file location. <code>initialize</code> bool <code>False</code> Create <code>bucket</code> if it does not exist."},{"location":"configuration/#file_keepersqlalchemy","title":"<code>file_keeper:sqlalchemy</code>","text":"Setting Type Default Description <code>db_url</code> str <code>\"\"</code> URL of the storage DB. <code>table_name</code> str <code>\"\"</code> Name of the storage table. <code>location_column</code> str <code>\"\"</code> Name of the column that contains file location. <code>content_column</code> str <code>\"\"</code> Name of the column that contains file content. <code>engine</code> Engine <code>None</code> Existing DB engine. <code>table</code> Engine <code>None</code> Existing DB table. <code>location</code> Engine <code>None</code> Existing column for location. <code>content</code> Engine <code>None</code> Existing column for content. <code>initialize</code> bool <code>False</code> Create <code>table_name</code> if it does not exist."},{"location":"configuration/#file_keeperzip","title":"<code>file_keeper:zip</code>","text":"Setting Type Default Description <code>zip_path</code> str <code>\"\"</code> Path of the ZIP archive for uploaded objects."},{"location":"error_handling/","title":"Error handling","text":"<p>file-keeper provides a comprehensive set of exceptions to help you handle errors gracefully. This page documents the available exceptions and provides guidance on how to handle them.</p>"},{"location":"error_handling/#general-exception-hierarchy","title":"General exception hierarchy","text":"<p>All exceptions in file-keeper inherit from the base <code>FilesError</code> exception.  This allows you to catch all file-keeper related errors with a single <code>except</code> block.  More specific exceptions are derived from FilesError to provide more detailed error information.</p> <p>Note</p> <p>file-keeper's exceptions can be imported from <code>file_keeper.core.exceptions</code></p> <pre><code>from file_keeper.core.exceptions import FilesError\n\ntry:\n    ...\nexcept FilesError:\n    ...\n</code></pre> <p>As a shortcut, they can be accessed from the <code>exc</code> object available at the root of file-keeper module</p> <pre><code>from file_keeper import exc\n\ntry:\n    ...\nexcept exc.FilesError:\n    ...\n</code></pre>"},{"location":"error_handling/#common-error-scenarios-and-solutions","title":"Common Error Scenarios and Solutions","text":""},{"location":"error_handling/#storage-configuration-errors","title":"Storage Configuration Errors","text":"<p>When creating a storage instance, configuration errors are common:</p> <pre><code>from file_keeper import make_storage, exc\n\ntry:\n    storage = make_storage(\"my_storage\", {\n        \"type\": \"file_keeper:fs\", \n        \"path\": \"/nonexistent/path\",\n        \"initialize\": False  # This will cause an error if path doesn't exist\n    })\nexcept exc.InvalidStorageConfigurationError as e:\n    print(f\"Storage configuration error: {e}\")\n    # Solution: Set initialize=True or ensure the path exists\n    storage = make_storage(\"my_storage\", {\n        \"type\": \"file_keeper:fs\", \n        \"path\": \"/tmp/my_files\",\n        \"initialize\": True\n    })\n\n# Unknown adapter type\ntry:\n    storage = make_storage(\"bad_storage\", {\"type\": \"unknown:adapter\"})\nexcept exc.UnknownAdapterError as e:\n    print(f\"Unknown adapter: {e}\")\n    # Solution: Check available adapters or register a custom one\n</code></pre>"},{"location":"error_handling/#file-operation-errors","title":"File Operation Errors","text":"<p>Common errors during file operations:</p> <pre><code>from file_keeper import make_storage, make_upload, exc\n\nstorage = make_storage(\"memory\", {\"type\": \"file_keeper:memory\"})\nupload = make_upload(b\"Hello, file-keeper!\")\n\n# Upload a file\nfile_info = storage.upload(\"test.txt\", upload)\n\n# Handle file not found\ntry:\n    content = storage.content(file_info)\nexcept exc.MissingFileError as e:\n    print(f\"File not found: {e}\")\n    # Handle missing file appropriately\n\n# Handle existing file when override is disabled\ntry:\n    storage.upload(\"test.txt\", make_upload(b\"New content\"))\nexcept exc.ExistingFileError as e:\n    print(f\"File already exists: {e}\")\n    # Either enable override_existing in settings or handle differently\n\n# Check capabilities before operations\nif storage.supports(exc.Capability.REMOVE):\n    storage.remove(file_info)\nelse:\n    print(\"Remove operation not supported by this storage\")\n</code></pre>"},{"location":"error_handling/#network-and-external-service-errors","title":"Network and External Service Errors","text":"<p>For cloud storage adapters:</p> <pre><code>from file_keeper import exc\n\ntry:\n    # This might fail due to network issues or service unavailability\n    storage.upload(\"remote_file.txt\", upload)\nexcept exc.UploadError as e:\n    print(f\"Upload failed: {e}\")\n    # Implement retry logic or fallback mechanism\nexcept exc.ConnectionError as e:\n    print(f\"Connection error: {e}\")\n    # Check network connectivity or service availability\n</code></pre>"},{"location":"error_handling/#best-practices-for-error-handling","title":"Best Practices for Error Handling","text":""},{"location":"error_handling/#1-check-capabilities-before-operations","title":"1. Check Capabilities Before Operations","text":"<pre><code>from file_keeper import Capability\n\n# Check if operation is supported before attempting it\nif storage.supports(Capability.REMOVE):\n    storage.remove(file_info)\nelse:\n    # Handle unsupported operation gracefully\n    print(\"Remove operation not supported\")\n</code></pre>"},{"location":"error_handling/#2-implement-retry-logic-for-transient-errors","title":"2. Implement Retry Logic for Transient Errors","text":"<pre><code>import time\nfrom file_keeper import exc\n\ndef upload_with_retry(storage, location, upload, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return storage.upload(location, upload)\n        except (exc.ConnectionError, exc.UploadError) as e:\n            if attempt == max_retries - 1:  # Last attempt\n                raise e\n            print(f\"Attempt {attempt + 1} failed: {e}. Retrying...\")\n            time.sleep(2 ** attempt)  # Exponential backoff\n</code></pre>"},{"location":"error_handling/#3-graceful-degradation","title":"3. Graceful Degradation","text":"<pre><code>from file_keeper import Capability, exc\n\ndef get_file_content_safe(storage, file_info):\n    \"\"\"Get file content with graceful degradation.\"\"\"\n    try:\n        # Try to get content directly\n        return storage.content(file_info)\n    except exc.MissingFileError:\n        print(\"File not found\")\n        return None\n    except exc.UnsupportedOperationError:\n        # If content() is not supported, try streaming\n        if storage.supports(Capability.STREAM):\n            return b\"\".join(storage.stream(file_info))\n        else:\n            print(\"No way to retrieve file content\")\n            return None\n    except exc.FilesError as e:\n        print(f\"Unexpected error: {e}\")\n        return None\n</code></pre>"},{"location":"error_handling/#4-proper-resource-cleanup","title":"4. Proper Resource Cleanup","text":"<pre><code>from file_keeper import exc\n\ndef process_file_with_cleanup(storage, upload_data):\n    \"\"\"Process a file and ensure cleanup on failure.\"\"\"\n    file_info = None\n    try:\n        file_info = storage.upload(\"temp_file.txt\", upload_data)\n\n        # Process the file\n        content = storage.content(file_info)\n        processed_content = content.upper()  # Example processing\n\n        # Save processed file\n        processed_info = storage.upload(\"processed_file.txt\", make_upload(processed_content))\n\n        return processed_info\n    except exc.FilesError as e:\n        print(f\"Processing failed: {e}\")\n        # Clean up temporary file if it was created\n        if file_info and storage.supports(Capability.REMOVE):\n            try:\n                storage.remove(file_info)\n            except exc.FilesError:\n                pass  # Ignore cleanup errors\n        raise  # Re-raise the original error\n</code></pre>"},{"location":"error_handling/#exception-reference","title":"Exception Reference","text":""},{"location":"error_handling/#file_keeper.exc","title":"<code>exc</code>","text":"<p>Exception definitions for the extension.</p> <p>Hierarchy:</p> <ul> <li>Exception<ul> <li>FilesError<ul> <li>StorageError<ul> <li>UnknownAdapterError</li> <li>UnknownStorageError</li> <li>UnsupportedOperationError</li> <li>PermissionError</li> <li>LocationError<ul> <li>MissingFileError</li> <li>ExistingFileError</li> </ul> </li> <li>StorageDataError</li> <li>ExtrasError<ul> <li>MissingExtrasError</li> </ul> </li> <li>InvalidStorageConfigurationError<ul> <li>MissingStorageConfigurationError</li> </ul> </li> <li>UploadError<ul> <li>WrongUploadTypeError</li> <li>LocationTransformerError</li> <li>ContentError</li> <li>LargeUploadError<ul> <li>UploadOutOfBoundError</li> </ul> </li> <li>UploadMismatchError<ul> <li>UploadTypeMismatchError</li> <li>UploadHashMismatchError</li> <li>UploadSizeMismatchError</li> </ul> </li> <li>ResumableUploadError</li> <li>MultipartUploadError</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"error_handling/#file_keeper.exc.ContentError","title":"<code>ContentError(storage, msg)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Storage cannot accept uploaded content.</p>"},{"location":"error_handling/#file_keeper.exc.ExistingFileError","title":"<code>ExistingFileError(storage, location)</code>","text":"<p>               Bases: <code>LocationError</code></p> <p>File already exists.</p>"},{"location":"error_handling/#file_keeper.exc.ExtrasError","title":"<code>ExtrasError(problem)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Wrong extras passed to storage method.</p>"},{"location":"error_handling/#file_keeper.exc.FilesError","title":"<code>FilesError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base error for catch-all scenario.</p>"},{"location":"error_handling/#file_keeper.exc.InvalidStorageConfigurationError","title":"<code>InvalidStorageConfigurationError(adapter_or_storage, problem)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Storage cannot be initialized with given configuration.</p>"},{"location":"error_handling/#file_keeper.exc.LargeUploadError","title":"<code>LargeUploadError(actual_size, max_size)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Planned upload exceeds allowed size.</p>"},{"location":"error_handling/#file_keeper.exc.LocationError","title":"<code>LocationError(storage, location)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Storage cannot use given location.</p>"},{"location":"error_handling/#file_keeper.exc.LocationTransformerError","title":"<code>LocationTransformerError(transformer)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Undefined location transformer.</p>"},{"location":"error_handling/#file_keeper.exc.MissingExtrasError","title":"<code>MissingExtrasError(key)</code>","text":"<p>               Bases: <code>ExtrasError</code></p> <p>Expected extras are missing from the call.</p>"},{"location":"error_handling/#file_keeper.exc.MissingFileError","title":"<code>MissingFileError(storage, location)</code>","text":"<p>               Bases: <code>LocationError</code></p> <p>File does not exist.</p>"},{"location":"error_handling/#file_keeper.exc.MissingStorageConfigurationError","title":"<code>MissingStorageConfigurationError(adapter_or_storage, option)</code>","text":"<p>               Bases: <code>InvalidStorageConfigurationError</code></p> <p>Storage cannot be initialized due to missing option.</p>"},{"location":"error_handling/#file_keeper.exc.MultipartUploadError","title":"<code>MultipartUploadError</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Error related to multipart upload process.</p>"},{"location":"error_handling/#file_keeper.exc.PermissionError","title":"<code>PermissionError(storage, operation, problem)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Storage client does not have required permissions.</p>"},{"location":"error_handling/#file_keeper.exc.ResumableUploadError","title":"<code>ResumableUploadError</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Error related to resumable upload process.</p>"},{"location":"error_handling/#file_keeper.exc.StorageDataError","title":"<code>StorageDataError(problem)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Wrong storage_data used for operation.</p>"},{"location":"error_handling/#file_keeper.exc.StorageError","title":"<code>StorageError</code>","text":"<p>               Bases: <code>FilesError</code></p> <p>Error related to storage.</p>"},{"location":"error_handling/#file_keeper.exc.UnknownAdapterError","title":"<code>UnknownAdapterError(adapter)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Specified storage adapter is not registered.</p>"},{"location":"error_handling/#file_keeper.exc.UnknownStorageError","title":"<code>UnknownStorageError(storage)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Storage with the given name is not configured.</p>"},{"location":"error_handling/#file_keeper.exc.UnsupportedOperationError","title":"<code>UnsupportedOperationError(operation, storage)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Requested operation is not supported by storage.</p>"},{"location":"error_handling/#file_keeper.exc.UploadError","title":"<code>UploadError</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Error related to file upload process.</p>"},{"location":"error_handling/#file_keeper.exc.UploadHashMismatchError","title":"<code>UploadHashMismatchError(actual, expected)</code>","text":"<p>               Bases: <code>UploadMismatchError</code></p> <p>Expected value of hash match the actual value.</p>"},{"location":"error_handling/#file_keeper.exc.UploadMismatchError","title":"<code>UploadMismatchError(attribute, actual, expected)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Expected value of file attribute doesn't match the actual value.</p>"},{"location":"error_handling/#file_keeper.exc.UploadOutOfBoundError","title":"<code>UploadOutOfBoundError(actual_size, max_size)</code>","text":"<p>               Bases: <code>LargeUploadError</code></p> <p>Ongoing upload exceeds expected size.</p>"},{"location":"error_handling/#file_keeper.exc.UploadSizeMismatchError","title":"<code>UploadSizeMismatchError(actual, expected)</code>","text":"<p>               Bases: <code>UploadMismatchError</code></p> <p>Expected value of upload size doesn't match the actual value.</p>"},{"location":"error_handling/#file_keeper.exc.UploadTypeMismatchError","title":"<code>UploadTypeMismatchError(actual, expected)</code>","text":"<p>               Bases: <code>UploadMismatchError</code></p> <p>Expected value of content type doesn't match the actual value.</p>"},{"location":"error_handling/#file_keeper.exc.WrongUploadTypeError","title":"<code>WrongUploadTypeError(content_type)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Storage does not support given MIMEType.</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Answers to common questions about file-keeper.</p>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-file-keeper","title":"What is file-keeper?","text":"<p>file-keeper is a Python library that provides a unified interface for storing, retrieving, and managing files across different storage backends. It supports local filesystem, cloud storage services (AWS S3, Google Cloud Storage, Azure), in-memory storage, and more.</p>"},{"location":"faq/#why-should-i-use-file-keeper-instead-of-directly-using-storage-sdks","title":"Why should I use file-keeper instead of directly using storage SDKs?","text":"<p>file-keeper provides several advantages:</p> <ol> <li>Unified API: Same interface regardless of storage backend</li> <li>Easy migration: Switch between storage systems without changing application code</li> <li>Built-in features: Security protections, capability detection, error handling</li> <li>Extensibility: Easy to add custom storage adapters</li> <li>Type safety: Comprehensive type annotations</li> </ol>"},{"location":"faq/#which-storage-backends-are-supported","title":"Which storage backends are supported?","text":"<p>file-keeper supports many storage backends including:</p> <ul> <li>Local filesystem (<code>file_keeper:fs</code>)</li> <li>In-memory storage (<code>file_keeper:memory</code>)</li> <li>AWS S3 (<code>file_keeper:s3</code>)</li> <li>Google Cloud Storage (<code>file_keeper:gcs</code>)</li> <li>Azure Blob Storage (<code>file_keeper:azure_blob</code>)</li> <li>Redis (<code>file_keeper:redis</code>)</li> <li>SQL databases via SQLAlchemy (<code>file_keeper:sqlalchemy</code>)</li> <li>And many more!</li> </ul>"},{"location":"faq/#configuration-questions","title":"Configuration Questions","text":""},{"location":"faq/#how-do-i-configure-different-storage-backends","title":"How do I configure different storage backends?","text":"<p>Each storage backend has its own configuration options, but they all follow the same pattern:</p> <pre><code>import file_keeper as fk\n\n# File system storage\nstorage = fk.make_storage(\"fs\", {\n    \"type\": \"file_keeper:fs\",\n    \"path\": \"/path/to/storage\",\n    \"initialize\": True\n})\n\n# S3 storage\nstorage = fk.make_storage(\"s3\", {\n    \"type\": \"file_keeper:s3\",\n    \"bucket\": \"my-bucket\",\n    \"key\": \"access-key\",\n    \"secret\": \"secret-key\"\n})\n</code></pre>"},{"location":"faq/#what-are-common-configuration-options","title":"What are common configuration options?","text":"<p>All storage adapters share these common options:</p> <ul> <li><code>type</code>: The adapter type (required)</li> <li><code>name</code>: Identifier for the storage instance</li> <li><code>path</code>: Base path/prefix for files</li> <li><code>override_existing</code>: Whether to overwrite existing files</li> <li><code>initialize</code>: Whether to create the storage container if it doesn't exist</li> <li><code>location_transformers</code>: Functions to transform file locations</li> </ul>"},{"location":"faq/#how-do-i-handle-sensitive-configuration-like-api-keys","title":"How do I handle sensitive configuration like API keys?","text":"<p>Never hardcode credentials. Use environment variables or configuration management:</p> <pre><code>import os\nimport file_keeper as fk\n\nstorage = fk.make_storage(\"s3\", {\n    \"type\": \"file_keeper:s3\",\n    \"bucket\": \"my-bucket\",\n    \"key\": os.getenv(\"AWS_ACCESS_KEY_ID\"),\n    \"secret\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n    \"region\": os.getenv(\"AWS_DEFAULT_REGION\", \"us-east-1\")\n})\n</code></pre>"},{"location":"faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/#how-do-i-upload-a-file","title":"How do I upload a file?","text":"<pre><code>import file_keeper as fk\n\nstorage = fk.make_storage(\"memory\", {\"type\": \"file_keeper:memory\"})\nupload = fk.make_upload(b\"Hello, world!\")\nfile_info = storage.upload(\"hello.txt\", upload)\n</code></pre>"},{"location":"faq/#how-do-i-check-if-a-file-exists","title":"How do I check if a file exists?","text":"<pre><code># Check if storage supports EXISTS capability first\nif storage.supports(fk.Capability.EXISTS):\n    exists = storage.exists(file_info)\n    print(f\"File exists: {exists}\")\nelse:\n    print(\"Existence check not supported by this storage\")\n</code></pre>"},{"location":"faq/#how-do-i-handle-errors-properly","title":"How do I handle errors properly?","text":"<pre><code>from file_keeper import exc\n\ntry:\n    file_info = storage.upload(\"myfile.txt\", upload)\n    content = storage.content(file_info)\nexcept exc.MissingFileError:\n    print(\"File not found\")\nexcept exc.ExistingFileError:\n    print(\"File already exists\")\nexcept exc.InvalidStorageConfigurationError as e:\n    print(f\"Storage configuration error: {e}\")\nexcept exc.FilesError as e:\n    print(f\"File operation failed: {e}\")\n</code></pre>"},{"location":"faq/#how-do-i-work-with-large-files-without-loading-everything-into-memory","title":"How do I work with large files without loading everything into memory?","text":"<p><code>make_file</code> function can transform file descriptior(file opened with built-in <code>open</code> function) into efficient streamable object. For other content generated in real-time, use <code>IterableBytesReader</code>:</p> <pre><code>from file_keeper import IterableBytesReader\n\ndef upload_large_file_in_chunks(storage, location, stream, file_size):\n    stream = IterableBytesReader(stream)\n    upload = fk.Upload(stream, location, file_size, \"application/octet-stream\")\n\n    return storage.upload(location, upload)\n\ndef chunk_generator():\n    yield b\"hello\"\n    yield b\" \"\n    yield b\"world\"\n\nupload_large_file_in_chunks(\n    make_storage(...),\n    \"big-file.txt\",\n    chunk_generator(),\n    11,\n)\n</code></pre>"},{"location":"faq/#performance-questions","title":"Performance Questions","text":""},{"location":"faq/#how-can-i-optimize-performance","title":"How can I optimize performance?","text":"<ol> <li>Use appropriate storage for your use case: Local filesystem for local access, CDN-backed storage for public files</li> <li>Enable compression if supported by your storage backend</li> <li>Use streaming for large files to avoid memory issues</li> <li>Cache frequently accessed files in memory or a fast storage tier</li> </ol>"},{"location":"faq/#does-file-keeper-support-multipart-uploads","title":"Does file-keeper support multipart uploads?","text":"<p>Yes, for storage backends that support it. Check for the <code>MULTIPART</code> capability:</p> <pre><code>if storage.supports(fk.Capability.MULTIPART):\n    # Use multipart upload\n    upload_info = storage.multipart_start(location, file_size)\n    # Upload parts...\n    final_info = storage.multipart_complete(upload_info)\n</code></pre>"},{"location":"faq/#security-questions","title":"Security Questions","text":""},{"location":"faq/#how-does-file-keeper-protect-against-directory-traversal-attacks","title":"How does file-keeper protect against directory traversal attacks?","text":"<p>file-keeper validates file locations to ensure they don't escape the configured storage path:</p> <pre><code># This would raise a LocationError if it tries to go outside the storage path\nfile_info = storage.upload(\"../../etc/passwd\", upload)\n</code></pre>"},{"location":"faq/#how-do-i-validate-file-types-and-sizes","title":"How do I validate file types and sizes?","text":"<p>Validate before uploading:</p> <pre><code>def safe_upload(storage, filename, upload, allowed_types, max_size):\n    # Validate file type\n    import mimetypes\n    mime_type, _ = mimetypes.guess_type(filename)\n    if mime_type not in allowed_types:\n        raise ValueError(f\"File type not allowed: {mime_type}\")\n\n    # Validate file size\n    if upload.size &gt; max_size:\n        raise ValueError(f\"File too large: {upload.size} bytes\")\n\n    # Sanitize filename\n    import os\n    safe_filename = os.path.basename(filename)\n\n    return storage.upload(safe_filename, upload)\n</code></pre>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#im-getting-unknown-adapter-error-what-should-i-do","title":"I'm getting \"Unknown adapter\" error, what should I do?","text":"<p>This usually means you're using an adapter type that isn't registered or available:</p> <ol> <li>Check the spelling of the adapter type</li> <li>Make sure required dependencies are installed (e.g., <code>pip install 'file-keeper[s3]'</code> for S3 support)</li> <li>Verify the adapter name format: <code>file_keeper:adapter_name</code></li> <li>Check the list of available adapters:    <pre><code>import file_keeper as fk\nprint(fk.list_adapters())\n</code></pre></li> </ol>"},{"location":"faq/#my-uploads-are-failing-with-connection-errors","title":"My uploads are failing with connection errors","text":"<p>This could be due to:</p> <ol> <li>Network connectivity issues</li> <li>Incorrect credentials</li> <li>Insufficient permissions</li> <li>Rate limiting by the storage service</li> </ol> <p>Implement retry logic:</p> <pre><code>import time\nfrom file_keeper import exc\n\ndef upload_with_retry(storage, location, upload, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return storage.upload(location, upload)\n        except (exc.ConnectionError, exc.UploadError) as e:\n            if attempt == max_retries - 1:\n                raise e\n            time.sleep(2 ** attempt)  # Exponential backoff\n</code></pre>"},{"location":"faq/#how-do-i-migrate-from-one-storage-backend-to-another","title":"How do I migrate from one storage backend to another?","text":"<p>Use the scan and copy functionality:</p> <pre><code>def migrate_storage(source_storage, dest_storage):\n    for location in source_storage.scan():\n        file_info = source_storage.analyze(location)\n        upload = source_storage.file_as_upload(file_info)\n        dest_storage.upload(location, upload)\n        print(f\"Migrated: {location}\")\n</code></pre>"},{"location":"faq/#development-questions","title":"Development Questions","text":""},{"location":"faq/#how-do-i-create-a-custom-storage-adapter","title":"How do I create a custom storage adapter?","text":"<p>Extend the Storage class and implement the required services:</p> <pre><code>from file_keeper import Storage, Settings, Uploader, Manager, Reader\nfrom file_keeper import Capability, FileData\n\nclass CustomSettings(Settings):\n    api_key: str = \"\"\n    endpoint: str = \"\"\n\nclass CustomUploader(Uploader):\n    capabilities = Capability.CREATE\n\n    def upload(self, location, upload, extras):\n        # Implement upload logic\n        pass\n\nclass CustomStorage(Storage):\n    SettingsFactory = CustomSettings\n    UploaderFactory = CustomUploader\n    # Implement other services as needed\n</code></pre> <p>Then register your adapter:</p> <pre><code>from file_keeper import adapters\nadapters.register(\"file_keeper:custom\", CustomStorage)\n</code></pre>"},{"location":"glossary/","title":"Glossary","text":"<p>Definitions of key terms used in file-keeper documentation.</p>"},{"location":"glossary/#a","title":"A","text":"<p>Adapter - A concrete implementation of the Storage interface that connects to a specific storage backend (e.g., S3, filesystem, memory).</p>"},{"location":"glossary/#c","title":"C","text":"<p>Capability - A flag indicating what operations a storage backend supports. Examples include CREATE, READ, UPDATE, DELETE, and more specialized operations.</p> <p>Configuration - Settings that define how a storage adapter behaves, including connection details, paths, and options.</p>"},{"location":"glossary/#d","title":"D","text":"<p>Data Model - The structure used to represent file information consistently across all storage backends.</p> <p>FileData - A data class containing metadata about a stored file, including location, size, content type, and hash.</p>"},{"location":"glossary/#l","title":"L","text":"<p>Location - A string identifier that uniquely identifies a file within a storage backend. This could be a file path, object key, or other identifier depending on the backend.</p>"},{"location":"glossary/#s","title":"S","text":"<p>Settings - The configuration object used to initialize a storage adapter.</p> <p>Storage - The main interface for interacting with a file storage backend. Provides methods for uploading, downloading, and managing files.</p> <p>Storage Service - Components of a storage implementation that handle specific types of operations (Uploader, Manager, Reader).</p>"},{"location":"glossary/#u","title":"U","text":"<p>Upload - An object representing data to be stored, including the content and metadata about the file.</p> <p>Uploader - A service responsible for writing data to storage.</p>"},{"location":"glossary/#r","title":"R","text":"<p>Reader - A service responsible for reading data from storage.</p>"},{"location":"glossary/#m","title":"M","text":"<p>Manager - A service responsible for maintenance operations like copying, moving, and deleting files.</p>"},{"location":"migration_guide/","title":"Migration Guide","text":"<p>Learn how to migrate from other file storage libraries to file-keeper.</p>"},{"location":"migration_guide/#migrating-from-direct-filesystem-operations","title":"Migrating from Direct Filesystem Operations","text":"<p>If you're currently using direct filesystem operations, here's how to migrate to file-keeper's filesystem adapter.</p>"},{"location":"migration_guide/#before-direct-filesystem-operations","title":"Before: Direct Filesystem Operations","text":"<pre><code>import os\nimport shutil\nfrom pathlib import Path\n\n# Creating directories\nupload_dir = Path(\"/uploads\")\nupload_dir.mkdir(parents=True, exist_ok=True)\n\n# Saving a file\nfile_path = upload_dir / \"myfile.txt\"\nwith open(file_path, 'wb') as f:\n    f.write(b\"file content\")\n\n# Reading a file\nwith open(file_path, 'rb') as f:\n    content = f.read()\n\n# Checking if file exists\nif file_path.exists():\n    print(\"File exists\")\n\n# Removing a file\nif file_path.exists():\n    file_path.unlink()\n</code></pre>"},{"location":"migration_guide/#after-using-file-keeper","title":"After: Using file-keeper","text":"<pre><code>import file_keeper as fk\n\n# Initialize storage\nstorage = fk.make_storage(\"fs\", {\n    \"type\": \"file_keeper:fs\",\n    \"path\": \"/uploads\",\n    \"initialize\": True  # Creates directory if it doesn't exist\n})\n\n# Uploading a file\nupload = fk.make_upload(b\"file content\")\nfile_info = storage.upload(\"myfile.txt\", upload)\n\n# Reading a file\ncontent = storage.content(file_info)\n\n# Checking if file exists\nif storage.supports(fk.Capability.EXISTS):\n    exists = storage.exists(file_info)\n    print(f\"File exists: {exists}\")\n\n# Removing a file\nif storage.supports(fk.Capability.REMOVE):\n    storage.remove(file_info)\n</code></pre>"},{"location":"migration_guide/#benefits-of-migration","title":"Benefits of Migration","text":"<ol> <li>Consistency: Same API regardless of storage backend</li> <li>Security: Built-in protection against directory traversal</li> <li>Flexibility: Easy switch to cloud storage later</li> <li>Capabilities: Runtime checking of supported operations</li> </ol>"},{"location":"migration_guide/#migrating-from-boto3-aws-s3","title":"Migrating from boto3 (AWS S3)","text":""},{"location":"migration_guide/#before-using-boto3-directly","title":"Before: Using boto3 directly","text":"<pre><code>import boto3\nfrom botocore.exceptions import ClientError\n\n# Initialize client\ns3_client = boto3.client(\n    's3',\n    aws_access_key_id='your-key',\n    aws_secret_access_key='your-secret',\n    region_name='us-east-1'\n)\n\nbucket_name = 'my-bucket'\n\n# Upload file\ndef upload_file(key, file_content):\n    s3_client.put_object(\n        Bucket=bucket_name,\n        Key=key,\n        Body=file_content\n    )\n\n# Download file\ndef download_file(key):\n    try:\n        response = s3_client.get_object(Bucket=bucket_name, Key=key)\n        return response['Body'].read()\n    except ClientError as e:\n        if e.response['Error']['Code'] == 'NoSuchKey':\n            return None\n        raise\n\n# Check if file exists\ndef file_exists(key):\n    try:\n        s3_client.head_object(Bucket=bucket_name, Key=key)\n        return True\n    except ClientError as e:\n        if e.response['Error']['Code'] == '404':\n            return False\n        raise\n\n# Delete file\ndef delete_file(key):\n    s3_client.delete_object(Bucket=bucket_name, Key=key)\n</code></pre>"},{"location":"migration_guide/#after-using-file-keeper-s3-adapter","title":"After: Using file-keeper S3 adapter","text":"<pre><code>import file_keeper as fk\nfrom file_keeper import exc\n\n# Initialize storage\nstorage = fk.make_storage(\"s3\", {\n    \"type\": \"file_keeper:s3\",\n    \"bucket\": \"my-bucket\",\n    \"key\": \"your-key\",\n    \"secret\": \"your-secret\",\n    \"region\": \"us-east-1\"\n})\n\n# Upload file\nupload = fk.make_upload(b\"file content\")\nfile_info = storage.upload(\"myfile.txt\", upload)\n\n# Download file\ntry:\n    content = storage.content(file_info)\nexcept exc.MissingFileError:\n    content = None\n\n# Check if file exists\nif storage.supports(fk.Capability.EXISTS):\n    exists = storage.exists(file_info)\n\n# Delete file\nif storage.supports(fk.Capability.REMOVE):\n    storage.remove(file_info)\n</code></pre>"},{"location":"migration_guide/#benefits-of-migration_1","title":"Benefits of Migration","text":"<ol> <li>Simplified Error Handling: Unified exception hierarchy</li> <li>Capability Checking: Know what operations are supported</li> <li>Backend Flexibility: Can switch to local storage for testing</li> <li>Consistent API: Same interface as other storage types</li> </ol>"},{"location":"migration_guide/#migrating-from-django-storages","title":"Migrating from Django Storages","text":""},{"location":"migration_guide/#before-using-django-storages","title":"Before: Using Django Storages","text":"<pre><code>from django.core.files.storage import default_storage\nfrom django.core.files.base import ContentFile\n\n# Save file\ndef save_file(filename, content):\n    path = default_storage.save(filename, ContentFile(content))\n    return path\n\n# Read file\ndef read_file(filename):\n    if default_storage.exists(filename):\n        with default_storage.open(filename, 'rb') as f:\n            return f.read()\n    return None\n\n# Get file URL\ndef get_file_url(filename):\n    if default_storage.exists(filename):\n        return default_storage.url(filename)\n    return None\n\n# Delete file\ndef delete_file(filename):\n    if default_storage.exists(filename):\n        default_storage.delete(filename)\n</code></pre>"},{"location":"migration_guide/#after-using-file-keeper-with-django","title":"After: Using file-keeper with Django","text":"<pre><code>import file_keeper as fk\nfrom file_keeper import exc\n\n# Initialize storage (could be configured via Django settings)\nstorage = fk.make_storage(\"django_files\", {\n    \"type\": \"file_keeper:fs\",  # or s3, gcs, etc.\n    \"path\": \"/path/to/django/media\",\n    \"initialize\": True\n})\n\ndef save_file(filename, content):\n    upload = fk.make_upload(content)\n    file_info = storage.upload(filename, upload)\n    return file_info.location\n\ndef read_file(filename):\n    # Find file by scanning or you'd need to store file_info elsewhere\n    for loc in storage.scan():\n        if loc == filename:\n            try:\n                file_info = storage.analyze(loc)\n                return storage.content(file_info)\n            except exc.FilesError:\n                return None\n    return None\n\ndef delete_file(filename):\n    # Find file by scanning\n    for loc in storage.scan():\n        if loc == filename:\n            try:\n                file_info = storage.analyze(loc)\n                if storage.supports(fk.Capability.REMOVE):\n                    return storage.remove(file_info)\n            except exc.FilesError:\n                return False\n    return False\n</code></pre>"},{"location":"migration_guide/#migrating-from-google-cloud-storage-gcs","title":"Migrating from Google Cloud Storage (gcs)","text":""},{"location":"migration_guide/#before-using-google-cloud-storage-directly","title":"Before: Using google-cloud-storage directly","text":"<pre><code>from google.cloud import storage\nfrom google.cloud.exceptions import NotFound\n\n# Initialize client\nclient = storage.Client()\nbucket = client.bucket('my-bucket')\n\n# Upload file\ndef upload_file(blob_name, file_content):\n    blob = bucket.blob(blob_name)\n    blob.upload_from_string(file_content)\n\n# Download file\ndef download_file(blob_name):\n    blob = bucket.blob(blob_name)\n    try:\n        return blob.download_as_bytes()\n    except NotFound:\n        return None\n\n# Check if file exists\ndef file_exists(blob_name):\n    blob = bucket.blob(blob_name)\n    return blob.exists()\n\n# Delete file\ndef delete_file(blob_name):\n    blob = bucket.blob(blob_name)\n    blob.delete()\n</code></pre>"},{"location":"migration_guide/#after-using-file-keeper-gcs-adapter","title":"After: Using file-keeper GCS adapter","text":"<pre><code>import file_keeper as fk\nfrom file_keeper import exc\n\n# Initialize storage\nstorage = fk.make_storage(\"gcs\", {\n    \"type\": \"file_keeper:gcs\",\n    \"bucket_name\": \"my-bucket\",\n    \"credentials_file\": \"/path/to/credentials.json\"  # or use other auth methods\n})\n\n# Upload file\nupload = fk.make_upload(b\"file content\")\nfile_info = storage.upload(\"myfile.txt\", upload)\n\n# Download file\ntry:\n    content = storage.content(file_info)\nexcept exc.MissingFileError:\n    content = None\n\n# Check if file exists\nif storage.supports(fk.Capability.EXISTS):\n    exists = storage.exists(file_info)\n\n# Delete file\nif storage.supports(fk.Capability.REMOVE):\n    storage.remove(file_info)\n</code></pre>"},{"location":"migration_guide/#migration-strategy","title":"Migration Strategy","text":""},{"location":"migration_guide/#1-gradual-migration-approach","title":"1. Gradual Migration Approach","text":"<p>Start by replacing one storage operation at a time:</p> <pre><code># Old code\nimport os\ndef save_user_avatar_old(user_id, avatar_data):\n    filepath = f\"/avatars/{user_id}.jpg\"\n    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n    with open(filepath, 'wb') as f:\n        f.write(avatar_data)\n    return filepath\n\n# New code\nimport file_keeper as fk\nstorage = fk.make_storage(\"avatars\", {\n    \"type\": \"file_keeper:fs\",\n    \"path\": \"/avatars\",\n    \"initialize\": True\n})\n\ndef save_user_avatar_new(user_id, avatar_data):\n    upload = fk.make_upload(avatar_data)\n    file_info = storage.upload(f\"{user_id}.jpg\", upload)\n    return file_info.location\n</code></pre>"},{"location":"migration_guide/#2-configuration-management","title":"2. Configuration Management","text":"<p>Create a configuration factory to make switching between environments easier:</p> <pre><code>import os\nimport file_keeper as fk\n\ndef create_file_storage(env=None):\n    \"\"\"Create appropriate storage based on environment.\"\"\"\n    env = env or os.getenv('ENVIRONMENT', 'development')\n\n    if env == 'production':\n        return fk.make_storage(\"prod\", {\n            \"type\": \"file_keeper:s3\",\n            \"bucket\": os.getenv('S3_BUCKET'),\n            \"key\": os.getenv('AWS_ACCESS_KEY_ID'),\n            \"secret\": os.getenv('AWS_SECRET_ACCESS_KEY'),\n            \"region\": os.getenv('AWS_DEFAULT_REGION', 'us-east-1')\n        })\n    else:\n        # Use local storage for development/testing\n        return fk.make_storage(\"dev\", {\n            \"type\": \"file_keeper:fs\",\n            \"path\": \"./dev-storage\",\n            \"initialize\": True\n        })\n\n# Use the factory\nstorage = create_file_storage()\n</code></pre>"},{"location":"migration_guide/#3-testing-during-migration","title":"3. Testing During Migration","text":"<p>Maintain both old and new implementations temporarily and compare results:</p> <pre><code>import tempfile\nimport os\n\ndef test_migration_consistency():\n    \"\"\"Test that old and new implementations produce same results.\"\"\"\n\n    # Old implementation\n    with tempfile.TemporaryDirectory() as tmpdir:\n        old_filepath = os.path.join(tmpdir, \"test.txt\")\n        with open(old_filepath, 'wb') as f:\n            f.write(b\"test content\")\n\n        with open(old_filepath, 'rb') as f:\n            old_content = f.read()\n\n    # New implementation\n    storage = fk.make_storage(\"test\", {\n        \"type\": \"file_keeper:fs\",\n        \"path\": tmpdir,\n        \"initialize\": True\n    })\n\n    upload = fk.make_upload(b\"test content\")\n    file_info = storage.upload(\"test.txt\", upload)\n    new_content = storage.content(file_info)\n\n    # Compare\n    assert old_content == new_content\n    print(\"Migration test passed!\")\n</code></pre>"},{"location":"migration_guide/#common-migration-pitfalls-to-avoid","title":"Common Migration Pitfalls to Avoid","text":"<ol> <li>Don't forget to check capabilities - Always verify that operations are supported before calling them</li> <li>Handle exceptions properly - Use file-keeper's exception hierarchy</li> <li>Update your tests - Mock file-keeper storage instead of filesystem operations</li> <li>Consider data migration - Plan how to migrate existing files to new storage</li> <li>Update security policies - Ensure new storage backend has appropriate access controls</li> </ol> <p>By following these migration patterns, you can gradually transition your application to use file-keeper while maintaining functionality and gaining the benefits of a unified storage interface.</p>"},{"location":"quick_reference/","title":"Quick Reference","text":"<p>Common file-keeper operations at a glance.</p>"},{"location":"quick_reference/#installation","title":"Installation","text":"<pre><code>pip install file-keeper\n# With specific adapter support\npip install 'file-keeper[s3]'  # for S3\npip install 'file-keeper[all]'  # for all adapters\n</code></pre>"},{"location":"quick_reference/#basic-usage","title":"Basic Usage","text":"<pre><code>import file_keeper as fk\n\n# Create storage\nstorage = fk.make_storage(\"my_storage\", {\n    \"type\": \"file_keeper:memory\"  # or :fs, :s3, etc.\n})\n\n# Upload a file\nupload = fk.make_upload(b\"file content\")\nfile_info = storage.upload(\"filename.txt\", upload)\n\n# Read file content\ncontent = storage.content(file_info)\n\n# Check capabilities\nif storage.supports(fk.Capability.REMOVE):\n    storage.remove(file_info)\n</code></pre>"},{"location":"quick_reference/#common-operations","title":"Common Operations","text":"Operation Method Capability Required Upload file <code>storage.upload(location, upload)</code> <code>CREATE</code> Read content <code>storage.content(file_data)</code> <code>STREAM</code> Stream content <code>storage.stream(file_data)</code> <code>STREAM</code> Check existence <code>storage.exists(file_data)</code> <code>EXISTS</code> Remove file <code>storage.remove(file_data)</code> <code>REMOVE</code> Copy file <code>storage.copy(new_location, file_data)</code> <code>COPY</code> Move file <code>storage.move(new_location, file_data)</code> <code>MOVE</code> Get file info <code>storage.analyze(location)</code> <code>ANALYZE</code> List files <code>storage.scan()</code> <code>SCAN</code>"},{"location":"quick_reference/#error-handling","title":"Error Handling","text":"<pre><code>from file_keeper import exc\n\ntry:\n    # file operations\n    pass\nexcept exc.MissingFileError:\n    # File doesn't exist\n    pass\nexcept exc.ExistingFileError:\n    # File already exists\n    pass\nexcept exc.FilesError as e:\n    # Other file-keeper errors\n    pass\n</code></pre>"},{"location":"quick_reference/#common-configurations","title":"Common Configurations","text":""},{"location":"quick_reference/#file-system-storage","title":"File System Storage","text":"<pre><code>{\n    \"type\": \"file_keeper:fs\",\n    \"path\": \"/path/to/storage\",\n    \"initialize\": True,\n    \"override_existing\": False\n}\n</code></pre>"},{"location":"quick_reference/#s3-storage","title":"S3 Storage","text":"<pre><code>{\n    \"type\": \"file_keeper:s3\",\n    \"bucket\": \"my-bucket\",\n    \"key\": \"access-key\",\n    \"secret\": \"secret-key\",\n    \"region\": \"us-east-1\"\n}\n</code></pre>"},{"location":"quick_reference/#memory-storage-for-testing","title":"Memory Storage (for testing)","text":"<pre><code>{\n    \"type\": \"file_keeper:memory\"\n}\n</code></pre>"},{"location":"quick_reference/#capability-checks","title":"Capability Checks","text":"<pre><code># Single capability\nif storage.supports(fk.Capability.CREATE):\n    # Can create files\n\n# Multiple capabilities\nif storage.supports(fk.Capability.CREATE | fk.Capability.REMOVE):\n    # Can create and remove files\n\n# Check for read capabilities\nread_capable = storage.supports(fk.Capability.STREAM | fk.Capability.ANALYZE)\n</code></pre>"},{"location":"quick_reference/#working-with-uploads","title":"Working with Uploads","text":"<pre><code># From bytes\nupload = fk.make_upload(b\"content\")\n\n# From file\nwith open(\"file.txt\", \"rb\") as f:\n    upload = fk.make_upload(f)\n    # make sure to upload the file before with-block ends\n    ...\n\n# Manual upload (for streaming)\nupload = fk.Upload(stream, location, size, content_type)\n</code></pre>"},{"location":"quick_reference/#file-information","title":"File Information","text":"<p>The <code>FileData</code> object contains:</p> <ul> <li><code>location</code>: Where the file is stored</li> <li><code>size</code>: Size in bytes</li> <li><code>content_type</code>: MIME type</li> <li><code>hash</code>: Content hash</li> <li><code>storage_data</code>: Backend-specific metadata</li> </ul>"},{"location":"testing/","title":"Testing","text":"<p>This document outlines the recommended approach for writing tests for new storage adapters within the <code>file_keeper</code> project.  The goal is to ensure consistent behavior and adherence to the defined capabilities.</p>"},{"location":"testing/#core-principles","title":"Core Principles","text":"<p>Tests should primarily focus on verifying that an adapter correctly implements the capabilities. Validate the behavior of the adapter, not necessarily the internal implementation details. User has certain expectations depending on the storage capabilities and assumes that two different storage adapters with same capabilities behave identically in similar conditions. That's why testing publicly exposed interface is the highest priority.</p>"},{"location":"testing/#the-standard-class-and-inheritance","title":"The <code>Standard</code> Class and Inheritance","text":"<p>The <code>tests/file_keeper/default/adapters/standard.py</code> file provides a <code>Standard</code> class that serves as a foundation for testing adapters. This class encapsulates a comprehensive suite of tests covering various storage capabilities.</p> <p>Tip</p> <p>When creating tests for a new adapter, inherit from the <code>Standard</code> class. This automatically provides a significant amount of pre-built test coverage.  You only need to override or add tests to address specific adapter behavior or unique capabilities.</p> <p>The <code>Standard</code> class defines a series of test methods (e.g., <code>test_append_content</code>, <code>test_remove_real</code>, <code>test_move_missing</code>). Each test specifies capabilities required for verification and is automatically skipped if given capabilities are not supported by the storage.</p> <p>If all the required capabilities are supported, test verifies that implementation of capability is predictable. For example:</p> <ul> <li>REMOVE: returns <code>True</code> when removing real file and <code>False</code> if file does not exists</li> <li>CREATE: raises ExistingFileError if file   already exists at the given location and   override_existing option is not   enabled</li> <li>STREAM: output of stream() and   content() is the same</li> <li>ANALYZE: content hash of the file is the same as one, computed during the upload</li> </ul> <p>Note</p> <p><code>Standard</code> class covers only adapter-agnostic functionality and it does not verify internal processes of the storage.</p> <p>Testing that filesystem storage actually writes data into the configured directory and cloud storage communicates with the cloud provider is still required. But these tests must be implemented individually for each provider and they are not included into generic <code>Standard</code> class.</p>"},{"location":"testing/#writing-new-tests","title":"Writing New Tests","text":""},{"location":"testing/#create-a-test-class","title":"Create a Test Class","text":"<p>Create a new test class that inherits from <code>Standard</code>.</p> <pre><code>from tests.file_keeper.default.adapters.standard import Standard\nimport pytest\n\nclass MyStorageAdapterTests(Standard):\n    # Your tests here\n    pass\n</code></pre>"},{"location":"testing/#override-existing-tests-if-necessary","title":"Override Existing Tests (if necessary)","text":"<p>If your adapter has specific behavior in addition to the default implementation, override the corresponding test method in the <code>Standard</code> class. Be sure to call <code>super()</code> to execute the original test logic first, and then add your custom assertions.</p> <pre><code>class MyStorageAdapterTests(Standard):\n    def test_create_content_non_modified(self, storage: fk.Storage, faker: Faker):\n        super().test_create_content_non_modified(storage, faker)\n        # Add custom assertions specific to your adapter\n        assert \"some_adapter_specific_check\" in storage.content(result)\n</code></pre>"},{"location":"testing/#add-new-tests","title":"Add New Tests","text":"<p>If your adapter supports new capabilities or has unique behavior that is not covered by the <code>Standard</code> class, add new test methods to your test class.</p> <pre><code>class MyStorageAdapterTests(Standard):\n\n    def test_my_custom_behavior(self, storage: fk.Storage, faker: Faker):\n        # Your test logic here\n        assert True\n</code></pre>"},{"location":"adapters/azure_blob/","title":"Azure Blob Storage","text":"<p>The <code>file_keeper:azure_blob</code> adapter allows you to use Microsoft Azure Blob Storage for storing and retrieving files. This adapter leverages the <code>azure-storage-blob</code> Python library.</p>"},{"location":"adapters/azure_blob/#overview","title":"Overview","text":"<p>This adapter provides a convenient way to integrate Azure Blob Storage with file-keeper. You'll need to have the <code>azure-storage-blob</code> library installed and configure it with the appropriate credentials for your Azure account.</p> <pre><code>pip install 'file-keeper[azure]'\n\n## or\n\npip install azure-storage-blob\n</code></pre>"},{"location":"adapters/azure_blob/#initialization","title":"Initialization","text":"Flow <pre><code>graph TB\n    subgraph a [Client initialization]\n        direction TB\n        has_client --&gt;|Yes| set_url_and_name\n        has_client --&gt;|No| has_account_name\n        has_account_name --&gt;|Yes| initialize_client\n        has_account_name --&gt;|No| has_account_key\n        has_account_key --&gt;|Yes| initialize_client\n        has_account_key --&gt;|No| initialize_client\n        initialize_client --&gt; set_url_and_name\n    end\n    a --&gt; b\n    subgraph b [Container initialization]\n        direction TB\n\n        has_container --&gt;|Yes| set_container_name\n        has_container --&gt;|No| initialize_container\n        initialize_container --&gt; set_container_name\n    end\n    b --&gt; c\n    subgraph c [Container creation]\n        direction TB\n\n        container_exists --&gt;|Yes| done\n        container_exists --&gt;|No| has_initialize_flag\n        has_initialize_flag --&gt;|Yes| create_container\n        has_initialize_flag --&gt;|No| fail\n        create_container --&gt; done\n    end\n\n    has_client{**client** specified?}\n    has_account_name{**account_name** specified?}\n    has_account_key{**account_key** specified?}\n    set_url_and_name[**account_url** and **account_name** from the **client** object are added to settings]\n    has_container{**container** specified?}\n    has_initialize_flag{**initialize** enabled?}\n    initialize_client[**client** initialized using **account_url** and available credentials]\n    initialize_container[**container** reference created]\n    set_container_name[**container_name** from the **container** is added to settings]\n    create_container[real **container** created in cloud]\n    container_exists{real **container** exists in cloud?}\n    done([Storage initialized])\n    fail([Exception raised])</code></pre> <p>Here's an example of how to initialize the Azure Blob Storage adapter:</p> Azure Blob StorageAzurite <pre><code>storage = make_storage(\"my_azure_storage\", {\n    \"type\": \"file_keeper:azure_blob\",\n    \"account_name\": \"***\",\n    \"account_key\": \"***\",\n    \"container_name\": \"file-keeper\",\n    \"initialize\": True,\n})\n</code></pre> <pre><code>storage = make_storage(\"my_azure_storage\", {\n    \"type\": \"file_keeper:azure_blob\",\n    \"account_name\": \"devstoreaccount1\",\n    \"account_key\": \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n    \"container_name\": \"file-keeper\",\n    \"initialize\": True,\n    \"account_url\": \"http://127.0.0.1:10000/{account_name}\",\n})\n</code></pre>"},{"location":"adapters/azure_blob/#important-notes","title":"Important Notes","text":"<ul> <li>Replace the placeholder values with your actual Azure account credentials     and configuration.</li> <li>Ensure that you have created a container in your Azure Blob Storage account     to store the files.</li> <li>For enhanced security, consider using Azure Active Directory (Azure AD)     authentication instead of account keys.  Refer to the <code>azure-storage-blob</code>     documentation for details on Azure AD authentication.</li> <li>Refer to the Azure Blob Storage     documentation     for more information about Azure Blob Storage.</li> </ul>"},{"location":"adapters/emulate/","title":"Emulate cloud providers with Docker","text":"<p>For local development and testing, you can emulate cloud providers using Docker containers. This allows you to test your file-keeper integrations without incurring costs or requiring access to real cloud resources.</p> <p>Remember to adjust the port mappings and environment variables as needed for your specific setup.</p> MinIO (S3-compatible)Azurite (Azure Blob Storage)Fake GCS Server (Google Cloud Storage) <pre><code>docker run -d -p 9000:9000 -p 9001:9001 \\\n    --name minio \\\n    -e MINIO_PUBLIC_ADDRESS=0.0.0.0:9000 \\\n    quay.io/minio/minio server /data --console-address \":9001\"\n</code></pre> Attribute Value Endpoint http://127.0.0.1:9000 Key <code>minioadmin</code> Secret <code>minioadmin</code> <pre><code>docker run -d -p 10000:10000 \\\n    --name azurite-blob \\\n    mcr.microsoft.com/azure-storage/azurite azurite-blob --blobHost 0.0.0.0 --skipApiVersionCheck\n</code></pre> Attribute Value Endpoint http://127.0.0.1:10000 Key <code>devstoreaccount1</code> Secret <code>Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==</code> <pre><code>docker run -d -p 4443:4443 \\\n     --name gcs \\\n     fsouza/fake-gcs-server -scheme http\n</code></pre> Attribute Value Endpoint http://127.0.0.1:4443 Key Not required Secret Not required"},{"location":"adapters/fs/","title":"Local Filesystem Adapter","text":"<p>The <code>file_keeper:fs</code> adapter allows you to use your local filesystem for storing and retrieving files. This adapter is useful for testing, development, or scenarios where you need to store files locally.</p>"},{"location":"adapters/fs/#overview","title":"Overview","text":"<p>This adapter provides a simple way to interact with the local filesystem. You'll need to specify the base path where files will be stored.</p>"},{"location":"adapters/fs/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the local filesystem adapter:</p> <pre><code>storage = make_storage(\"my_local_storage\", {\n    \"type\": \"file_keeper:fs\",\n    \"path\": \"/tmp/file-keeper\",\n    \"initialize\": True,\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace <code>/tmp/file-keeper</code> with the desired base path on your system.</li> <li>The <code>initialize</code> option (defaulting to <code>False</code>) determines whether the     adapter should attempt to create the specified directory if it doesn't     exist. If <code>initialize</code> is <code>True</code> and the directory cannot be created (e.g.,     due to permissions issues), an error will be raised.</li> <li>Ensure that the process running file-keeper has the necessary permissions     to read and write to the specified directory.</li> </ul>"},{"location":"adapters/fsspec/","title":"Fsspec","text":"<p>The <code>file_keeper:fsspec</code> adapter uses the fsspec library to provide access to a wide range of storage backends supported by <code>fsspec</code> (e.g., local filesystems, cloud storage like S3, GCS, Azure Blob Storage, and more). The core idea is to use <code>fsspec</code>'s unified interface to interact with these different storage systems.</p>"},{"location":"adapters/fsspec/#overview","title":"Overview","text":"<pre><code>pip install 'file-keeper[fsspec]'\n\n## or\n\npip install fsspec\n</code></pre>"},{"location":"adapters/fsspec/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the fsspec adapter:</p> MemoryLocal FS <pre><code>storage = make_storage(\"my_fsspec_storage\", {\n     \"type\": \"file_keeper:fsspec\",\n     \"protocol\": \"memory\",\n})\n</code></pre> <pre><code>storage = make_storage(\"my_fsspec_storage\", {\n     \"type\": \"file_keeper:fsspec\",\n     \"protocol\": \"local\",\n     \"path\": \"/tmp/file-keeper\",\n     \"params\": {\"auto_mkdir\": True}\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the params with your actual fsspec configuration</li> <li>Ensure that you have the necessary credentials and permissions to access     the specified storage location.</li> </ul>"},{"location":"adapters/gcs/","title":"Google Cloud Storage","text":"<p>The <code>file_keeper:gcs</code> adapter allows you to use Google Cloud Storage for storing and retrieving files. This adapter leverages the <code>google-cloud-storage</code> Python library.</p>"},{"location":"adapters/gcs/#overview","title":"Overview","text":"<p>This adapter provides a convenient way to integrate Google Cloud Storage with file-keeper. You'll need to have the <code>google-cloud-storage</code> library installed and configure it with the appropriate credentials for your Google Cloud project.</p> <pre><code>pip install 'file-keeper[gcs]'\n\n## or\n\npip install google-cloud-storage\n</code></pre>"},{"location":"adapters/gcs/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the Google Cloud Storage adapter:</p> Google Cloud StorageFake GCS <pre><code>storage = make_storage(\"my_gcs_storage\", {\n    \"type\": \"file_keeper:gcs\",\n    \"project_id\": \"file-keeper\",  # Replace with your Google Cloud project ID\n    \"bucket_name\": \"file-keeper\",\n    \"initialize\": True,\n    \"credentials_file\": \"/path/to/your/credentials.json\",  # Replace with the path to your service account key file\n})\n</code></pre> <pre><code>storage = make_storage(\"my_gcs_storage\", {\n    \"type\": \"file_keeper:gcs\",\n    \"project_id\": \"file-keeper\",\n    \"bucket_name\": \"file-keeper\",\n    \"initialize\": True,\n    \"client_options\": {\"api_endpoint\": \"http://127.0.0.1:4443\"},\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values with your actual Google Cloud project ID,     GCS bucket name, and the path to your service account key file.</li> <li>Ensure that you have created a bucket in your Google Cloud Storage account     to store the files.</li> <li>For enhanced security, it's recommended to use a service account with     limited permissions.</li> <li>Refer to the Google Cloud Storage     documentation for more information     about Google Cloud Storage.</li> </ul>"},{"location":"adapters/libcloud/","title":"Apache Libcloud Adapter","text":"<p>The <code>file_keeper:libcloud</code> adapter allows you to use Apache Libcloud to connect to a wide range of cloud storage providers. Libcloud provides a unified interface for interacting with various storage services, simplifying integration and reducing vendor lock-in.</p>"},{"location":"adapters/libcloud/#overview","title":"Overview","text":"<p>This adapter leverages the Libcloud library to provide storage functionality. You'll need to have Libcloud installed and configure it with the appropriate credentials for your chosen provider.</p> <pre><code>pip install 'file-keeper[libcloud]'\n\n## or\n\npip install apache-libcloud\n</code></pre>"},{"location":"adapters/libcloud/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the Libcloud adapter:</p> AWS S3MinIOAzurite <pre><code>storage = make_storage(\"my_libcloud_storage\", {\n    \"type\": \"file_keeper:libcloud\",\n    \"provider\": \"S3\",\n    \"key\": \"***\",\n    \"secret\": \"***\",\n    \"container_name\": \"file-keeper\",\n    \"initialize\": True,\n})\n</code></pre> <pre><code>storage = make_storage(\"my_libcloud_storage\", {\n    \"type\": \"file_keeper:libcloud\",\n    \"provider\": \"MINIO\",\n    \"key\": \"minioadmin\",\n    \"secret\": \"minioadmin\",\n    \"container_name\": \"file-keeper\",\n    \"initialize\": True,\n    \"params\": {\n        \"host\": \"127.0.0.1\",\n        \"port\": 9000,\n        \"secure\": False,\n    },\n})\n</code></pre> <pre><code>storage = make_storage(\"my_libcloud_storage\", {\n    \"type\": \"file_keeper:libcloud\",\n    \"provider\": \"AZURE_BLOBS\",\n    \"key\": \"devstoreaccount1\",\n    \"secret\": \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n    \"container_name\": \"file-keeper\",\n    \"initialize\": True,\n    \"params\": {\n        \"host\": \"127.0.0.1\",\n        \"port\": 10000,\n        \"secure\": False,\n    },\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values (provider, host, port, secure, key, secret,     container_name) with your actual provider credentials and configuration.</li> <li>Refer to the Apache Libcloud     documentation for a complete list of     supported providers and their specific configuration options.</li> <li>Ensure that the necessary Libcloud drivers are installed for your chosen     provider.</li> </ul>"},{"location":"adapters/memory/","title":"In-Memory Storage Adapter","text":"<p>The <code>file_keeper:memory</code> adapter provides a simple in-memory storage solution. This adapter is primarily intended for testing and development purposes, as data is not persisted to disk.</p>"},{"location":"adapters/memory/#overview","title":"Overview","text":"<p>This adapter stores files entirely in memory, making it very fast but also volatile. Data is lost when the application exits. It's a convenient way to simulate a storage backend without requiring external dependencies.</p>"},{"location":"adapters/memory/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the in-memory storage adapter:</p> <pre><code>storage = make_storage(\"my_memory_storage\", {\n    \"type\": \"file_keeper:memory\",\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>No configuration options are required for this adapter.</li> <li>All data is stored in memory and will be lost when the application     terminates.</li> <li>This adapter is not suitable for production environments where data     persistence is required.</li> </ul>"},{"location":"adapters/null/","title":"Null Adapter","text":"<p>The <code>file_keeper:null</code> adapter provides a no-op storage solution. It does not actually store any files and is primarily intended for testing or situations where you want to disable storage functionality.</p>"},{"location":"adapters/null/#overview","title":"Overview","text":"<p>This adapter implements all the required storage interfaces but performs no real operations. Any attempt to upload, download, or manage files will be silently ignored. It's a useful tool for isolating issues or running tests without interacting with external storage.</p>"},{"location":"adapters/null/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the null adapter:</p> <pre><code>storage = make_storage(\"my_null_storage\", {\n    \"type\": \"file_keeper:null\",\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>No configuration options are required for this adapter.</li> <li>All operations are silently ignored.</li> <li>This adapter is not suitable for production environments where data     persistence is required.</li> </ul>"},{"location":"adapters/obstore/","title":"Object Store","text":"<p>The <code>file_keeper:obstore</code> adapter uses the obstore library to provide access to various object storage backends. It supports any backend supported by obstore, including local filesystem, S3, Google Cloud Storage, Azure Blob Storage, and more.</p>"},{"location":"adapters/obstore/#overview","title":"Overview","text":"<pre><code>pip install 'file-keeper[obstore]'\n\n## or\n\npip install obstore\n</code></pre>"},{"location":"adapters/obstore/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the obstore adapter:</p> MemoryLocal FS <pre><code>storage = make_storage(\"my_fsspec_storage\", {\n     \"type\": \"file_keeper:obstore\",\n     \"url\": \"memory:///\",\n})\n</code></pre> <pre><code>storage = make_storage(\"my_fsspec_storage\", {\n     \"type\": \"file_keeper:obstore\",\n     \"url\": \"file:///tmp/file-keeper\",\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the params with your actual obstore configuration</li> </ul>"},{"location":"adapters/opendal/","title":"Apache OpenDAL","text":"<p>The <code>file_keeper:opendal</code> adapter allows you to use Apache OpenDAL for storing and retrieving files. OpenDAL provides a unified interface for interacting with various object storage systems and local filesystems.</p>"},{"location":"adapters/opendal/#overview","title":"Overview","text":"<p>This adapter leverages the OpenDAL library to provide storage functionality. You'll need to have OpenDAL installed and configure it with the appropriate credentials and configuration for your chosen storage provider.</p> <pre><code>pip install 'file-keeper[opendal]'\n\n## or\n\npip install opendal\n</code></pre>"},{"location":"adapters/opendal/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the OpenDAL adapter:</p> MinIOAzuriteFake GCSLocal FS <pre><code>storage = make_storage(\"my_opendal_storage\", {\n     \"type\": \"file_keeper:opendal\",\n     \"scheme\": \"s3\",\n     \"params\": {\n         \"bucket\": \"file-keeper\",\n         \"access_key_id\": \"minioadmin\",\n         \"secret_access_key\": \"minioadmin\",\n         \"endpoint\": \"http://127.0.0.1:9000\",\n         \"region\": \"auto\"\n     },\n })\n</code></pre> <pre><code>storage = make_storage(\"my_opendal_storage\", {\n     \"type\": \"file_keeper:opendal\",\n     \"scheme\": \"azblob\",\n     \"params\": {\n         \"container\": \"file-keeper\",\n         \"account_name\": \"devstoreaccount1\",\n         \"account_key\": \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n         \"endpoint\": \"http://127.0.0.1:10000/devstoreaccount1\",\n     },\n })\n</code></pre> <pre><code>storage = make_storage(\"my_opendal_storage\", {\n     \"type\": \"file_keeper:opendal\",\n     \"scheme\": \"gcs\",\n     \"params\": {\n         \"bucket\": \"file-keeper\",\n         \"endpoint\": \"http://127.0.0.1:4443\",\n     },\n })\n</code></pre> <pre><code>storage = make_storage(\"my_opendal_storage\", {\n     \"type\": \"file_keeper:opendal\",\n     \"scheme\": \"fs\",\n     \"params\": {\n         \"root\": \"/tmp/file-keeper\",\n     },\n })\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the params with your actual OpenDAL location and credentials.</li> <li>Refer to the Apache OpenDAL     documentation for a complete list of     supported storage systems and their specific configuration options.</li> <li>Ensure that you have the necessary credentials and permissions to access     the specified storage location.</li> </ul>"},{"location":"adapters/redis/","title":"Redis Adapter","text":"<p>The <code>file_keeper:redis</code> adapter allows you to use Redis as a storage backend. This adapter stores files as binary data within Redis HASH.</p>"},{"location":"adapters/redis/#overview","title":"Overview","text":"<p>This adapter provides a simple way to integrate Redis with file-keeper. You'll need to have a running Redis instance and the <code>redis</code> Python library installed. This adapter is suitable for smaller files and scenarios where fast access is critical.</p> <pre><code>pip install 'file-keeper[redis]'\n\n## or\n\npip install redis\n</code></pre>"},{"location":"adapters/redis/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the Redis adapter:</p> <pre><code>storage = make_storage(\"my_redis_storage\", {\n    \"type\": \"file_keeper:redis\",\n    \"url\": \"redis://localhost:6379/0\",\n    \"bucket\": \"file-keeper\",\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values with your actual Redis configuration.</li> <li>Ensure that your Redis instance is running and accessible.</li> <li>Consider the limitations of storing large files in Redis, as it is an     in-memory data store.</li> <li>The <code>bucket</code> parameter is used as name of the Redis HASH key under which     files will be stored.</li> </ul>"},{"location":"adapters/s3/","title":"AWS S3 Adapter","text":"<p>The <code>file_keeper:s3</code> adapter allows you to use Amazon S3 for storing and retrieving files. This adapter leverages the <code>boto3</code> Python library.</p>"},{"location":"adapters/s3/#overview","title":"Overview","text":"<p>This adapter provides a convenient way to integrate AWS S3 with file-keeper. You'll need to have the <code>boto3</code> library installed and configure it with the appropriate credentials for your AWS account.</p> <pre><code>pip install 'file-keeper[s3]'\n\n## or\n\npip install boto3\n</code></pre>"},{"location":"adapters/s3/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the S3 adapter:</p> AWS S3MinIOEnvironment variablesAWS config fileExisting S3 client <pre><code>storage = make_storage(\"my_s3_storage\", {\n    \"type\": \"file_keeper:s3\",\n    \"key\": \"***\",\n    \"secret\": \"***\",\n    \"bucket\": \"file-keeper\",\n    \"initialize\": True,\n})\n</code></pre> <pre><code>storage = make_storage(\"my_s3_storage\", {\n    \"type\": \"file_keeper:s3\",\n    \"key\": \"minioadmin\",\n    \"secret\": \"minioadmin\",\n    \"bucket\": \"file-keeper\",\n    \"endpoint\": \"http://127.0.0.1:9000/\",\n    \"initialize\": True,\n})\n</code></pre> <p>Set environment variables mentioned in boto3 documentations. In this way you don't need to use hardcoded secrets.</p> <pre><code>export AWS_ACCESS_KEY_ID=minioadmin\nexport AWS_SECRET_ACCESS_KEY=minioadmin\nexport AWS_ENDPOINT_URL=http://127.0.0.1:9000\n</code></pre> <p>Then storage will automatically pick any missing options from envvars</p> <pre><code>storage = make_storage(\"my_s3_storage\", {\n    \"type\": \"file_keeper:s3\",\n    \"bucket\": \"file-keeper\",\n})\n</code></pre> <p>Just like environment variables, you can use configuration file. It will be located at <code>~/.aws/credentials</code> or a different location specified by <code>AWS_CONFIG_FILE</code>.</p> <p>Add default profile there</p> <pre><code>[default]\naws_access_key_id = minioadmin\naws_secret_access_key = minioadmin\nendpoint_url = http://127.0.0.1:9000\n</code></pre> <p>Or, if you want to use different profile name, set <code>AWS_PROFILE</code> with the name of preferred credentials profile. And now you can initialize the storage without specifying parameters that are set by the profile:</p> <pre><code>storage = make_storage(\"my_s3_storage\", {\n    \"type\": \"file_keeper:s3\",\n    \"bucket\": \"file-keeper\",\n})\n</code></pre> <p>If you already have initialized <code>boto3.client(\"s3\")</code>, you can just reuse it instead of specifying key and secret:</p> <pre><code>s3_client = boto3.client(\"s3\", ...) # somehow you've initialized it\n\nstorage = make_storage(\"my_s3_storage\", {\n    \"type\": \"file_keeper:s3\",\n    \"bucket\": \"file-keeper\",\n    \"client\": s3_client\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values (bucket, region, access_key_id,     secret_access_key) with your actual AWS account credentials and     configuration.</li> <li>Ensure that you have created a bucket in your AWS S3 account to store the     files.</li> <li>For enhanced security, consider using IAM roles instead of hardcoding     access keys.</li> <li>Refer to the AWS S3 documentation for more     information about AWS S3.</li> </ul>"},{"location":"adapters/sqlalchemy/","title":"SQLAlchemy Adapter","text":"<p>The <code>file_keeper:sqlalchemy</code> adapter allows you to use a relational database managed by SQLAlchemy for storing and retrieving file content. This adapter stores file content as binary data within the database.</p>"},{"location":"adapters/sqlalchemy/#overview","title":"Overview","text":"<p>This adapter provides a way to integrate a relational database with file-keeper. You'll need to have SQLAlchemy installed and a configured database connection string.  This adapter is suitable for scenarios where you need to store file metadata alongside the file content in a structured manner.</p> <pre><code>pip install 'file-keeper[sqlalchemy]'\n\n## or\n\npip install sqlalchemy\n</code></pre>"},{"location":"adapters/sqlalchemy/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the SQLAlchemy adapter:</p> <pre><code>storage = make_storage(\"my_sqlalchemy_storage\", {\n    \"type\": \"file_keeper:sqlalchemy\",\n    \"db_url\": \"sqlite:///:memory:\",\n    \"table_name\": \"file-keeper\",\n    \"location_column\": \"location\",\n    \"content_column\": \"content\",\n    \"initialize\": True,\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values with your actual database     connection string and table name.</li> <li>Enable <code>initialize</code> flag or ensure that the specified table exists in your     database.</li> <li>The database connection string should be in a format supported by     SQLAlchemy.  Refer to the SQLAlchemy     documentation for     details.</li> <li>Consider the performance implications of storing large files directly in     the database.</li> </ul>"},{"location":"adapters/zip/","title":"ZIP Archive Adapter","text":"<p>The <code>file_keeper:zip</code> adapter allows you to use a ZIP archive as a storage backend. This adapter is useful for packaging and distributing files in a single archive.</p>"},{"location":"adapters/zip/#overview","title":"Overview","text":"<p>This adapter stores files within a ZIP archive. You'll need to provide the <code>zip_path</code> to the ZIP archive.  The adapter can create the ZIP archive if it doesn't exist.</p>"},{"location":"adapters/zip/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the ZIP archive adapter:</p> <pre><code>storage = make_storage(\"my_zip_storage\", {\n    \"type\": \"file_keeper:zip\",\n    \"zip_path\": \"/tmp/my_archive.zip\",\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace <code>/tmp/my_archive.zip</code> with the desired path to your ZIP archive.</li> <li>This adapter is suitable for smaller files, as ZIP archives can become slow     to process with a large number of files.</li> </ul>"},{"location":"core_concepts/","title":"Core Concepts","text":"<p>file-keeper is built around three fundamental concepts that work together to provide a unified interface for file storage across multiple backends. Understanding these concepts is essential for effectively using the library.</p>"},{"location":"core_concepts/#overview","title":"Overview","text":"<p>The three core concepts are:</p> <ul> <li>Data Model: How file information is represented and managed</li> <li>Capabilities: How different storage backends declare their supported operations</li> <li>Uploads: How data flows from your application to storage</li> </ul> <p>These concepts work together to provide a consistent abstraction layer regardless of the underlying storage system.</p>"},{"location":"core_concepts/#how-they-work-together","title":"How They Work Together","text":"<ol> <li>Data Model provides the standardized representation of files (<code>FileData</code>) that remains consistent across all storage backends</li> <li>Capabilities enable the library to adapt its behavior based on what operations each storage system supports</li> <li>Uploads handle the transport of data from your application to the storage system, with appropriate buffering and integrity checking</li> </ol> <p>This architecture allows file-keeper to provide a uniform API while maximizing the efficiency of each storage backend's native capabilities.</p>"},{"location":"core_concepts/capabilities/","title":"Capabilities","text":"<p>The Capability enum defines the features supported by different storage backends. It's a fundamental concept in file-keeper, enabling the system to adapt to the limitations and strengths of each storage provider. This document explains the Capability enum, how to check for support, and provides practical examples.</p>"},{"location":"core_concepts/capabilities/#what-are-capabilities","title":"What are Capabilities?","text":"<p>Not all storage systems are created equal. Some support resumable uploads, while others don't. Some offer efficient copy and move operations, while others require you to handle this type of tasks yourself. Capabilities provide a standardized way to describe what a particular storage backend can do.</p> <p>The Capability enum is a set of flags, each representing a specific feature.  By checking which capabilities a Storage object possesses, file-keeper can dynamically adjust its behavior to ensure compatibility and optimal performance.</p>"},{"location":"core_concepts/capabilities/#the-capability-enum","title":"The Capability Enum","text":"<p>Here's a breakdown of the key capabilities:</p> Capability Description ANALYZE Return file details from the storage. APPEND Add content to the existing file. COMPOSE Combine multiple files into a new one in the same storage. COPY Make a copy of the file inside the same storage. CREATE Create a file as an atomic object. EXISTS Check if file exists. LINK_PERMANENT Make permanent download link. LINK_TEMPORAL Make expiring download link. LINK_ONE_TIME Make one-time download link. MOVE Move file to a different location inside the same storage. MULTIPART Upload fragments in parallel and combine them into a file in the end. RANGE Return specific range of bytes from the file. REMOVE Remove file from the storage. RESUMABLE Perform resumable uploads that can be continued after interruption. SCAN Iterate over all files in the storage. SIGNED Generate signed URL for specific operation. STREAM Return file content as stream of bytes."},{"location":"core_concepts/capabilities/#checking-for-capability-support","title":"Checking for Capability Support","text":"<p>You can determine if a Storage object supports a specific capability using the supports() method:</p> <pre><code>from file_keeper import Capability, make_storage, make_upload\n\nstorage = make_storage(\"my_storage\", {\"adapter\": \"s3\", \"region\": \"us-east-1\"})\n\nif storage.supports(Capability.REMOVE | Capability.CREATE):\n    upload = make_upload(b\"hello world\")\n    info = storage.upload(\"hello.txt\", upload)\n    storage.remove(info)\n\nelse:\n    raise TypeError(\"File creation and removal is not supported by this storage.\")\n\n\nif storage.supports(Capability.EXISTS):\n    assert not storage.exists(info)\n\nelse:\n    print(\"File existence cannot be checked by this storage.\")\n</code></pre> <p>The <code>supports()</code> method returns <code>True</code> if the storage backend has the specified capability, and <code>False</code> otherwise.</p>"},{"location":"core_concepts/data_model/","title":"Data model","text":"<p>This document details the core data models used within the file-keeper system: Storage, FileData, and Location. It explains the why behind these concepts and how they work together to provide a flexible and reliable file storage solution.</p>"},{"location":"core_concepts/data_model/#storage","title":"Storage","text":"<p>The Storage class is the central point of interaction with any underlying storage system \u2013 whether that's a cloud provider like AWS S3 or Google Cloud Storage, a local filesystem, or something else entirely. Think of it as an adapter that translates file-keeper's generic requests into the specific language of the storage backend.</p> <p>Instead of directly dealing with the complexities of each storage system, you interact with Storage objects.  file-keeper handles the details of connecting, authenticating, and performing operations.  Each Storage instance is equipped with specialized services \u2013 Uploader, Reader, and Manager \u2013 to handle different types of tasks.  This separation of concerns makes the system more modular and easier to extend.</p>"},{"location":"core_concepts/data_model/#filedata","title":"FileData","text":"<p>FileData represents a file as known to file-keeper. It's a metadata record that contains everything we need to identify and manage a file, regardless of where it's physically stored.</p> <p>Crucially, FileData is independent of the underlying storage. It's a consistent representation that allows file-keeper to work with different storage backends seamlessly.  It tracks essential information like the file's name, size, content type, and content hash.  It can also store additional, storage-specific metadata in the storage_data field.</p> <p>FileData is the key to operations like tracking progress during uploads, managing file versions, and providing consistent access to file information across different storage systems.</p>"},{"location":"core_concepts/data_model/#location","title":"Location","text":"<p>Location represents the address of a file within a specific storage system.  It's a simple string, but it's given a specific meaning within file-keeper.</p> <p>Think of it as a path or key that uniquely identifies a file within its storage backend.  The format of a Location will vary depending on the storage system (e.g., an S3 key, a filesystem path).</p> <p>file-keeper uses Location objects to tell the Storage adapter where to find a file.  It also provides a mechanism for transforming Location objects to different formats if needed, allowing for flexibility and compatibility with different storage systems.</p>"},{"location":"core_concepts/uploads/","title":"Upload","text":"<p>The Upload class represents the data you want to store in a storage backend. It's a key component in file-keeper, encapsulating the file's content, metadata, and instructions for how to transfer it to storage. This document explains how to create and use Upload objects, covering streaming uploads and hashing for data integrity.</p>"},{"location":"core_concepts/uploads/#what-is-an-upload","title":"What is an Upload?","text":"<p>Think of an Upload as a package containing everything needed to send a file to storage. It's more than just the raw data; it includes information about the file itself, like its name, size, and content type. This metadata is essential for proper storage and retrieval.</p> <p>The Upload object decouples the source of the data from the transfer process. This allows file-keeper to handle various data sources \u2013 files on disk, in-memory buffers, network streams \u2013 without changing the core storage logic.</p>"},{"location":"core_concepts/uploads/#creating-an-upload-object","title":"Creating an Upload Object","text":"<p>You can create an Upload object in several ways, depending on the source of your data. The recommended way is using make_upload helper:</p> File-like objectByte stringWerkzeug's FileStorageManually <p>If you have an open file, you can directly pass it to the make_upload function. file-keeper will handle reading the data from the file.</p> <pre><code>src = open(\"my_image.jpg\", \"rb\")\nupload = make_upload(src)\n</code></pre> <p>If your data is already in memory as a byte string, you can pass it directly.</p> <pre><code>data = b\"This is the content of my file.\"\nupload = make_upload(data)\n</code></pre> <p>When writing an application using werkzeug-based framework you can handle uploaded files in this way.</p> <pre><code>from werkzeug.datastructures import FileStorage\n\ndata = FileStorage(..., \"my_data.txt\")\nupload = make_upload(data)\n</code></pre> <p>This is useful for large files that don't fit in memory. You need to provide an object that has methods <code>read</code> and <code>__iter__</code> producing byte string as a first argument to Upload class. If you have a generator that yields data, wrap it into IterableBytesReader instead of manually implementing class with required methods:</p> <pre><code>from file_keeper import Upload, IterableBytesReader\n\ndef data_generator():\n    yield b\"hello\"\n    yield b\" \"\n    yield b\"world\"\n\nstream = IterableBytesReader(data_generator())\nupload = Upload(stream, \"my_file.txt\", 11, \"text/plain\")\n</code></pre> <p>The make_upload function automatically determines the file size and content type for supported source types.</p>"},{"location":"core_concepts/uploads/#streaming-uploads","title":"Streaming Uploads","text":"<p>For very large files, loading the entire content into memory is impractical. Streaming uploads allow you to send the data in chunks, reducing memory usage and improving performance.</p> <p>As shown in the example above, you can create an Upload object from an iterable of bytes. file-keeper will then stream the data to the storage backend as it becomes available. This is the preferred method for handling large files, but it expects that you compute the size and content type of the upload in advance and provide these details to the Upload constructor.</p>"},{"location":"core_concepts/uploads/#hashing-for-data-integrity","title":"Hashing for Data Integrity","text":"<p>Data integrity is crucial when transferring files. Hashing ensures that the data you upload is exactly the same as the data stored in the backend. file-keeper automatically calculates a hash of the upload data during the transfer process.</p> <p>The calculated hash is stored as part of the FileData metadata. When you retrieve the file, file-keeper can recalculate the hash via Storage.analyze method to verify data integrity. If the hashes don't match, it indicates that the file has been corrupted or tampered with.</p> <p>Hashing is performed transparently during the upload process, so you don't need to worry about implementing it yourself. It provides an extra layer of assurance that your data is stored reliably.</p>"},{"location":"examples/","title":"file-keeper Examples","text":"<p>This directory contains comprehensive examples demonstrating various use cases and patterns for file-keeper.</p>"},{"location":"examples/#available-examples","title":"Available Examples","text":""},{"location":"examples/#1-web-application","title":"1. Web Application","text":"<p>A complete Flask web application showing file uploads, downloads, and management with security best practices.</p>"},{"location":"examples/#2-file-migration","title":"2. File Migration","text":"<p>Examples of migrating files between different storage backends, including validation and error handling.</p>"},{"location":"examples/#3-batch-processing","title":"3. Batch Processing","text":"<p>Demonstrations of efficient batch operations, parallel processing, and performance optimization techniques.</p>"},{"location":"examples/#4-custom-adapters","title":"4. Custom Adapters","text":"<p>Examples of creating custom storage adapters for specialized backends like SQLite databases and encrypted storage.</p>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<p>Each example directory contains: - Complete, runnable code - Detailed README with usage instructions - Requirements file if needed - Best practices and patterns</p> <p>To run an example:</p> <ol> <li> <p>Navigate to the example directory: <pre><code>cd docs/examples/web_app\n</code></pre></p> </li> <li> <p>Install dependencies (if any): <pre><code>pip install -r requirements.txt\n</code></pre></p> </li> <li> <p>Run the example: <pre><code>python app.py\n</code></pre></p> </li> </ol>"},{"location":"examples/#use-cases-covered","title":"Use Cases Covered","text":"<p>These examples demonstrate: - Basic file operations (upload, download, delete) - Security best practices - Performance optimization - Error handling and resilience - Migration between storage backends - Custom adapter development - Integration with web frameworks - Batch processing patterns</p>"},{"location":"examples/#contributing","title":"Contributing","text":"<p>Feel free to contribute additional examples! Each example should be: - Self-contained and runnable - Well-documented - Following security best practices - Including error handling - Demonstrating real-world use cases</p>"},{"location":"examples/batch_processing/","title":"Batch Processing Example","text":"<p>This example demonstrates how to efficiently process multiple files in batch using file-keeper.</p>"},{"location":"examples/batch_processing/#overview","title":"Overview","text":"<p>This script shows:</p> <ul> <li>Batch uploading of multiple files</li> <li>Parallel processing of file operations</li> <li>Performance comparison between parallel and sequential processing</li> <li>Advanced batch operations with filtering and grouping</li> <li>Error handling in batch operations</li> </ul>"},{"location":"examples/batch_processing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>file-keeper library</li> </ul>"},{"location":"examples/batch_processing/#installation","title":"Installation","text":"<pre><code>pip install file-keeper\n</code></pre>"},{"location":"examples/batch_processing/#usage","title":"Usage","text":""},{"location":"examples/batch_processing/#run-the-basic-batch-processing-example","title":"Run the basic batch processing example:","text":"<pre><code>python batch_example.py\n</code></pre> <p>This will:</p> <ol> <li>Create sample files</li> <li>Demonstrate batch upload operations</li> <li>Show filtering and metadata extraction</li> <li>Compare parallel vs sequential processing</li> <li>Demonstrate advanced batch operations</li> </ol>"},{"location":"examples/batch_processing/#key-features-demonstrated","title":"Key Features Demonstrated","text":""},{"location":"examples/batch_processing/#1-basic-batch-upload","title":"1. Basic Batch Upload","text":"<pre><code>from batch_example import BatchProcessor\n\nprocessor = BatchProcessor(storage)\nfiles_to_upload = [\n    (\"file1.txt\", b\"content1\"),\n    (\"file2.txt\", b\"content2\"),\n    (\"file3.txt\", b\"content3\"),\n]\n\nresults = processor.upload_batch(files_to_upload)\n</code></pre>"},{"location":"examples/batch_processing/#2-parallel-processing","title":"2. Parallel Processing","text":"<pre><code># Process multiple files in parallel\nprocessor = BatchProcessor(storage, max_workers=4)\nresults = processor.process_batch(my_operation, file_locations)\n</code></pre>"},{"location":"examples/batch_processing/#3-batch-downloads","title":"3. Batch Downloads","text":"<pre><code># Download multiple files efficiently\ncontents = processor.download_batch(locations)\n</code></pre>"},{"location":"examples/batch_processing/#4-batch-deletion","title":"4. Batch Deletion","text":"<pre><code># Delete multiple files in batch\nresults = processor.delete_batch(locations)\n</code></pre>"},{"location":"examples/batch_processing/#performance-optimization","title":"Performance Optimization","text":"<p>The example demonstrates how parallel processing can significantly improve performance:</p> <pre><code># Sequential processing\nstart_time = time.time()\nfor location in locations:\n    process_file_sequential(storage, location)\nsequential_time = time.time() - start_time\n\n# Parallel processing\nstart_time = time.time()\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    futures = [executor.submit(process_file_parallel, storage, loc) for loc in locations]\n    for future in futures:\n        future.result()\nparallel_time = time.time() - start_time\n\nprint(f\"Speedup: {sequential_time/parallel_time:.2f}x\")\n</code></pre>"},{"location":"examples/batch_processing/#error-handling","title":"Error Handling","text":"<p>Batch operations include comprehensive error handling:</p> <pre><code>def handle_batch_errors(results):\n    successful = [r for r in results if r.success]\n    failed = [r for r in results if not r.success]\n\n    print(f\"Successful: {len(successful)}\")\n    print(f\"Failed: {len(failed)}\")\n\n    for result in failed:\n        print(f\"Error processing {result.location}: {result.error}\")\n</code></pre>"},{"location":"examples/batch_processing/#real-world-use-cases","title":"Real-World Use Cases","text":""},{"location":"examples/batch_processing/#1-bulk-file-import","title":"1. Bulk File Import","text":"<pre><code>def import_bulk_files(storage, file_paths):\n    files_to_upload = []\n    for path in file_paths:\n        with open(path, 'rb') as f:\n            content = f.read()\n        files_to_upload.append((Path(path).name, content))\n\n    processor = BatchProcessor(storage)\n    return processor.upload_batch(files_to_upload)\n</code></pre>"},{"location":"examples/batch_processing/#2-media-processing-pipeline","title":"2. Media Processing Pipeline","text":"<pre><code>def process_media_pipeline(storage):\n    # Get all image files\n    image_files = filter_files_by_criteria(\n        storage,\n        lambda info: info.content_type.startswith('image/')\n    )\n\n    # Process images in parallel\n    processor = BatchProcessor(storage)\n    results = processor.process_batch(resize_image, image_files)\n\n    return results\n</code></pre>"},{"location":"examples/batch_processing/#3-data-migration","title":"3. Data Migration","text":"<pre><code>def migrate_data_batch(source_storage, dest_storage, batch_size=100):\n    all_files = list(source_storage.scan())\n\n    for i in range(0, len(all_files), batch_size):\n        batch = all_files[i:i+batch_size]\n\n        # Process batch\n        processor = BatchProcessor(source_storage)\n        contents = processor.download_batch(batch)\n\n        # Upload to destination\n        dest_processor = BatchProcessor(dest_storage)\n        upload_batch = [(loc, contents[loc]) for loc in batch if loc in contents]\n        results = dest_processor.upload_batch(upload_batch)\n\n        print(f\"Processed batch {i//batch_size + 1}/{(len(all_files)-1)//batch_size + 1}\")\n</code></pre>"},{"location":"examples/batch_processing/#best-practices","title":"Best Practices","text":"<ol> <li>Use appropriate worker counts: Match the number of workers to your storage backend's capabilities</li> <li>Handle errors gracefully: Always check batch operation results</li> <li>Monitor resource usage: Batch operations can consume significant memory</li> <li>Validate results: Ensure batch operations completed successfully</li> <li>Use filters: Pre-filter files to reduce unnecessary processing</li> </ol>"},{"location":"examples/batch_processing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>For I/O-bound operations, parallel processing provides significant speedups</li> <li>For CPU-bound operations, consider using multiprocessing instead of threading</li> <li>Monitor memory usage when processing large batches</li> <li>Consider using streaming for very large files to avoid memory issues</li> </ul>"},{"location":"examples/custom_adapter/","title":"Custom Adapter Example","text":"<p>This example demonstrates how to create custom storage adapters for file-keeper.</p>"},{"location":"examples/custom_adapter/#overview","title":"Overview","text":"<p>This script shows:</p> <ul> <li>How to create a SQLite-based storage adapter</li> <li>How to create an encrypted file storage adapter</li> <li>Best practices for adapter development</li> <li>Minimal adapter implementation example</li> </ul>"},{"location":"examples/custom_adapter/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>file-keeper library</li> <li>cryptography library (for encrypted adapter)</li> </ul>"},{"location":"examples/custom_adapter/#installation","title":"Installation","text":"<pre><code>pip install file-keeper cryptography\n</code></pre>"},{"location":"examples/custom_adapter/#usage","title":"Usage","text":""},{"location":"examples/custom_adapter/#run-the-custom-adapter-examples","title":"Run the custom adapter examples:","text":"<pre><code>python custom_adapter_example.py\n</code></pre> <p>This will:</p> <ol> <li>Demonstrate the SQLite storage adapter</li> <li>Demonstrate the encrypted storage adapter</li> <li>Show best practices for adapter development</li> <li>Provide a minimal adapter example</li> </ol>"},{"location":"examples/custom_adapter/#key-concepts","title":"Key Concepts","text":""},{"location":"examples/custom_adapter/#1-adapter-structure","title":"1. Adapter Structure","text":"<p>Every adapter consists of:</p> <ul> <li>Settings: Configuration class inheriting from <code>fk.Settings</code></li> <li>Uploader: Handles file uploads (inherits from <code>fk.Uploader</code>)</li> <li>Manager: Handles file management (inherits from <code>fk.Manager</code>)</li> <li>Reader: Handles file reading (inherits from <code>fk.Reader</code>)</li> <li>Storage: Main class combining all components (inherits from <code>fk.Storage</code>)</li> </ul>"},{"location":"examples/custom_adapter/#2-capability-system","title":"2. Capability System","text":"<p>Adapters declare their capabilities by setting the <code>capabilities</code> attribute:</p> <pre><code>class MyUploader(Uploader):\n    capabilities = Capability.CREATE\n\nclass MyManager(Manager):\n    capabilities = Capability.EXISTS | Capability.ANALYZE | Capability.REMOVE\n\nclass MyReader(Reader):\n    capabilities = Capability.STREAM | Capability.RANGE\n</code></pre>"},{"location":"examples/custom_adapter/#3-sqlite-adapter-example","title":"3. SQLite Adapter Example","text":"<pre><code>class SQLiteStorage(Storage):\n    SettingsFactory = SQLiteSettings\n    UploaderFactory = SQLiteUploader\n    ManagerFactory = SQLiteManager\n    ReaderFactory = SQLiteReader\n\n    def __init__(self, settings):\n        super().__init__(settings)\n        # Initialize database connection\n        self.db = sqlite3.connect(settings.db_path)\n</code></pre>"},{"location":"examples/custom_adapter/#creating-your-own-adapter","title":"Creating Your Own Adapter","text":""},{"location":"examples/custom_adapter/#step-1-define-settings","title":"Step 1: Define Settings","text":"<pre><code>from file_keeper import Settings\nimport dataclasses\n\n@dataclasses.dataclass\nclass MyCustomSettings(Settings):\n    host: str = \"localhost\"\n    port: int = 8080\n    username: str = \"\"\n    password: str = \"\"\n    _required_options = [\"host\"]  # Required fields\n</code></pre>"},{"location":"examples/custom_adapter/#step-2-implement-services","title":"Step 2: Implement Services","text":"<pre><code>from file_keeper import Uploader, Manager, Reader, Capability\n\nclass MyUploader(Uploader):\n    capabilities = Capability.CREATE\n\n    def upload(self, location, upload, extras):\n        # Implement upload logic\n        pass\n\nclass MyManager(Manager):\n    capabilities = Capability.EXISTS | Capability.ANALYZE | Capability.REMOVE\n\n    def exists(self, data, extras):\n        # Implement existence check\n        pass\n\n    def analyze(self, location, extras):\n        pass\n\n    def remove(self, data, extras):\n        pass\n\nclass MyReader(Reader):\n    capabilities = Capability.STREAM\n\n    def stream(self, data, extras):\n        # Implement streaming logic\n        pass\n</code></pre>"},{"location":"examples/custom_adapter/#step-3-create-storage-class","title":"Step 3: Create Storage Class","text":"<pre><code>from file_keeper import Storage\n\nclass MyCustomStorage(Storage):\n    SettingsFactory = MyCustomSettings\n    UploaderFactory = MyUploader\n    ManagerFactory = MyManager\n    ReaderFactory = MyReader\n\n    def __init__(self, settings):\n        super().__init__(settings)\n        # Initialize your backend connection\n        self.client = self.initialize_backend()\n\n    def initialize_backend(self):\n        # Initialize your storage backend\n        pass\n</code></pre>"},{"location":"examples/custom_adapter/#step-4-register-the-adapter","title":"Step 4: Register the Adapter","text":"<pre><code>import file_keeper as fk\n\n# initialize adapter registry. This is not required when storages\n# are registered via package entry-points.\nfk.ext.setup()\n\n# Register your adapter\nfk.adapters.register(\"file_keeper:myadapter\", MyCustomStorage)\n\n# Now you can use it\nstorage = fk.make_storage(\"my_storage\", {\n    \"type\": \"file_keeper:myadapter\",\n    \"host\": \"myserver.com\",\n    \"port\": 9000\n})\n</code></pre>"},{"location":"examples/custom_adapter/#best-practices","title":"Best Practices","text":"<ol> <li>Define appropriate settings: Include validation and required fields</li> <li>Implement only supported capabilities: Don't claim capabilities your backend doesn't support</li> <li>Handle errors properly: Use file-keeper's exception hierarchy</li> <li>Respect security: Validate paths and implement proper access controls</li> <li>Test thoroughly: Test with the standard test suite</li> <li>Document limitations: Clearly document what your adapter supports</li> <li>Follow interface contracts: Maintain compatibility with file-keeper interfaces</li> <li>Consider thread safety: If your backend supports concurrent access</li> </ol>"},{"location":"examples/custom_adapter/#security-considerations","title":"Security Considerations","text":"<p>When creating custom adapters, pay attention to: - Path traversal prevention - Authentication and authorization - Data encryption at rest - Secure credential handling - Input validation</p>"},{"location":"examples/custom_adapter/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Minimize network calls when possible</li> <li>Implement efficient streaming for large files</li> <li>Use appropriate caching strategies</li> <li>Consider connection pooling for network backends</li> <li>Optimize for your specific use case</li> </ul>"},{"location":"examples/custom_adapter/#testing-your-adapter","title":"Testing Your Adapter","text":"<p>Create tests that verify:</p> <ul> <li>All claimed capabilities work correctly</li> <li>Error conditions are handled properly</li> <li>Security measures are effective</li> <li>Performance meets requirements</li> <li>Edge cases are handled</li> </ul>"},{"location":"examples/file_migration/","title":"File Migration Example","text":"<p>This example demonstrates how to migrate files between different storage backends using file-keeper.</p>"},{"location":"examples/file_migration/#overview","title":"Overview","text":"<p>This script shows:</p> <ul> <li>How to migrate files from one storage backend to another</li> <li>Validation of migrated files</li> <li>Error handling during migration</li> <li>Selective migration based on filters</li> <li>Examples for cloud storage migrations</li> </ul>"},{"location":"examples/file_migration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>file-keeper library</li> </ul>"},{"location":"examples/file_migration/#installation","title":"Installation","text":"<pre><code>pip install file-keeper\n</code></pre>"},{"location":"examples/file_migration/#usage","title":"Usage","text":""},{"location":"examples/file_migration/#run-the-basic-migration-example","title":"Run the basic migration example:","text":"<pre><code>python migration_example.py\n</code></pre> <p>This will:</p> <ol> <li>Create sample files in memory storage</li> <li>Migrate them to temporary filesystem storage</li> <li>Validate the migration</li> <li>Show results</li> </ol>"},{"location":"examples/file_migration/#key-features-demonstrated","title":"Key Features Demonstrated","text":""},{"location":"examples/file_migration/#1-basic-migration","title":"1. Basic Migration","text":"<pre><code>from file_migration import migrate_files\n\nsuccessful, failed = migrate_files(source_storage, dest_storage)\n</code></pre>"},{"location":"examples/file_migration/#2-filtered-migration","title":"2. Filtered Migration","text":"<pre><code># Only migrate .txt files\ndef txt_filter(location: str) -&gt; bool:\n    return location.endswith('.txt')\n\nsuccessful, failed = migrate_files(\n    source_storage,\n    dest_storage,\n    file_filter=txt_filter\n)\n</code></pre>"},{"location":"examples/file_migration/#3-validated-migration","title":"3. Validated Migration","text":"<pre><code># Enable validation to ensure file integrity\nsuccessful, failed = migrate_files(\n    source_storage,\n    dest_storage,\n    validate_after_migration=True\n)\n</code></pre>"},{"location":"examples/file_migration/#real-world-usage","title":"Real-World Usage","text":""},{"location":"examples/file_migration/#migrating-from-s3-to-google-cloud-storage","title":"Migrating from S3 to Google Cloud Storage:","text":"<pre><code>import file_keeper as fk\n\n# Source: S3\ns3_source = fk.make_storage(\"s3_source\", {\n    \"type\": \"file_keeper:s3\",\n    \"bucket\": \"my-source-bucket\",\n    \"key\": os.getenv('AWS_ACCESS_KEY_ID'),\n    \"secret\": os.getenv('AWS_SECRET_ACCESS_KEY'),\n    \"region\": \"us-east-1\"\n})\n\n# Destination: Google Cloud Storage\ngcs_dest = fk.make_storage(\"gcs_dest\", {\n    \"type\": \"file_keeper:gcs\",\n    \"bucket_name\": \"my-dest-bucket\",\n    \"credentials_file\": \"/path/to/gcs-credentials.json\"\n})\n\n# Perform migration\nsuccessful, failed = migrate_files(s3_source, gcs_dest)\n</code></pre>"},{"location":"examples/file_migration/#migrating-with-size-limits","title":"Migrating with size limits:","text":"<pre><code>def size_filter(location: str) -&gt; bool:\n    # Skip files larger than 100MB\n    file_info = source_storage.analyze(location)\n    return file_info.size &lt;= (100 * 1024 * 1024)\n\nsuccessful, failed = migrate_files(\n    source_storage,\n    dest_storage,\n    file_filter=size_filter\n)\n</code></pre>"},{"location":"examples/file_migration/#error-handling","title":"Error Handling","text":"<p>The migration function handles various error conditions:</p> <ul> <li>Network interruptions</li> <li>Permission issues</li> <li>Storage capacity limits</li> <li>Invalid file formats</li> </ul> <p>Failed migrations are logged and returned separately for further processing.</p>"},{"location":"examples/file_migration/#security-considerations","title":"Security Considerations","text":"<ul> <li>Files are validated after migration to ensure integrity</li> <li>Content is compared between source and destination</li> <li>Filters can be used to prevent migration of unsafe file types</li> <li>All operations respect storage capabilities</li> </ul>"},{"location":"examples/file_migration/#performance-tips","title":"Performance Tips","text":"<ul> <li>For large migrations, consider batching operations</li> <li>Use appropriate storage backends for temporary files</li> <li>Monitor storage quotas during migration</li> <li>Consider using multipart uploads for large files</li> </ul>"},{"location":"examples/web_app/","title":"File Upload Web Application Example","text":"<p>This example demonstrates how to use file-keeper in a web application with Flask.</p>"},{"location":"examples/web_app/#overview","title":"Overview","text":"<p>This Flask application shows:</p> <ul> <li>File uploads using file-keeper</li> <li>Security best practices</li> <li>Error handling</li> <li>Different storage backends (filesystem, memory)</li> </ul>"},{"location":"examples/web_app/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>pip</li> </ul>"},{"location":"examples/web_app/#installation","title":"Installation","text":"<ol> <li>Install dependencies: <pre><code>pip install -r requirements.txt\n</code></pre></li> </ol>"},{"location":"examples/web_app/#usage","title":"Usage","text":""},{"location":"examples/web_app/#run-with-filesystem-storage-default","title":"Run with Filesystem Storage (Default)","text":"<pre><code>python app.py\n</code></pre>"},{"location":"examples/web_app/#run-with-memory-storage-for-testing","title":"Run with Memory Storage (for testing)","text":"<pre><code>STORAGE_TYPE=memory python app.py\n</code></pre>"},{"location":"examples/web_app/#run-with-custom-storage-path","title":"Run with Custom Storage Path","text":"<pre><code>STORAGE_PATH=/path/to/uploads python app.py\n</code></pre>"},{"location":"examples/web_app/#api-endpoints","title":"API Endpoints","text":"<ul> <li><code>POST /upload</code> - Upload a file</li> <li><code>GET /files</code> - List all files</li> <li><code>GET /file/&lt;filename&gt;</code> - Download a file</li> <li><code>DELETE /file/&lt;filename&gt;</code> - Delete a file</li> <li><code>GET /health</code> - Health check</li> </ul>"},{"location":"examples/web_app/#example-usage","title":"Example Usage","text":""},{"location":"examples/web_app/#upload-a-file","title":"Upload a file:","text":"<pre><code>curl -X POST -F \"file=@myfile.txt\" http://localhost:5000/upload\n</code></pre>"},{"location":"examples/web_app/#list-files","title":"List files:","text":"<pre><code>curl http://localhost:5000/files\n</code></pre>"},{"location":"examples/web_app/#download-a-file","title":"Download a file:","text":"<pre><code>curl http://localhost:5000/file/myfile.txt -O\n</code></pre>"},{"location":"examples/web_app/#security-features","title":"Security Features","text":"<ul> <li>Filename sanitization using werkzeug's secure_filename</li> <li>File type validation</li> <li>File size limits</li> <li>Directory traversal prevention</li> <li>Capability checking before operations</li> </ul>"},{"location":"examples/web_app/#configuration","title":"Configuration","text":"<p>You can configure the application using environment variables:</p> <ul> <li><code>STORAGE_TYPE</code>: Storage backend ('fs' for filesystem, 'memory' for in-memory)</li> <li><code>STORAGE_PATH</code>: Path for filesystem storage (defaults to './uploads')</li> </ul>"},{"location":"examples/web_app/#testing","title":"Testing","text":"<p>The application includes basic functionality testing. For comprehensive testing, consider using pytest with the following test structure:</p> <pre><code># test_app.py\nimport pytest\nfrom app import app\n\n@pytest.fixture\ndef client():\n    app.config['TESTING'] = True\n    with app.test_client() as client:\n        yield client\n\ndef test_health(client):\n    rv = client.get('/health')\n    assert b'healthy' in rv.data\n</code></pre>"},{"location":"extending/adapters/","title":"Storage adapters","text":"<p>The core of file-keeper's flexibility lies in its storage adapters. Adapters encapsulate the logic for interacting with a specific storage system, allowing file-keeper to remain agnostic to the underlying implementation.  To create a custom adapter, you'll need to define a class that inherits from Storage and implements its services.</p>"},{"location":"extending/adapters/#steps-to-create-a-custom-adapter","title":"Steps to create a custom adapter","text":""},{"location":"extending/adapters/#define-a-settings-class","title":"Define a Settings class","text":"<p>Create a dataclass to hold the configuration options for your storage adapter. This class should inherit from Settings.  This allows file-keeper to handle validation and default values.</p> <p>Example</p> <pre><code>from dataclasses import dataclass\nimport file_keeper as fk\n\n@dataclass\nclass MyStorageSettings(fk.Settings):\n    api_key: str = \"\"\n    endpoint: str = \"\"\n</code></pre>"},{"location":"extending/adapters/#extend-the-storage-class","title":"Extend the Storage class","text":"<p>Create a class that inherits from Storage and sets the SettingsFactory class attribute to your settings class. It also sets UploaderFactory and ReaderFactory in the same way. The implementation will follow soon.</p> <p>Example</p> <pre><code>...\n\nclass MyStorage(fk.Storage):\n    settings: MyStorageSettings\n\n    SettingsFactory = MyStorageSettings\n    UploaderFactory = MyUploader\n    ReaderFactory = MyReader\n</code></pre>"},{"location":"extending/adapters/#implement-uploader-and-reader-services","title":"Implement Uploader and Reader services","text":"<p>Create classes for UploaderFactory and ReaderFactory that inherit from Uploader and Reader respectively. These classes will contain the logic for uploading and reading files to and from your storage system.</p> <p>Make sure to add CREATE capability to <code>Uploader</code> and STREAM capability to <code>Reader</code>. Otherwise storage will pretend that these services do not support these operations</p> <p>Example</p> <pre><code>class MyUploader(fk.Uploader):\n\n    capabilities = fk.Capability.CREATE\n\n    def upload(self, location: fk.Location, upload: fk.Upload, extras: dict[str, Any]) -&gt; fk.FileData:\n        # Implement your upload logic here\n        reader = upload.hashing_reader()\n        for chunk in reader:\n            # send fragment to storage\n\n        return fk.FileData(location, upload.size, upload.content_type, reader.get_hash())\n\n\nclass MyReader(fk.Reader):\n\n    capabilities = fk.Capability.STREAM\n\n    def stream(self, data: fk.FileData, extras: dict[str, Any]) -&gt; Iterable[bytes]:\n        # Implement your streaming logic here\n        for chunk in file_stream:\n            yield chunk\n</code></pre>"},{"location":"extending/adapters/#register-the-adapter","title":"Register the adapter","text":"<p>If you are going to use custom adapter only inside a single script, you can register it directly using <code>adapters</code> registry:</p> <pre><code>fk.adapters.register(\"local\", MyStorage)\n</code></pre> <p>If you are writing a library that will be used across multiple project it's better to register storage using entrypoints of the python package.</p> <p>Use the <code>register_adapters</code> hook to register your adapter. This makes it available as a <code>type</code> inside make_storage.</p> <pre><code>@fk.hookimpl\ndef register_adapters(registry: fk.Registry[type[fk.Storage]]):\n    registry.register(\"local\", MyStorage)\n</code></pre>"},{"location":"extending/adapters/#initialize-the-adapter","title":"Initialize the adapter","text":"<p>Now you can use your custom adapter:</p> <pre><code>storage = fk.make_storage(\"local\", {\n    \"adapter\": \"local\",\n    \"api_key\": \"123\",\n    \"endpoint\": \"https://example.local\",\n})\n</code></pre> <p>This is a basic example, but it demonstrates the fundamental principles of creating a custom storage adapter.  You can extend this example to support more complex features and integrate with a wider range of storage systems.</p>"},{"location":"extending/dissection/","title":"Dissecting storage adapter","text":"<p>This guide walks you through implementation of filesystem adapter. We'll look at every part of it from the outer layer and going inwards, analyzing the meaning of the units and reasons they exist.</p>"},{"location":"extending/dissection/#register-an-adapter","title":"Register an adapter","text":"<p>Adapters must be registered to make them available via make_storage.</p> <p>Define an entry-point with the name <code>file_keeper_ext</code> in the distribution. This entry-point specifies python module that contains implementation of the pluggy hooks that extends file-keeper.</p> <pre><code>[project.entry-points.file_keeper_ext]\nfile_keeper = \"file_keeper.default\"\n</code></pre> <p>file-keeper expects to find a function decorated with <code>@file_keeper.hookimpl</code> and named <code>register_adapters</code> inside the module. This function registers all custom adapters of the module via call to <code>register()</code> method of the Registry with adapters. This call accepts the name of the adapter as a first argument, and the class of the adapter as a second argument.</p> <pre><code>@ext.hookimpl\ndef register_adapters(registry: Registry[type[Storage]]):  # noqa: C901\n    \"\"\"Built-in storage adapters.\"\"\"\n    registry.register(\"file_keeper:fs\", adapters.FsStorage)\n</code></pre> <p>Adapter names have no restrictions regarding length or allowed symbols, but it's recommended to use <code>&lt;package&gt;:&lt;type&gt;</code> structure, like in <code>file_keeper:fs</code>. If you provide multiple similar adapters, consider adding third segment, i.e. <code>file_keeper:fs:v1</code>, <code>file_keeper:fs:v2</code>.</p>"},{"location":"extending/dissection/#create-an-adapter","title":"Create an adapter","text":"<p>The adapter itself is simple and usually contains just few lines of code.</p> <p>It must extend Storage and, optionally, it can override SettingsFactory, UploaderFactory, ManagerFactory, and ReaderFactory.</p> <pre><code>class FsStorage(fk.Storage):\n    \"\"\"Filesystem storage adapter.\n\n    Stores files on the local filesystem. The `path` setting must be\n    configured to point to the base directory where files are stored.\n\n    Example configuration:\n\n    ```py\n    import file_keeper as fk\n\n    settings = {\n        \"type\": \"file_keeper:fs\",\n        \"path\": \"/path/to/storage\",\n        \"initialize\": True,\n        \"override_existing\": False,\n    }\n    storage = fk.make_storage(\"fs\", settings)\n    ```\n\n    Note:\n    * The `path` must be an absolute path.\n    * The `path` directory must be writable by the application.\n    * The `location` used in file operations is relative to the `path`.\n    * If `Storage.path` is not empty, the `location` is validated\n      to prevent directory traversal.\n    * Consider using combination of `storage.prepare_location` with\n      `settings.location_transformers` that further sanitizes the path, like\n      `safe_relative_path`.\n    * If `initialize` is `True`, the storage will attempt to create the\n      directory if it does not exist.\n    * If `override_existing` is `False`, operations that would overwrite an\n      existing file will raise an `ExistingFileError`.\n    \"\"\"\n\n    settings: Settings\n\n    SettingsFactory = Settings\n    UploaderFactory = Uploader\n    ReaderFactory = Reader\n    ManagerFactory = Manager\n</code></pre> <p>Filesystem adapter overrides all these attributes because:</p> <ul> <li>it contains custom settings(<code>SettingsFactory</code>)</li> <li>it defines how the file is uploaded (<code>UploaderFactory</code>)</li> <li>it defines how the file is managed, i.e. removed, copied, analyzed (<code>ManagerFactory</code>)</li> <li>it defines how file is read (<code>ReaderFactory</code>)</li> </ul> <p>Additionally it specifies type of <code>settings</code> attribute as <code>settings: Settings</code>, i.e. custom <code>Settings</code> class that is defined in the same module. This is done to simplify typing and does not affect the behavior of the adapter. Without this line typechecker assumes that storage uses base Settings and complains when custom options are accessed.</p>"},{"location":"extending/dissection/#define-storage-settings","title":"Define storage settings","text":"<p>Create a dataclass <code>Settings</code> to hold configuration options specific to your storage. This class should inherit from Settings.</p> <pre><code>@dataclasses.dataclass()\nclass Settings(fk.Settings):\n    \"\"\"Settings for FS storage.\"\"\"\n</code></pre> <p>Filesystem settings do not introduce new options, so there are no attributes here. But some other provider would do the following:</p> <pre><code>@dataclasses.dataclass()\nclass Settings(fk.Settings):\n    bucket: str = \"\"\n    username: str = \"\"\n    password: str = \"\"\n    params: dict[str, Any] = dataclasses.field(default_factory=dict)\n</code></pre> <p>These options must include default values due to dataclass restrictions, even if storage will not work with these defaults. E.g., empty password and username won't work usually, but you still have to specify them.</p> <p>And now happens validation. It's provided by the <code>__post_init__</code> method of the dataclass.</p> <pre><code>    def __post_init__(self, **kwargs: Any):\n        super().__post_init__(**kwargs)\n\n        if not os.path.exists(self.path):\n            if not self.initialize:\n                raise fk.exc.InvalidStorageConfigurationError(\n                    self.name,\n                    f\"path `{self.path}` does not exist\",\n                )\n\n            try:\n                os.makedirs(self.path)\n            except PermissionError as err:\n                raise fk.exc.InvalidStorageConfigurationError(\n                    self.name,\n                    f\"path `{self.path}` is not writable\",\n                ) from err\n</code></pre> <p>Filesystem adapter relies on two generic options. First is <code>initialize</code>. When this flag is enabled, storage checks whether directory for files exists and, if it's missing, tries to create this directory. If task fails, InvalidStorageConfigurationError is raised. That's how storage reacts on problems with configuration.</p> <p>Second option is <code>path</code>. <code>path</code>, usually absolute, defines the location in filesystem where files are stored. Other storages may treat it differently: cloud storages use <code>path</code> as a prefix of the file name, because cloud storages do not support directory hierarchy; SQLAlchemy storage ignores path as it has no meaning in DB context.</p> <p>Apart from this, storages often initialize and store connections to external services as storage attributes. For example, <code>file_keeper:azure_blob</code> has <code>container_name</code> string options that holds the name of cloud container where files are stored. Inside <code>__post_init__</code> it connects to the container and stores the container object itself as <code>container</code> property, so that storage doesn't need to constantly re-connect to the containers.</p>"},{"location":"extending/dissection/#create-the-uploader-service","title":"Create the uploader service","text":"<p>The next target is <code>Uploader</code>. It's a service reesponsible for file creation that must extend Uploader.</p> <pre><code>class Uploader(fk.Uploader):\n    \"\"\"Filesystem uploader.\"\"\"\n</code></pre> <p>Any service consists of two parts - methods and capabilities. Methods describe the logic but are hidden from storage initially. I.e., if you only define methods and ignore capabilities, storage will pretend that service cannot perform the task.</p> <pre><code>    @override\n    def upload(self, location: fk.types.Location, upload: fk.Upload, extras: dict[str, Any]) -&gt; fk.FileData:\n        ...\n</code></pre> <p>And capabilities actually tell storage about operations supported by storage. Because of this separation, you can pretend that storage cannot perform an operation, even when it's supported. In this way you can transform filesystem into a read-only storage without code changes and guarantee that files won't be accidentally removed from it.</p> <pre><code>    capabilities: fk.Capability = fk.Capability.CREATE | fk.Capability.RESUMABLE\n</code></pre> <p>As you can see, FS storage supports CREATE and RESUMABLE. Capabilities are implemented as bit masks and can be combined using <code>|</code> operator.</p> <p>Let's look closer at the <code>upload()</code> method.</p> <p>It computes full path to the file location in the beginning. <code>full_path()</code> is a generic method of the storage that naively combine <code>path</code> option of the storage with <code>location</code> of the file. Every storage has <code>path</code> option and every storage can use <code>full_path()</code> to attach <code>path</code> as a prefix to the location, if it makes any sense.</p> <pre><code>        dest = self.storage.full_path(location)\n</code></pre> <p>Now storage uses another generic option, <code>override_existing</code>. If it's disabled and given location already taken by another file, uploader raises ExistingFileError. That's recommended reaction in such situation and you'll notice that other storages also follow this process.</p> <pre><code>        if not self.storage.settings.override_existing and os.path.exists(dest):\n            raise fk.exc.ExistingFileError(self.storage, location)\n</code></pre> <p>Then storage ensures that all intermediate folders from the final file's location are present. If you expect that files are loaded directly into <code>path</code>, it may seem redundant. But FS storage does not imply such restrictions and there may be nested directories under the <code>path</code>, so verifying that all folders are created is a safest option.</p> <pre><code>        os.makedirs(os.path.dirname(dest), exist_ok=True)\n</code></pre> <p>Then file is actually written to the FS. You can read file content using <code>upload.stream.read()</code>, but here we create HashingReader using <code>hashing_reader()</code> method of the Upload. This object also has <code>read()</code> method, but in addition it computes the content hash of the file while it's consumed. As result we have the hash in the end almost for free.</p> <pre><code>        reader = upload.hashing_reader()\n        with open(dest, \"wb\") as fd:\n            for chunk in reader:\n                fd.write(chunk)\n</code></pre> <p>Other storages, like AWS S3 or Azure Blob Storage, do not need this step, because content hash is computed by cloud provided and returned with the metadata of the uploaded object. But if you don't have a cheap way to obtain the hash, using HashingReader is the recommended option.</p> <p>In the end, <code>upload()</code> method of the service builds FileData with details of the uploaded file and returns it to the caller.</p> <pre><code>        return fk.FileData(\n            location,\n            os.path.getsize(dest),\n            upload.content_type,\n            reader.get_hash(),\n        )\n</code></pre>"},{"location":"extending/dissection/#create-the-reader-service","title":"Create the reader service","text":"<p><code>Reader</code> service is much simpler than <code>Uploader</code>. It exposes STREAM capability to notify the storage, that files can be read from the storage. And it implements <code>stream()</code> method, that explains how exactly bytes of content are obtained.</p> <pre><code>class Reader(fk.Reader):\n    \"\"\"Filesystem reader.\"\"\"\n\n    storage: FsStorage\n    capabilities: fk.Capability = fk.Capability.STREAM\n\n    @override\n    def stream(self, data: fk.FileData, extras: dict[str, Any]) -&gt; IO[bytes]:\n        filepath = self.storage.full_path(data.location)\n        if not os.path.exists(filepath):\n            raise fk.exc.MissingFileError(self.storage, data.location)\n\n        return open(filepath, \"rb\")  # noqa: SIM115\n</code></pre> <p>Note how it computes the path to the file using <code>full_path()</code>, just as <code>Uploader</code> did. Basically, every method that access the file should use <code>full_path()</code>.</p> <p>Also, pay attention to MissingFileError raised if file does not exist. That's the recommended way to handle missing files.</p> <p>Finally, look at return result. The <code>stream()</code> method must return <code>Iterable[bytes]</code>, but not just bytes, e.g. <code>return b\"hello\"</code> is not valid output.</p> <p>Anything that can be used in a for-loop and produce <code>bytes</code> is a valid output of the <code>steam()</code> method. Few examples:</p> List of byte stringsGenerator of bytesio.BytesIOFile object opened in <code>rb</code> mode <pre><code>...\nreturn [b\"hello\", b\" \", b\"world\"]\n</code></pre> <pre><code>...\nyield b\"hello\"\nyield b\" \"\nyield b\"world\"\n</code></pre> <pre><code>...\nreturn BytesIO(b\"hello world\")\n</code></pre> <pre><code>...\nreturn open(path, \"rb\")\n</code></pre>"},{"location":"extending/dissection/#create-the-manager-service","title":"Create the manager service","text":"<p><code>Manager</code> service contains a lot of methods and capabilities, but all of them are pretty straightforward.</p> <p>Capabilities tell the storage \"what\" service can do, while method implementations explain \"how\" it's done. Usually, capability and method come in pair, unless you are certain that you need to separate them.</p> <pre><code>    capabilities: fk.Capability = (\n        fk.Capability.REMOVE\n        | fk.Capability.SCAN\n        | fk.Capability.EXISTS\n        | fk.Capability.ANALYZE\n        | fk.Capability.COPY\n        | fk.Capability.MOVE\n        | fk.Capability.COMPOSE\n        | fk.Capability.APPEND\n    )\n</code></pre> <p><code>remove()</code> method removes the object. If it's removed, the result is <code>True</code>. If it's not removed(because it does not exists), the result is <code>False</code>.</p> <pre><code>    @override\n    def remove(self, data: fk.FileData, extras: dict[str, Any]) -&gt; bool:\n        filepath = self.storage.full_path(data.location)\n        if not os.path.exists(filepath):\n            return False\n\n        os.remove(filepath)\n        return True\n</code></pre> <p><code>scan()</code> returns an iterable of strings with names of all files available in the storage. Note, this method yields filename relative to the value of <code>path</code> option of the storage. In this way, when you iterate through <code>scan()</code> results and pass filenames back to storage, it can wrap filenames into <code>storage.full_path()</code> and build correct locations.</p> <p>If <code>os.path.relname()</code> is not called, when you use items from the <code>ckan()</code>, <code>path</code> would be included twice, producing incorrect locations.</p> <pre><code>    @override\n    def scan(self, extras: dict[str, Any]) -&gt; Iterable[str]:\n        path = self.storage.settings.path\n        search_path = os.path.join(path, \"**\")\n\n        for entry in glob.glob(search_path, recursive=True):\n            if not os.path.isfile(entry):\n                continue\n            yield os.path.relpath(entry, path)\n</code></pre> <p><code>exists()</code> returns <code>True</code> if file exists, and <code>False</code> if file is missing.</p> <pre><code>    @override\n    def exists(self, data: fk.FileData, extras: dict[str, Any]) -&gt; bool:\n        filepath = self.storage.full_path(data.location)\n        return os.path.exists(filepath)\n</code></pre> <p><code>analyze()</code> returns the same FileData as one, produced during <code>upload()</code>.</p> <pre><code>    @override\n    def analyze(self, location: fk.types.Location, extras: dict[str, Any]) -&gt; fk.FileData:\n        return fk.FileData(\n            location,\n            size=self.size(location, extras),\n            content_type=self.content_type(location, extras),\n            hash=self.hash(location, extras),\n        )\n</code></pre> <p><code>copy()</code> creates a copy of the file, raising MissingFileError if source file is missing and ExistingFileError if destination file already exist and <code>override_existing</code> is disabled.</p> <pre><code>    @override\n    def copy(self, location: fk.types.Location, data: fk.FileData, extras: dict[str, Any]) -&gt; fk.FileData:\n        src = self.storage.full_path(data.location)\n        dest = self.storage.full_path(location)\n\n        if not os.path.exists(src):\n            raise fk.exc.MissingFileError(self.storage, data.location)\n\n        if os.path.exists(dest) and not self.storage.settings.override_existing:\n            raise fk.exc.ExistingFileError(self.storage, location)\n\n        os.makedirs(os.path.dirname(dest), exist_ok=True)\n        shutil.copy(src, dest)\n        return fk.FileData.from_object(data, location=location)\n</code></pre> <p><code>move()</code> behaves exactly like <code>copy()</code>. In addition, the original file is not available after the move. Basically, <code>move()</code> does \"rename\" if possible, and \"copy\" + \"remove\" if not.</p> <pre><code>    @override\n    def move(self, location: fk.types.Location, data: fk.FileData, extras: dict[str, Any]) -&gt; fk.FileData:\n        src = self.storage.full_path(data.location)\n        dest = self.storage.full_path(location)\n\n        if not os.path.exists(src):\n            raise fk.exc.MissingFileError(self.storage, data.location)\n\n        if os.path.exists(dest):\n            if self.storage.settings.override_existing:\n                os.remove(dest)\n            else:\n                raise fk.exc.ExistingFileError(self.storage, location)\n\n        os.makedirs(os.path.dirname(dest), exist_ok=True)\n        shutil.move(src, dest)\n        return fk.FileData.from_object(data, location=location)\n</code></pre> <p><code>compose()</code> is the most challenging method of the <code>Manager</code>. It takes few existing files and combines them into a new one, similar to the <code>cat</code> utility.</p> <pre><code>    @override\n    def compose(self, location: fk.types.Location, datas: Iterable[fk.FileData], extras: dict[str, Any]) -&gt; fk.FileData:\n        dest = self.storage.full_path(location)\n\n        if os.path.exists(dest) and not self.storage.settings.override_existing:\n            raise fk.exc.ExistingFileError(self.storage, location)\n\n        sources: list[str] = []\n        for data in datas:\n            src = self.storage.full_path(data.location)\n\n            if not os.path.exists(src):\n                raise fk.exc.MissingFileError(self.storage, data.location)\n\n            sources.append(src)\n\n        with open(dest, \"wb\") as to_fd:\n            for src in sources:\n                with open(src, \"rb\") as from_fd:\n                    shutil.copyfileobj(from_fd, to_fd)\n\n        return self.analyze(location, extras)\n</code></pre> <p><code>append()</code> takes content of the existing file and adds it in the end of another file.</p> <pre><code>    @override\n    def append(self, data: fk.FileData, upload: fk.Upload, extras: dict[str, Any]) -&gt; fk.FileData:\n        dest = self.storage.full_path(data.location)\n        if not os.path.exists(dest):\n            raise fk.exc.MissingFileError(self.storage, data.location)\n\n        with open(dest, \"ab\") as fd:\n            fd.write(upload.stream.read())\n\n        return self.analyze(data.location, extras)\n</code></pre>"},{"location":"extending/location_transformers/","title":"Location transformers","text":"<p>Location transformers allow you to modify the Location string before it's used to access a file in the storage backend. This is useful for scenarios where you need to perform additional processing or formatting on the location, such as adding prefixes, encoding characters, or generating unique identifiers.</p>"},{"location":"extending/location_transformers/#what-are-location-transformers","title":"What are location transformers?","text":"<p>A location transformer is a callable (usually a function) that takes the original Location string, optional Upload, and any extra data as input, and returns a modified Location string. They provide a flexible way to customize how locations are handled by file-keeper.</p> <p>Location transformers are set per-storage via location_transformers option. To apply them call prepare_location() method.</p> <p>Example</p> <pre><code>storage = make_storage(\"test\", {\n    \"type\": \"file_keeper:fs\",\n    \"location_transformers\": [\"safe_relative_path\"],\n    \"path\": \"/tmp\",\n})\n\nunsafe_location = \"../etc/passwd\"\n\nsafe_location = storage.prepare_location(unsafe_location)\n\nstorage.upload(safe_location, ...)\n</code></pre>"},{"location":"extending/location_transformers/#steps-to-create-a-custom-location-transformer","title":"Steps to create a custom location transformer","text":""},{"location":"extending/location_transformers/#define-your-transformer","title":"Define your transformer","text":"<p>Create a function that accepts the Location, optional Upload, and <code>extras</code> as input and returns the transformed Location.</p> <pre><code>def my_location_transformer(location, upload_or_none, extras):\n    # Perform custom transformation here\n    return \"prefix_\" + location\n</code></pre>"},{"location":"extending/location_transformers/#register-the-transformer","title":"Register the transformer","text":"<p>Use the <code>register_location_transformers</code> hook to register your transformer. This makes it available for use when creating or accessing files.</p> <pre><code>import file_keeper as fk\n\n@fk.hookimpl\ndef register_location_transformers(registry):\n    registry.register(\"my_transformer\", my_location_transformer)\n</code></pre>"},{"location":"extending/location_transformers/#using-your-custom-transformer","title":"Using Your Custom Transformer","text":"<p>To use your custom transformer, specify its name when creating a <code>Storage</code> object in the settings.</p> <pre><code>storage = make_storage(\"my_storage\", {\n    \"adapter\": \"s3\",\n    \"location_transformers\": [\"my_transformer\"],\n})\n</code></pre> <p>And apply <code>Storage.prepare_location</code> to original location:</p> <pre><code>transformed = storage.prepare_location(\"hello.txt\")\n\nassert transformed == \"prefix_hello.txt\"\n</code></pre>"},{"location":"extending/overview/","title":"Register file-keeper extension","text":"<p>file-keeper is designed to be highly extensible, allowing you to add new storage backends, upload factories, and location transformers without modifying the core library. This is achieved through the use of the Pluggy framework.</p>"},{"location":"extending/overview/#overview","title":"Overview","text":"<p>If you want to make your module that extends of file-keeper externally available, register it as a <code>file_keeper_ext</code> entry-point of your distribution.</p> pyproject.tomlsetup.py <pre><code>...\n\n[project.entry-points.file_keeper_ext]\nmy_storage_extension = \"my_storage.my_module\"\n</code></pre> <pre><code>from setuptools import setup\n\nsetup(\n    ...,\n    entry_points={\n        \"file_keeper_ext\": [\"my_storage_extension = my_storage.my_module\"],\n    },\n)\n</code></pre> <p>file-keeper iterates through all modules that are registered under <code>file_keeper_ext</code> entry-point and extract pluggy hook implementations from them. In this way, anyone who've installed your library will have access to your customizations.</p>"},{"location":"extending/overview/#available-extension-points","title":"Available extension points","text":"<p>The module that contains file-keeper's extension has to define functions that register new functionality. These functions must have the same name as one of file-keeper's hooks and be decorated with <code>@file_keeper.hookimpl</code> decorator.</p> <p>The following hooks are currently available:</p> Hook Description <code>register_adapters</code> Use this function to register new storage adapters. The <code>registry</code> object allows you to add your <code>Storage</code> class to the list of available storage options. <code>register_location_transformers</code> Use this function to register new location transformers. The <code>registry</code> object allows you to add your <code>LocationTransformer</code> function to the list of available location transformers. <p>Example</p> <p>Register new storage adapter:</p> <pre><code>@fk.hookimpl\ndef register_adapters(registry):\n    registry.register(\"my_custom_adapter\", MyStorageClass)\n</code></pre> <p>Register new location transformers:</p> <pre><code>@fk.hookimpl\ndef register_location_transformers(registry):\n    registry.register(\"my_custom_transformer\", transformer_func)\n</code></pre>"},{"location":"extending/overview/#example-registering-a-custom-storage-adapter","title":"Example: registering a custom storage adapter","text":"<p>Let's say you want to add a new storage adapter that stores files in a local directory.  Here's how you would do it:</p> <ol> <li> <p>Create a new Python package (<code>my_storage_extension</code>)</p> </li> <li> <p>Inside your package, create a module ( <code>my_storage.py</code>) with the following content:</p> <pre><code>import file_keeper as fk\n\nclass MyLocalStorage(fk.Storage):\n    ...\n\n@fk.hookimpl\ndef register_adapters(registry):\n    registry.register(\"my_local\", MyLocalStorage)\n</code></pre> </li> <li> <p>Add an entry point to your <code>setup.py</code> or <code>pyproject.toml</code> file:</p> setup.pypyproject.toml <pre><code>from setuptools import setup\n\nsetup(\n    ...,\n    entry_points={\n        \"file_keeper_ext\": [\"my_storage_extension = my_storage\"],\n    },\n)\n</code></pre> <pre><code>...\n\n[project.entry-points.file_keeper_ext]\nmy_storage_extension = \"my_storage\"\n</code></pre> </li> </ol> <p>Now, when file-keeper discovers your extension, it will register <code>MyLocalStorage</code> as a new storage option, accessible by the name \"my_local\".</p>"},{"location":"extending/overview/#example-registering-a-custom-location-transformer","title":"Example: registering a custom location transformer","text":"<p>Let's say you want to add a location transformer that prepends a prefix to all locations.</p> <ol> <li> <p>Create a new Python package</p> </li> <li> <p>Inside your package, create a file  with the following content:</p> <pre><code>import file_keeper as fk\n\ndef my_location_transformer(location: str, upload, extras: dict[str, any]) -&gt; str:\n    return f\"prefix_{location}\"\n\n@fk.hookimpl\ndef register_location_transformers(registry: Registry[types.LocationTransformer]):\n    registry.register(\"my_prefix\", my_location_transformer)\n</code></pre> </li> <li> <p>Add an entry point to your <code>setup.py</code> or <code>pyproject.toml</code> file</p> </li> </ol> <p>Now, when file-keeper discovers your extension, it will register a new location transformer, accessible by the name \"my_prefix\".</p>"},{"location":"extending/overview/#manual-extension","title":"Manual extension","text":"<p>Entry-points are good for python packages, but sometimes you need a custom storage just for the current script and don't want to mess with packages. In this case you can register adapter or location transformer manually, using <code>file_keeper.core.storage.adapters</code> and <code>file_keeper.core.storage.location_transformers</code> Registries.</p> <p>Example</p> <p>Let's say you have <code>MyStorage</code> adapter and <code>my_transformer</code> transformer. In the following snippet, they will be available after the <code>register()</code> calls in the corresponding registries.</p> <pre><code>from file_keeper.core.storage import location_transformers, adapters\n\n# not available\n\nadapters.register(\"my_storage\", MyStorage)\nlocation_transformers.register(\"my_transformer\", my_transformer)\n\n# available\n\nstorage = make_storage(\"test\", {\n    \"type\": \"my_storage\",\n    \"location_transformers\": [\"my_transformer\"]\n})\n</code></pre>"},{"location":"tutorials/advanced_patterns/","title":"Advanced patterns","text":"<p>This tutorial covers advanced patterns and techniques for using file-keeper effectively in production applications. We'll explore performance optimization, error handling strategies, and integration with web frameworks.</p>"},{"location":"tutorials/advanced_patterns/#performance-optimization","title":"Performance Optimization","text":""},{"location":"tutorials/advanced_patterns/#efficient-large-file-handling","title":"Efficient Large File Handling","text":"<p>For large files, use streaming to avoid loading everything into memory:</p> <pre><code>import file_keeper as fk\nfrom file_keeper import IterableBytesReader\n\ndef upload_large_file_from_chunks(storage, location, chunk_generator):\n    \"\"\"Upload a large file from a generator of chunks.\"\"\"\n    # Create an upload from an iterable of bytes\n    stream = IterableBytesReader(chunk_generator)\n    upload = fk.Upload(stream, location, estimate_size(chunk_generator), \"application/octet-stream\")\n\n    return storage.upload(location, upload)\n\ndef estimate_size(chunk_generator):\n    \"\"\"Estimate total size from chunks.\"\"\"\n    total = 0\n    for chunk in chunk_generator:\n        total += len(chunk)\n    return total\n\n# Example usage\ndef file_chunk_generator(filepath, chunk_size=8192):\n    \"\"\"Generate chunks from a file.\"\"\"\n    with open(filepath, 'rb') as f:\n        while True:\n            chunk = f.read(chunk_size)\n            if not chunk:\n                break\n            yield chunk\n\n# Upload a large file efficiently\n# file_info = upload_large_file_from_chunks(storage, \"large_file.dat\", file_chunk_generator(\"/path/to/large/file\"))\n</code></pre> <p>Note, the code above reads data from the existing file and can be written simply as: <pre><code>with open(\"/path/to/large/file\", \"rb\") as src:\n    upload = fk.make_upload(src)\n    return storage.upload(location, upload)\n</code></pre></p> <p>But when the content does not exist and needs to be generated in real-time, <code>make_upload</code> cannot produce the required upload object. That's when <code>IterableBytesReader</code> comes in handy.</p>"},{"location":"tutorials/advanced_patterns/#batch-operations","title":"Batch Operations","text":"<p>When working with multiple files, consider batch operations where possible:</p> <pre><code>def batch_upload(storage, file_dict):\n    \"\"\"Upload multiple files efficiently.\"\"\"\n    results = {}\n\n    for filename, content in file_dict.items():\n        try:\n            upload = fk.make_upload(content)\n            file_info = storage.upload(filename, upload)\n            results[filename] = file_info\n        except fk.exc.FilesError as e:\n            print(f\"Failed to upload {filename}: {e}\")\n            results[filename] = None\n\n    return results\n\n# Example usage\nfiles_to_upload = {\n    \"file1.txt\": b\"Content of file 1\",\n    \"file2.txt\": b\"Content of file 2\",\n    \"file3.txt\": b\"Content of file 3\"\n}\n\nresults = batch_upload(storage, files_to_upload)\n</code></pre>"},{"location":"tutorials/advanced_patterns/#error-handling-strategies","title":"Error Handling Strategies","text":""},{"location":"tutorials/advanced_patterns/#retry-mechanisms","title":"Retry Mechanisms","text":"<p>Implement robust retry logic for transient failures:</p> <pre><code>import time\nimport random\nfrom file_keeper import exc\n\ndef retry_on_failure(func, max_retries=3, base_delay=1, backoff_factor=2, jitter=True):\n    \"\"\"Execute a function with exponential backoff retry.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            return func()\n        except (exc.ConnectionError, exc.UploadError) as e:\n            if attempt == max_retries - 1:  # Last attempt\n                raise e\n\n            delay = base_delay * (backoff_factor ** attempt)\n            if jitter:\n                delay += random.uniform(0, 0.1 * delay)  # Add jitter\n\n            print(f\"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.2f}s...\")\n            time.sleep(delay)\n\n# Example usage\ndef upload_with_retry(storage, location, upload):\n    def attempt():\n        return storage.upload(location, upload)\n\n    return retry_on_failure(attempt)\n</code></pre>"},{"location":"tutorials/advanced_patterns/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<p>For external services, implement circuit breaker pattern:</p> <pre><code>import time\nfrom enum import Enum\n\nclass CircuitState(Enum):\n    CLOSED = 1\n    OPEN = 2\n    HALF_OPEN = 3\n\nclass StorageCircuitBreaker:\n    def __init__(self, failure_threshold=5, timeout=60):\n        self.failure_threshold = failure_threshold\n        self.timeout = timeout\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = CircuitState.CLOSED\n\n    def call(self, storage_func, *args, **kwargs):\n        if self.state == CircuitState.OPEN:\n            if time.time() - self.last_failure_time &gt; self.timeout:\n                self.state = CircuitState.HALF_OPEN\n            else:\n                raise exc.FilesError(\"Circuit breaker is OPEN\")\n\n        try:\n            result = storage_func(*args, **kwargs)\n            if self.state == CircuitState.HALF_OPEN:\n                self.state = CircuitState.CLOSED\n                self.failure_count = 0\n            return result\n        except exc.FilesError:\n            self.failure_count += 1\n            self.last_failure_time = time.time()\n\n            if self.failure_count &gt;= self.failure_threshold:\n                self.state = CircuitState.OPEN\n\n            raise\n\n# Example usage\nbreaker = StorageCircuitBreaker()\n\ntry:\n    file_info = breaker.call(storage.upload, \"test.txt\", upload)\nexcept exc.FilesError as e:\n    print(f\"Operation failed: {e}\")\n</code></pre>"},{"location":"tutorials/advanced_patterns/#framework-integration","title":"Framework Integration","text":""},{"location":"tutorials/advanced_patterns/#flask-integration","title":"Flask Integration","text":"<p>Integrate file-keeper with Flask for web file uploads:</p> <pre><code>from flask import Flask, request, jsonify\nimport file_keeper as fk\nfrom werkzeug.utils import secure_filename\n\napp = Flask(__name__)\n\n# Initialize storage\nstorage = fk.make_storage(\"flask_storage\", {\n    \"type\": \"file_keeper:fs\",\n    \"path\": \"./uploads\",\n    \"initialize\": True\n})\n\n@app.route('/upload', methods=['POST'])\ndef upload_file():\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    try:\n        # Create upload from Werkzeug FileStorage\n        upload = fk.make_upload(file)\n\n        # Sanitize filename\n        filename = secure_filename(file.filename)\n\n        # Upload the file\n        file_info = storage.upload(filename, upload)\n\n        return jsonify({\n            'success': True,\n            'location': file_info.location,\n            'size': file_info.size,\n            'hash': file_info.hash\n        })\n    except fk.exc.FilesError as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/download/&lt;filename&gt;')\ndef download_file(filename):\n    try:\n        # Find file by scanning (in a real app, you'd store file_info elsewhere)\n        for loc in storage.scan():\n            if loc == filename:\n                file_info = storage.analyze(loc)\n                content = storage.content(file_info)\n                return content, 200, {'Content-Type': file_info.content_type}\n\n        return jsonify({'error': 'File not found'}), 404\n    except fk.exc.FilesError as e:\n        return jsonify({'error': str(e)}), 500\n</code></pre>"},{"location":"tutorials/advanced_patterns/#django-integration","title":"Django Integration","text":"<p>For Django applications:</p> <pre><code># views.py\nfrom django.http import HttpResponse, JsonResponse\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.views.decorators.http import require_http_methods\nimport json\nimport file_keeper as fk\nfrom file_keeper import exc\n\n# Initialize storage\nstorage = fk.make_storage(\"django_storage\", {\n    \"type\": \"file_keeper:fs\",\n    \"path\": \"./media\",\n    \"initialize\": True\n})\n\n@csrf_exempt\n@require_http_methods([\"POST\"])\ndef upload_view(request):\n    if 'file' not in request.FILES:\n        return JsonResponse({'error': 'No file provided'}, status=400)\n\n    uploaded_file = request.FILES['file']\n\n    try:\n        # Create upload from Django UploadedFile\n        upload = fk.make_upload(uploaded_file)\n\n        # Upload the file\n        file_info = storage.upload(uploaded_file.name, upload)\n\n        return JsonResponse({\n            'success': True,\n            'location': file_info.location,\n            'size': file_info.size\n        })\n    except exc.FilesError as e:\n        return JsonResponse({'error': str(e)}, status=500)\n\ndef download_view(request, filename):\n    try:\n        # Find and serve the file\n        for loc in storage.scan():\n            if loc == filename:\n                file_info = storage.analyze(loc)\n                content = storage.content(file_info)\n\n                response = HttpResponse(content, content_type=file_info.content_type)\n                response['Content-Disposition'] = f'attachment; filename=\"{filename}\"'\n                return response\n\n        return JsonResponse({'error': 'File not found'}, status=404)\n    except exc.FilesError as e:\n        return JsonResponse({'error': str(e)}, status=500)\n</code></pre>"},{"location":"tutorials/advanced_patterns/#security-considerations","title":"Security Considerations","text":""},{"location":"tutorials/advanced_patterns/#input-validation","title":"Input Validation","text":"<p>Always validate file inputs to prevent security vulnerabilities:</p> <pre><code>import mimetypes\nimport os\nfrom file_keeper import exc\n\ndef is_safe_file_type(filename, allowed_types):\n    \"\"\"Check if file type is allowed.\"\"\"\n    mime_type, _ = mimetypes.guess_type(filename)\n    if mime_type:\n        return mime_type in allowed_types\n    return False\n\ndef is_safe_file_size(upload, max_size_mb=10):\n    \"\"\"Check if file size is within limits.\"\"\"\n    return upload.size &lt;= (max_size_mb * 1024 * 1024)\n\ndef safe_upload(storage, filename, upload, allowed_types, max_size_mb=10):\n    \"\"\"Safely upload a file with validation.\"\"\"\n    # Validate file type\n    if not is_safe_file_type(filename, allowed_types):\n        raise exc.FilesError(f\"File type not allowed: {filename}\")\n\n    # Validate file size\n    if not is_safe_file_size(upload, max_size_mb):\n        raise exc.FilesError(f\"File too large: {filename}\")\n\n    # Sanitize filename to prevent directory traversal\n    safe_filename = os.path.basename(filename)\n\n    return storage.upload(safe_filename, upload)\n\n# Example usage\nALLOWED_TYPES = [\n    'image/jpeg',\n    'image/png',\n    'image/gif',\n    'text/plain',\n    'application/pdf'\n]\n\ntry:\n    file_info = safe_upload(\n        storage,\n        \"../../../etc/passwd\",  # This would be sanitized\n        upload,\n        ALLOWED_TYPES,\n        max_size_mb=5\n    )\nexcept exc.FilesError as e:\n    print(f\"Upload rejected: {e}\")\n</code></pre>"},{"location":"tutorials/advanced_patterns/#monitoring-and-logging","title":"Monitoring and Logging","text":"<p>Implement proper monitoring for file operations:</p> <pre><code>import logging\nimport time\nfrom file_keeper import exc\n\nlogger = logging.getLogger(__name__)\n\nclass MonitoredStorage:\n    def __init__(self, storage):\n        self.storage = storage\n\n    def upload(self, location, upload):\n        start_time = time.time()\n        try:\n            result = self.storage.upload(location, upload)\n            duration = time.time() - start_time\n            logger.info(f\"Upload successful: {location}, size: {upload.size}, duration: {duration:.2f}s\")\n            return result\n        except exc.FilesError as e:\n            duration = time.time() - start_time\n            logger.error(f\"Upload failed: {location}, duration: {duration:.2f}s, error: {e}\")\n            raise\n\n    def content(self, file_data):\n        start_time = time.time()\n        try:\n            result = self.storage.content(file_data)\n            duration = time.time() - start_time\n            logger.info(f\"Content read successful: {file_data.location}, size: {len(result)}, duration: {duration:.2f}s\")\n            return result\n        except exc.FilesError as e:\n            duration = time.time() - start_time\n            logger.error(f\"Content read failed: {file_data.location}, duration: {duration:.2f}s, error: {e}\")\n            raise\n\n    def __getattr__(self, name):\n        # Delegate other methods to the wrapped storage\n        return getattr(self.storage, name)\n\n# Example usage\nmonitored_storage = MonitoredStorage(storage)\nfile_info = monitored_storage.upload(\"test.txt\", upload)\ncontent = monitored_storage.content(file_info)\n</code></pre>"},{"location":"tutorials/advanced_patterns/#conclusion","title":"Conclusion","text":"<p>This tutorial covered advanced patterns for using file-keeper effectively:</p> <ol> <li>Performance: Use streaming for large files and batch operations for multiple files</li> <li>Reliability: Implement retry mechanisms and circuit breakers for external services</li> <li>Integration: Connect file-keeper with popular web frameworks</li> <li>Security: Validate inputs to prevent vulnerabilities</li> <li>Monitoring: Track performance and errors for operational insight</li> </ol> <p>These patterns will help you build robust, scalable applications with file-keeper.</p>"},{"location":"tutorials/getting_started/","title":"Getting Started","text":"<p>This tutorial will walk you through the basics of using file-keeper to manage files across different storage backends. By the end of this tutorial, you'll understand how to:</p> <ul> <li>Install and set up file-keeper</li> <li>Configure different storage backends</li> <li>Perform basic file operations</li> <li>Handle errors gracefully</li> <li>Choose the right storage for your needs</li> </ul>"},{"location":"tutorials/getting_started/#installation","title":"Installation","text":"<p>First, install file-keeper using pip:</p> <pre><code>pip install file-keeper\n</code></pre> <p>For specific storage backends, you may need additional dependencies:</p> <pre><code># For S3 support\npip install 'file-keeper[s3]'\n\n# For all optional dependencies\npip install 'file-keeper[all]'\n</code></pre>"},{"location":"tutorials/getting_started/#basic-setup","title":"Basic Setup","text":"<p>Let's start with a simple example using in-memory storage for testing:</p> <pre><code>import file_keeper as fk\n\n# Create an in-memory storage for testing\nstorage = fk.make_storage(\"test_storage\", {\n    \"type\": \"file_keeper:memory\"\n})\n\n# Create an upload object from data\nupload = fk.make_upload(b\"Hello, file-keeper!\")\n\n# Upload the file\nfile_info = storage.upload(\"hello.txt\", upload)\n\nprint(f\"File uploaded: {file_info.location}\")\nprint(f\"File size: {file_info.size} bytes\")\n</code></pre>"},{"location":"tutorials/getting_started/#using-file-system-storage","title":"Using File System Storage","text":"<p>For production use, you'll likely want to use file system storage:</p> <pre><code>import tempfile\nimport file_keeper as fk\n\n# Create a temporary directory for our example\nwith tempfile.TemporaryDirectory() as tmpdir:\n    # Create file system storage\n    fs_storage = fk.make_storage(\"fs_example\", {\n        \"type\": \"file_keeper:fs\",\n        \"path\": tmpdir,\n        \"initialize\": True  # Create directory if it doesn't exist\n    })\n\n    # Upload a file\n    upload = fk.make_upload(b\"Sample file content\")\n    file_info = fs_storage.upload(\"sample.txt\", upload)\n\n    print(f\"File saved at: {fs_storage.full_path(file_info.location)}\")\n\n    # Read the file content back\n    content = fs_storage.content(file_info)\n    print(f\"Content: {content.decode('utf-8')}\")\n</code></pre>"},{"location":"tutorials/getting_started/#working-with-different-storage-types","title":"Working with Different Storage Types","text":"<p>file-keeper supports many storage backends. Here are examples of common ones:</p>"},{"location":"tutorials/getting_started/#memory-storage-for-testing","title":"Memory Storage (for testing)","text":"<pre><code>import file_keeper as fk\n\nmemory_storage = fk.make_storage(\"test\", {\n    \"type\": \"file_keeper:memory\"\n})\n</code></pre>"},{"location":"tutorials/getting_started/#file-system-storage-for-local-files","title":"File System Storage (for local files)","text":"<pre><code>fs_storage = fk.make_storage(\"local\", {\n    \"type\": \"file_keeper:fs\",\n    \"path\": \"/path/to/storage\",\n    \"initialize\": True\n})\n</code></pre>"},{"location":"tutorials/getting_started/#checking-capabilities","title":"Checking Capabilities","text":"<p>Different storage backends support different operations. Always check capabilities before performing operations:</p> <pre><code>import file_keeper as fk\n\nstorage = fk.make_storage(\"example\", {\n    \"type\": \"file_keeper:memory\"\n})\n\n# Check if storage supports removing files\nif storage.supports(fk.Capability.REMOVE):\n    # Safe to call remove\n    storage.remove(file_info)\nelse:\n    print(\"Remove operation not supported by this storage\")\n</code></pre>"},{"location":"tutorials/getting_started/#error-handling","title":"Error Handling","text":"<p>Always handle potential errors when working with file storage:</p> <pre><code>import file_keeper as fk\nfrom file_keeper import exc\n\nstorage = fk.make_storage(\"safe_storage\", {\n    \"type\": \"file_keeper:memory\"\n})\n\ntry:\n    upload = fk.make_upload(b\"Test content\")\n    file_info = storage.upload(\"test.txt\", upload)\n\n    # Try to read the content\n    content = storage.content(file_info)\n    print(f\"Successfully read: {content.decode()}\")\n\nexcept exc.FilesError as e:\n    print(f\"File operation failed: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"tutorials/getting_started/#complete-example-application","title":"Complete Example Application","text":"<p>Here's a complete example of a simple file management application:</p> <pre><code>import file_keeper as fk\nfrom file_keeper import exc\nimport tempfile\n\nclass FileManager:\n    def __init__(self, storage_config):\n        self.storage = fk.make_storage(\"manager\", storage_config)\n\n    def save_file(self, filename, content):\n        \"\"\"Save content to a file.\"\"\"\n        try:\n            upload = fk.make_upload(content.encode() if isinstance(content, str) else content)\n            file_info = self.storage.upload(filename, upload)\n            return file_info\n        except exc.FilesError as e:\n            print(f\"Failed to save file {filename}: {e}\")\n            return None\n\n    def load_file(self, file_info):\n        \"\"\"Load content from a file.\"\"\"\n        try:\n            if self.storage.supports(fk.Capability.STREAM):\n                return self.storage.content(file_info)\n        except exc.MissingFileError:\n            print(f\"File not found: {file_info.location}\")\n            return None\n        except exc.FilesError as e:\n            print(f\"Failed to load file {file_info.location}: {e}\")\n            return None\n\n    def delete_file(self, file_info):\n        \"\"\"Delete a file.\"\"\"\n        if self.storage.supports(fk.Capability.REMOVE):\n            try:\n                return self.storage.remove(file_info)\n            except exc.FilesError as e:\n                print(f\"Failed to delete file {file_info.location}: {e}\")\n                return False\n        else:\n            print(\"Delete operation not supported\")\n            return False\n\n# Example usage\nwith tempfile.TemporaryDirectory() as tmpdir:\n    # Initialize the file manager with file system storage\n    fm = FileManager({\n        \"type\": \"file_keeper:fs\",\n        \"path\": tmpdir,\n        \"initialize\": True\n    })\n\n    # Save a file\n    file_info = fm.save_file(\"example.txt\", \"Hello, file-keeper!\")\n    if file_info:\n        print(f\"File saved: {file_info.location}\")\n\n        # Load the file\n        content = fm.load_file(file_info)\n        if content:\n            print(f\"File content: {content.decode()}\")\n</code></pre>"},{"location":"tutorials/getting_started/#next-steps","title":"Next Steps","text":"<p>Now that you've learned the basics, you can:</p> <ul> <li>Learn about advanced features in the Core Concepts section</li> <li>Check out the API Reference for detailed information about all functions</li> <li>Look at the Configuration guide for advanced setup options</li> </ul> <p>In the next tutorial, we'll cover more advanced topics like custom adapters and performance optimization.</p>"}]}