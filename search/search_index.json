{"config":{"lang":["en"],"separator":"[\\s\\-\\.\\_]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>file-keeper provides an abstraction layer for storing and retrieving files, supporting various storage backends like local filesystems, cloud storage (S3, GCS), and more.  It simplifies file management by providing a consistent API regardless of the underlying storage.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install file-keeper using pip:</p> <pre><code>pip install file-keeper\n</code></pre> Additional dependencies <p>To use specific storage adapters, you may need to install extra dependencies. Most standard adapters do not require extras, but some \u2013 like those interfacing with external cloud providers \u2013 do. Here's a table of available extras:</p> Storage Type Adapter Name Extras Driver AWS S3 <code>file_keeper:s3</code> <code>s3</code> boto3 Apache Libcloud <code>file_keeper:libcloud</code> <code>libcloud</code> apache-libcloud Apache OpenDAL <code>file_keeper:opendal</code> <code>opendal</code> opendal Azure Blob Storage <code>file_keeper:azure_blob</code> <code>azure</code> azure-storage-blob Google Cloud Storage <code>file_keeper:gcs</code> <code>gcs</code> google-cloud-storage Redis <code>file_keeper:redis</code> <code>redis</code> redis SQLAlchemy <code>file_keeper:sqlalchemy</code> <code>sqlalchemy</code> SQLAlchemy <p>For example, to install file-keeper with S3 support:</p> <pre><code>pip install 'file-keeper[s3]'\n</code></pre>"},{"location":"#basic-configuration-and-usage-fs-adapter","title":"Basic configuration and usage (FS adapter)","text":"<p>Let's start with a simple example using the local filesystem (FS) adapter.</p> <p>Example</p> <pre><code>from file_keeper import make_storage, make_upload\n\n# Create a storage instance.  The 'path' setting specifies the root directory\n# for storing files. 'initialize' will automatically create the directory\n# if it doesn't exist.\nstorage = make_storage(\n    \"my_fs_storage\",  # A name for your storage (for logging/debugging)\n    {\n        \"type\": \"file_keeper:fs\",\n        \"path\": \"/tmp/my_filekeeper_files\",\n        \"initialize\": True,\n    },\n)\n\n# Create an upload object from a byte string.\nupload = make_upload(b\"Hello, file-keeper!\")\n\n# Upload the file.  This returns a FileData object containing information\n# about the uploaded file.\nfile_data = storage.upload(\"my_file.txt\", upload)\n\n# Print the file data.\nprint(file_data)\n\n# The file is now stored in /tmp/my_filekeeper_files/my_file.txt\n\n# Get the content of file using corresponding FileData object\ncontent: bytes = storage.content(file_data)\n</code></pre> <p>Explanation:</p> <ul> <li><code>make_storage()</code>: Creates a storage instance with the specified configuration.</li> <li><code>make_upload()</code>: Creates an <code>Upload</code> object from the data you want to store.</li> <li><code>storage.upload()</code>: Uploads the data to the storage.</li> <li><code>FileData</code>:  A dataclass that contains metadata about the uploaded file, including its location, size, content type, and hash.</li> <li><code>storage.content()</code>: Locates file using <code>FileData</code> and returs byte string with its content</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Utilities available for import directly from the <code>file_keeper</code> module.</p>"},{"location":"api/#file_keeper.make_storage","title":"<code>make_storage(name, settings)</code>","text":"<p>Initialize storage instance with specified settings.</p> <p>Storage adapter is defined by <code>type</code> key of the settings. The rest of settings depends on the specific adapter.</p> PARAMETER DESCRIPTION <code>name</code> <p>name of the storage</p> <p> TYPE: <code>str</code> </p> <code>settings</code> <p>configuration for the storage</p> <p> TYPE: <code>dict[str, Any]</code> </p> RETURNS DESCRIPTION <code>Storage</code> <p>storage instance</p> RAISES DESCRIPTION <code>UnknownAdapterError</code> <p>storage adapter is not registered</p> Example <pre><code>storage = make_storage(\"memo\", {\"type\": \"files:redis\"})\n</code></pre>"},{"location":"api/#file_keeper.get_storage","title":"<code>get_storage(name, settings=None)</code>","text":"<p>Get storage from the pool.</p> <p>If storage accessed for the first time, it's initialized and added to the pool. After that the same storage is returned every time the function is called with the given name.</p> <p>Settings are required only for initialization, so you can omit them if you are sure that storage exists. Additionally, if <code>settins</code> are not specified but storage is missing from the pool, file-keeper makes an attempt to initialize storage usign global configuration. Global configuration can be provided as:</p> <ul> <li><code>FILE_KEEPER_CONFIG</code> environment variable that points to a file with configuration</li> <li> <p><code>.file-keeper.json</code> in the current directory hierarchy</p> </li> <li> <p><code>file-keeper/file-keeper.json</code> in the user's config directory(usually,   <code>~/.config/</code>) when platformdirs   installed in the environment, for example via <code>pip install   'file-keeper[user_config]'</code> extras.</p> </li> </ul> <p>File must contain storage configuration provided in format</p> <pre><code>{\n    \"storages\": {\n        # storage name\n        \"my_storage\": {\n            # storage options\n            \"type\": \"file_keeper:memory\"\n        }\n    }\n}\n</code></pre> <p>JSON configuration is used by default, because python has built-in JSON support. Additional file extensions are checked if environment contains correspoinding package:</p> Package Extension tomllib <code>.toml</code> tomli <code>.toml</code> pyyaml <code>.yaml</code>, <code>.yml</code> <p>Extensions are checked in order <code>.toml</code>, <code>.yaml</code>, <code>.yml</code>, <code>.json</code>.</p>"},{"location":"api/#file_keeper.make_upload","title":"<code>make_upload(value)</code>","text":"<p>Convert value into Upload object.</p> <p>Use this function for simple and reliable initialization of Upload object. Avoid creating Upload manually, unless you are 100% sure you can provide correct MIMEtype, size and stream.</p> PARAMETER DESCRIPTION <code>value</code> <p>content of the file</p> <p> TYPE: <code>Any</code> </p> RAISES DESCRIPTION <code>TypeError</code> <p>content has unsupported type</p> RETURNS DESCRIPTION <code>Upload</code> <p>upload object with specified content</p> Example <pre><code>upload = storage.upload(\"file.txt\", make_upload(b\"hello world\"))\n</code></pre>"},{"location":"api/#file_keeper.Storage","title":"<code>Storage</code>","text":"<p>Base class for storage implementation.</p> PARAMETER DESCRIPTION <code>settings</code> <p>storage configuration</p> <p> TYPE: <code>Mapping[str, Any] | Settings</code> </p> Example <pre><code>class MyStorage(Storage):\n    SettingsFactory = MySettings\n    UploaderFactory = MyUploader\n    ManagerFactory = MyManager\n    ReaderFactory = MyReader\n</code></pre>"},{"location":"api/#file_keeper.Storage.ManagerFactory","title":"<code>ManagerFactory = Manager</code>  <code>class-attribute</code>","text":"<p>Factory class for manager service.</p>"},{"location":"api/#file_keeper.Storage.ReaderFactory","title":"<code>ReaderFactory = Reader</code>  <code>class-attribute</code>","text":"<p>Factory class for reader service.</p>"},{"location":"api/#file_keeper.Storage.SettingsFactory","title":"<code>SettingsFactory = Settings</code>  <code>class-attribute</code>","text":"<p>Factory class for storage settings.</p>"},{"location":"api/#file_keeper.Storage.UploaderFactory","title":"<code>UploaderFactory = Uploader</code>  <code>class-attribute</code>","text":"<p>Factory class for uploader service.</p>"},{"location":"api/#file_keeper.Storage.capabilities","title":"<code>capabilities = self.compute_capabilities()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Operations supported by storage. Computed from capabilities of services during storage initialization.</p>"},{"location":"api/#file_keeper.Storage.hidden","title":"<code>hidden = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Flag that marks unsafe/experimental storages.</p>"},{"location":"api/#file_keeper.Storage.analyze","title":"<code>analyze(location, /, **kwargs)</code>","text":"<p>Return file details.</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the file to analyze.</p> <p> TYPE: <code>Location</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.append","title":"<code>append(data, upload, /, **kwargs)</code>","text":"<p>Append content to existing file.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to append to.</p> <p> TYPE: <code>FileData</code> </p> <code>upload</code> <p>The Upload object containing the content to append.</p> <p> TYPE: <code>Upload</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.compose","title":"<code>compose(location, /, *files, **kwargs)</code>","text":"<p>Combine multiple files into a new file.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the composed file.</p> <p> TYPE: <code>Location</code> </p> <code>*files</code> <p>FileData objects representing the files to combine.</p> <p> TYPE: <code>FileData</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.compose_synthetic","title":"<code>compose_synthetic(location, dest_storage, /, *files, **kwargs)</code>","text":"<p>Generic composition that relies on APPEND.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the composed file.</p> <p> TYPE: <code>Location</code> </p> <code>dest_storage</code> <p>The destination storage</p> <p> TYPE: <code>Storage</code> </p> <code>*files</code> <p>FileData objects representing the files to combine.</p> <p> TYPE: <code>FileData</code> DEFAULT: <code>()</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.compute_capabilities","title":"<code>compute_capabilities()</code>","text":"<p>Computes the capabilities of the storage based on its services.</p>"},{"location":"api/#file_keeper.Storage.configure","title":"<code>configure(settings)</code>  <code>classmethod</code>","text":"<p>Initialize storage configuration.</p> <p>This method is responsible for transforming mapping with options into storage's settings. It also can initialize additional services and perform extra work, like verifying that storage is ready to accept uploads.</p> PARAMETER DESCRIPTION <code>settings</code> <p>mapping with storage configuration</p> <p> TYPE: <code>Mapping[str, Any] | Settings</code> </p>"},{"location":"api/#file_keeper.Storage.content","title":"<code>content(data, /, **kwargs)</code>","text":"<p>Return file content as a single byte object.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.copy","title":"<code>copy(location, data, /, **kwargs)</code>","text":"<p>Copy file inside the storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the copied file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to copy.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.copy_synthetic","title":"<code>copy_synthetic(location, data, dest_storage, /, **kwargs)</code>","text":"<p>Generic implementation of the copy operation that relies on CREATE.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the copied file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to copy.</p> <p> TYPE: <code>FileData</code> </p> <code>dest_storage</code> <p>The destination storage</p> <p> TYPE: <code>Storage</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.exists","title":"<code>exists(data, /, **kwargs)</code>","text":"<p>Check if file exists in the storage.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to check.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.file_as_upload","title":"<code>file_as_upload(data, **kwargs)</code>","text":"<p>Make an Upload with file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object to wrap into Upload</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.full_path","title":"<code>full_path(location, /, **kwargs)</code>","text":"<p>Compute path to the file from the storage's root.</p> PARAMETER DESCRIPTION <code>location</code> <p>location of the file object</p> <p> TYPE: <code>Location</code> </p> <code>**kwargs</code> <p>exra parameters for custom storages</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>str</code> <p>full path required to access location</p>"},{"location":"api/#file_keeper.Storage.make_manager","title":"<code>make_manager()</code>","text":"<p>Initialize manager service.</p>"},{"location":"api/#file_keeper.Storage.make_reader","title":"<code>make_reader()</code>","text":"<p>Initialize reader service.</p>"},{"location":"api/#file_keeper.Storage.make_uploader","title":"<code>make_uploader()</code>","text":"<p>Initialize uploader service.</p>"},{"location":"api/#file_keeper.Storage.move","title":"<code>move(location, data, /, **kwargs)</code>","text":"<p>Move file to a different location inside the storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the moved file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to move.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.move_synthetic","title":"<code>move_synthetic(location, data, dest_storage, /, **kwargs)</code>","text":"<p>Generic implementation of move operation.</p> <p>Relies on CREATE and REMOVE.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the moved file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to move.</p> <p> TYPE: <code>FileData</code> </p> <code>dest_storage</code> <p>The destination storage</p> <p> TYPE: <code>Storage</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.multipart_complete","title":"<code>multipart_complete(data, /, **kwargs)</code>","text":"<p>Verify file integrity and finalize incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.multipart_refresh","title":"<code>multipart_refresh(data, /, **kwargs)</code>","text":"<p>Show details of the incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.multipart_remove","title":"<code>multipart_remove(data, /, **kwargs)</code>","text":"<p>Interrupt and remove incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.multipart_start","title":"<code>multipart_start(data, /, **kwargs)</code>","text":"<p>Prepare everything for multipart upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.multipart_update","title":"<code>multipart_update(data, /, **kwargs)</code>","text":"<p>Add data to the incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.one_time_link","title":"<code>one_time_link(data, /, **kwargs)</code>","text":"<p>Return one-time download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.permanent_link","title":"<code>permanent_link(data, /, **kwargs)</code>","text":"<p>Return permanent download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.prepare_location","title":"<code>prepare_location(location, upload_or_data=None, /, **kwargs)</code>","text":"<p>Transform and sanitize location using configured functions.</p>"},{"location":"api/#file_keeper.Storage.range","title":"<code>range(data, start=0, end=None, /, **kwargs)</code>","text":"<p>Return slice of the file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>start</code> <p>The starting byte offset.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>end</code> <p>The ending byte offset (inclusive).</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.range_synthetic","title":"<code>range_synthetic(data, start=0, end=None, /, **kwargs)</code>","text":"<p>Generic implementation of range operation that relies on STREAM.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>start</code> <p>The starting byte offset.</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>end</code> <p>The ending byte offset (inclusive).</p> <p> TYPE: <code>int | None</code> DEFAULT: <code>None</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.remove","title":"<code>remove(data, /, **kwargs)</code>","text":"<p>Remove file from the storage.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to remove.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.resumable_refresh","title":"<code>resumable_refresh(data, /, **kwargs)</code>","text":"<p>Show details of the incomplete resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.resumable_remove","title":"<code>resumable_remove(data, /, **kwargs)</code>","text":"<p>Remove incomplete resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.resumable_resume","title":"<code>resumable_resume(data, /, **kwargs)</code>","text":"<p>Resume the interrupted resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.resumable_start","title":"<code>resumable_start(data, /, **kwargs)</code>","text":"<p>Prepare everything for resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.scan","title":"<code>scan(**kwargs)</code>","text":"<p>List all locations(filenames) in storage.</p> PARAMETER DESCRIPTION <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.signed","title":"<code>signed(action, duration, location, **kwargs)</code>","text":"<p>Make an URL for signed action.</p> PARAMETER DESCRIPTION <code>action</code> <p>The action to sign (e.g., \"upload\", \"download\").</p> <p> TYPE: <code>SignedAction</code> </p> <code>duration</code> <p>The duration for which the signed URL is valid.</p> <p> TYPE: <code>int</code> </p> <code>location</code> <p>The location of the file to sign.</p> <p> TYPE: <code>Location</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.stream","title":"<code>stream(data, /, **kwargs)</code>","text":"<p>Return byte-stream of the file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to stream.</p> <p> TYPE: <code>FileData</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.supports","title":"<code>supports(operation)</code>","text":"<p>Check whether the storage supports operation.</p>"},{"location":"api/#file_keeper.Storage.supports_synthetic","title":"<code>supports_synthetic(operation, dest)</code>","text":"<p>Check if the storage can emulate operation using other operations.</p>"},{"location":"api/#file_keeper.Storage.temporal_link","title":"<code>temporal_link(data, duration, /, **kwargs)</code>","text":"<p>Return temporal download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>duration</code> <p>The duration for which the link is valid.</p> <p> TYPE: <code>int</code> </p> <code>**kwargs</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Storage.upload","title":"<code>upload(location, upload, /, **kwargs)</code>","text":"<p>Upload file using single stream.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the upload.</p> <p> TYPE: <code>Location</code> </p> <code>upload</code> <p>The Upload object containing the file data.</p> <p> TYPE: <code>Upload</code> </p> <code>**kwargs</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p>"},{"location":"api/#file_keeper.Capability","title":"<code>Capability</code>","text":"<p>               Bases: <code>Flag</code></p> <p>Enumeration of operations supported by the storage.</p> Example <pre><code>read_and_write = Capability.STREAM | Capability.CREATE\nif storage.supports(read_and_write)\n    ...\n</code></pre>"},{"location":"api/#file_keeper.Capability.ANALYZE","title":"<code>ANALYZE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Return file details from the storage.</p>"},{"location":"api/#file_keeper.Capability.APPEND","title":"<code>APPEND = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Add content to the existing file.</p>"},{"location":"api/#file_keeper.Capability.COMPOSE","title":"<code>COMPOSE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Combine multiple files into a new one in the same storage.</p>"},{"location":"api/#file_keeper.Capability.COPY","title":"<code>COPY = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Make a copy of the file inside the same storage.</p>"},{"location":"api/#file_keeper.Capability.CREATE","title":"<code>CREATE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Create a file as an atomic object.</p>"},{"location":"api/#file_keeper.Capability.EXISTS","title":"<code>EXISTS = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Check if file exists.</p>"},{"location":"api/#file_keeper.Capability.LINK_ONE_TIME","title":"<code>LINK_ONE_TIME = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Make one-time download link.</p>"},{"location":"api/#file_keeper.Capability.LINK_PERMANENT","title":"<code>LINK_PERMANENT = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Make permanent download link.</p>"},{"location":"api/#file_keeper.Capability.LINK_TEMPORAL","title":"<code>LINK_TEMPORAL = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Make expiring download link.</p>"},{"location":"api/#file_keeper.Capability.MOVE","title":"<code>MOVE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Move file to a different location inside the same storage.</p>"},{"location":"api/#file_keeper.Capability.MULTIPART","title":"<code>MULTIPART = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Create file in 3 stages: initialize, upload(repeatable), complete.</p>"},{"location":"api/#file_keeper.Capability.RANGE","title":"<code>RANGE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Return specific range of bytes from the file.</p>"},{"location":"api/#file_keeper.Capability.REMOVE","title":"<code>REMOVE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Remove file from the storage.</p>"},{"location":"api/#file_keeper.Capability.RESUMABLE","title":"<code>RESUMABLE = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Perform resumable uploads that can be continued after interruption.</p>"},{"location":"api/#file_keeper.Capability.SCAN","title":"<code>SCAN = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Iterate over all files in the storage.</p>"},{"location":"api/#file_keeper.Capability.SIGNED","title":"<code>SIGNED = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Generate signed URL for specific operation.</p>"},{"location":"api/#file_keeper.Capability.STREAM","title":"<code>STREAM = enum.auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Return file content as stream of bytes.</p>"},{"location":"api/#file_keeper.Capability.can","title":"<code>can(operation)</code>","text":"<p>Check whether the cluster supports given operation.</p>"},{"location":"api/#file_keeper.Capability.exclude","title":"<code>exclude(*capabilities)</code>","text":"<p>Remove capabilities from the cluster.</p> PARAMETER DESCRIPTION <code>capabilities</code> <p>removed capabilities</p> <p> TYPE: <code>Capability</code> </p> Example <pre><code>cluster = cluster.exclude(Capability.REMOVE)\n</code></pre>"},{"location":"api/#file_keeper.Settings","title":"<code>Settings</code>  <code>dataclass</code>","text":"<p>Settings for the storage adapter.</p>"},{"location":"api/#file_keeper.Settings.disabled_capabilities","title":"<code>disabled_capabilities = cast('list[str]', dataclasses.field(default_factory=list))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Capabilities that are not supported even if implemented.</p>"},{"location":"api/#file_keeper.Settings.initialize","title":"<code>initialize = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prepare storage backend for uploads(create path, bucket, DB)</p>"},{"location":"api/#file_keeper.Settings.location_transformers","title":"<code>location_transformers = cast('list[str]', dataclasses.field(default_factory=list))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of transformations applied to the file location.</p>"},{"location":"api/#file_keeper.Settings.name","title":"<code>name = 'unknown'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Descriptive name of the storage used for debugging.</p>"},{"location":"api/#file_keeper.Settings.override_existing","title":"<code>override_existing = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>If file already exists, replace it with new content.</p>"},{"location":"api/#file_keeper.Settings.path","title":"<code>path = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Prefix for the file's location.</p>"},{"location":"api/#file_keeper.Settings.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Make settings object using dictionary as a source.</p> <p>Any unexpected options are extracted from the <code>data</code> to avoid initialization errors from dataclass constructor.</p> PARAMETER DESCRIPTION <code>data</code> <p>mapping with settings</p> <p> TYPE: <code>Mapping[str, Any]</code> </p> RETURNS DESCRIPTION <code>Settings</code> <p>settings object built from data</p>"},{"location":"api/#file_keeper.Uploader","title":"<code>Uploader</code>","text":"<p>               Bases: <code>StorageService</code></p> <p>Service responsible for writing data into a storage.</p> <p><code>Storage</code> internally calls methods of this service. For example, <code>Storage.upload(location, upload, **kwargs)</code> results in <code>Uploader.upload(location, upload, kwargs)</code>.</p> Example <pre><code>class MyUploader(Uploader):\n    capabilities = Capability.CREATE\n\n    def upload(\n        self, location: types.Location, upload: Upload, extras: dict[str, Any]\n    ) -&gt; FileData:\n        reader = upload.hashing_reader()\n\n        with open(location, \"wb\") as dest:\n            dest.write(reader.read())\n\n        return FileData(\n            location, upload.size,\n            upload.content_type,\n            reader.get_hash()\n        )\n</code></pre>"},{"location":"api/#file_keeper.Uploader.multipart_complete","title":"<code>multipart_complete(data, extras)</code>","text":"<p>Verify file integrity and finalize incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.multipart_refresh","title":"<code>multipart_refresh(data, extras)</code>","text":"<p>Show details of the incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.multipart_remove","title":"<code>multipart_remove(data, extras)</code>","text":"<p>Interrupt and remove incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.multipart_start","title":"<code>multipart_start(data, extras)</code>","text":"<p>Prepare everything for multipart(resumable) upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.multipart_update","title":"<code>multipart_update(data, extras)</code>","text":"<p>Add data to the incomplete upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.resumable_refresh","title":"<code>resumable_refresh(data, extras)</code>","text":"<p>Show details of the incomplete resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.resumable_remove","title":"<code>resumable_remove(data, extras)</code>","text":"<p>Remove incomplete resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.resumable_resume","title":"<code>resumable_resume(data, extras)</code>","text":"<p>Resume the interrupted resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.resumable_start","title":"<code>resumable_start(data, extras)</code>","text":"<p>Prepare everything for resumable upload.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object containing the upload metadata.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Uploader.upload","title":"<code>upload(location, upload, extras)</code>","text":"<p>Upload file using single stream.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the upload.</p> <p> TYPE: <code>Location</code> </p> <code>upload</code> <p>The Upload object containing the file data.</p> <p> TYPE: <code>Upload</code> </p> <code>extras</code> <p>Additional metadata for the upload.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager","title":"<code>Manager</code>","text":"<p>               Bases: <code>StorageService</code></p> <p>Service responsible for maintenance file operations.</p> <p><code>Storage</code> internally calls methods of this service. For example, <code>Storage.remove(data, **kwargs)</code> results in <code>Manager.remove(data, kwargs)</code>.</p> Example <pre><code>class MyManager(Manager):\n    capabilities = Capability.REMOVE\n    def remove(\n        self, data: FileData|FileData, extras: dict[str, Any]\n    ) -&gt; bool:\n        os.remove(data.location)\n        return True\n</code></pre>"},{"location":"api/#file_keeper.Manager.analyze","title":"<code>analyze(location, extras)</code>","text":"<p>Return details about location.</p> PARAMETER DESCRIPTION <code>location</code> <p>The location of the file to analyze.</p> <p> TYPE: <code>Location</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.append","title":"<code>append(data, upload, extras)</code>","text":"<p>Append content to existing file.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to append to.</p> <p> TYPE: <code>FileData</code> </p> <code>upload</code> <p>The Upload object containing the content to append.</p> <p> TYPE: <code>Upload</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.compose","title":"<code>compose(location, datas, extras)</code>","text":"<p>Combine multipe file inside the storage into a new one.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the composed file.</p> <p> TYPE: <code>Location</code> </p> <code>datas</code> <p>An iterable of FileData objects representing the files to combine.</p> <p> TYPE: <code>Iterable[FileData]</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.copy","title":"<code>copy(location, data, extras)</code>","text":"<p>Copy file inside the storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the copied file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to copy.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.exists","title":"<code>exists(data, extras)</code>","text":"<p>Check if file exists in the storage.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to check.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.move","title":"<code>move(location, data, extras)</code>","text":"<p>Move file to a different location inside the storage.</p> PARAMETER DESCRIPTION <code>location</code> <p>The destination location for the moved file.</p> <p> TYPE: <code>Location</code> </p> <code>data</code> <p>The FileData object representing the file to move.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.remove","title":"<code>remove(data, extras)</code>","text":"<p>Remove file from the storage.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to remove.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.scan","title":"<code>scan(extras)</code>","text":"<p>List all locations(filenames) in storage.</p> PARAMETER DESCRIPTION <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Manager.signed","title":"<code>signed(action, duration, location, extras)</code>","text":"<p>Make an URL for signed action.</p> PARAMETER DESCRIPTION <code>action</code> <p>The action to sign (e.g., \"upload\", \"download\").</p> <p> TYPE: <code>SignedAction</code> </p> <code>duration</code> <p>The duration for which the signed URL is valid.</p> <p> TYPE: <code>int</code> </p> <code>location</code> <p>The location of the file to sign.</p> <p> TYPE: <code>Location</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Reader","title":"<code>Reader</code>","text":"<p>               Bases: <code>StorageService</code></p> <p>Service responsible for reading data from the storage.</p> <p><code>Storage</code> internally calls methods of this service. For example, <code>Storage.stream(data, **kwargs)</code> results in <code>Reader.stream(data, kwargs)</code>.</p> Example <pre><code>class MyReader(Reader):\n    capabilities = Capability.STREAM\n\n    def stream(\n        self, data: data.FileData, extras: dict[str, Any]\n    ) -&gt; Iterable[bytes]:\n        return open(data.location, \"rb\")\n</code></pre>"},{"location":"api/#file_keeper.Reader.content","title":"<code>content(data, extras)</code>","text":"<p>Return file content as a single byte object.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Reader.one_time_link","title":"<code>one_time_link(data, extras)</code>","text":"<p>Return one-time download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Reader.permanent_link","title":"<code>permanent_link(data, extras)</code>","text":"<p>Return permanent download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Reader.range","title":"<code>range(data, start, end, extras)</code>","text":"<p>Return slice of the file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to read.</p> <p> TYPE: <code>FileData</code> </p> <code>start</code> <p>The starting byte offset.</p> <p> TYPE: <code>int</code> </p> <code>end</code> <p>The ending byte offset (inclusive).</p> <p> TYPE: <code>int | None</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Reader.stream","title":"<code>stream(data, extras)</code>","text":"<p>Return byte-stream of the file content.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file to stream.</p> <p> TYPE: <code>FileData</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.Reader.temporal_link","title":"<code>temporal_link(data, duration, extras)</code>","text":"<p>Return temporal download link.</p> PARAMETER DESCRIPTION <code>data</code> <p>The FileData object representing the file.</p> <p> TYPE: <code>FileData</code> </p> <code>duration</code> <p>The duration for which the link is valid.</p> <p> TYPE: <code>int</code> </p> <code>extras</code> <p>Additional metadata for the operation.</p> <p> TYPE: <code>dict[str, Any]</code> </p>"},{"location":"api/#file_keeper.FileData","title":"<code>FileData</code>  <code>dataclass</code>","text":"<p>               Bases: <code>BaseData[TData]</code></p> <p>Information required by storage to operate the file.</p> PARAMETER DESCRIPTION <code>location</code> <p>filepath, filename or any other type of unique identifier</p> <p> TYPE: <code>Location</code> </p> <code>size</code> <p>size of the file in bytes</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> <code>content_type</code> <p>MIMEtype of the file</p> <p> TYPE: <code>str</code> DEFAULT: <code>'application/octet-stream'</code> </p> <code>hash</code> <p>checksum of the file</p> <p> TYPE: <code>str</code> DEFAULT: <code>''</code> </p> <code>storage_data</code> <p>additional details set by storage adapter</p> <p> TYPE: <code>dict[str, Any]</code> DEFAULT: <code>cast('dict[str, Any]', field(default_factory=dict))</code> </p> Example <pre><code>FileData(\n    \"local/path.txt\",\n    123,\n    \"text/plain\",\n    md5_of_content,\n)\n</code></pre>"},{"location":"api/#file_keeper.Upload","title":"<code>Upload</code>  <code>dataclass</code>","text":"<p>Standard upload details.</p> PARAMETER DESCRIPTION <code>stream</code> <p>iterable of bytes or file-like object</p> <p> TYPE: <code>PStream</code> </p> <code>filename</code> <p>name of the file</p> <p> TYPE: <code>str</code> </p> <code>size</code> <p>size of the file in bytes</p> <p> TYPE: <code>int</code> </p> <code>content_type</code> <p>MIMEtype of the file</p> <p> TYPE: <code>str</code> </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Upload(\n&gt;&gt;&gt;     BytesIO(b\"hello world\"),\n&gt;&gt;&gt;     \"file.txt\",\n&gt;&gt;&gt;     11,\n&gt;&gt;&gt;     \"text/plain\",\n&gt;&gt;&gt; )\n</code></pre>"},{"location":"api/#file_keeper.Upload.content_type","title":"<code>content_type</code>  <code>instance-attribute</code>","text":"<p>MIME Type of the file</p>"},{"location":"api/#file_keeper.Upload.filename","title":"<code>filename</code>  <code>instance-attribute</code>","text":"<p>Name of the file</p>"},{"location":"api/#file_keeper.Upload.seekable_stream","title":"<code>seekable_stream</code>  <code>property</code>","text":"<p>Return stream that can be rewinded after reading.</p> <p>If internal stream does not support file-like <code>seek</code>, nothing is returned from this property.</p> <p>Use this property if you want to read the file ahead, to get CSV column names, list of files inside ZIP, EXIF metadata. If you get <code>None</code> from it, stream does not support seeking and you won't be able to rewind cursor to the beginning of the file after reading something.</p> Example <pre><code>upload = make_upload(...)\nif fd := upload.seekable_stream():\n    # read fragment of the file\n    chunk = fd.read(1024)\n    # move cursor to the end of the stream\n    fd.seek(0, 2)\n    # position of the cursor is the same as number of bytes in stream\n    size = fd.tell()\n    # move cursor back, because you don't want to accidentally loose\n    # any bites from the beginning of stream when uploader reads from it\n    fd.seek(0)\n</code></pre> RETURNS DESCRIPTION <code>PSeekableStream | None</code> <p>file-like stream or nothing</p>"},{"location":"api/#file_keeper.Upload.size","title":"<code>size</code>  <code>instance-attribute</code>","text":"<p>Size of the file</p>"},{"location":"api/#file_keeper.Upload.stream","title":"<code>stream</code>  <code>instance-attribute</code>","text":"<p>Content as iterable of bytes</p>"},{"location":"api/#file_keeper.Upload.hashing_reader","title":"<code>hashing_reader(**kwargs)</code>","text":"<p>Get reader for the upload that computes hash while reading content.</p>"},{"location":"api/#file_keeper.Location","title":"<code>Location = NewType('Location', str)</code>  <code>module-attribute</code>","text":""},{"location":"api/#file_keeper.SignedAction","title":"<code>SignedAction = Literal['upload', 'download', 'delete']</code>  <code>module-attribute</code>","text":""},{"location":"api/#file_keeper.HashingReader","title":"<code>HashingReader</code>","text":"<p>IO stream wrapper that computes content hash while stream is consumed.</p> PARAMETER DESCRIPTION <code>stream</code> <p>iterable of bytes or file-like object</p> <p> TYPE: <code>PStream</code> </p> <code>chunk_size</code> <p>max number of bytes read at once</p> <p> TYPE: <code>int</code> DEFAULT: <code>CHUNK_SIZE</code> </p> <code>algorithm</code> <p>hashing algorithm</p> <p> TYPE: <code>str</code> DEFAULT: <code>'md5'</code> </p> Example <pre><code>reader = HashingReader(readable_stream)\nfor chunk in reader:\n    ...\nprint(f\"Hash: {reader.get_hash()}\")\n</code></pre>"},{"location":"api/#file_keeper.HashingReader.exhaust","title":"<code>exhaust()</code>","text":"<p>Exhaust internal stream to compute final version of content hash.</p> <p>Note, this method does not returns data from the stream. The content will be irreversibly lost after method execution.</p>"},{"location":"api/#file_keeper.HashingReader.get_hash","title":"<code>get_hash()</code>","text":"<p>Get current content hash as a string.</p>"},{"location":"api/#file_keeper.HashingReader.read","title":"<code>read()</code>","text":"<p>Read and return all bytes from stream at once.</p>"},{"location":"api/#file_keeper.Registry","title":"<code>Registry</code>","text":"<p>               Bases: <code>Generic[V, K]</code></p> <p>Mutable collection of objects.</p> Example <pre><code>col = Registry()\n\ncol.register(\"one\", 1)\nassert col.get(\"one\") == 1\n\ncol.reset()\nassert col.get(\"one\") is None\n\nassert list(col) == [\"one\"]\n</code></pre>"},{"location":"api/#file_keeper.Registry.collect","title":"<code>collect()</code>","text":"<p>Collect members of the registry.</p>"},{"location":"api/#file_keeper.Registry.decorated","title":"<code>decorated(key)</code>","text":"<p>Collect member via decorator.</p>"},{"location":"api/#file_keeper.Registry.get","title":"<code>get(key)</code>","text":"<p>Get the optional member from registry.</p>"},{"location":"api/#file_keeper.Registry.pop","title":"<code>pop(key)</code>","text":"<p>Remove the member from registry.</p>"},{"location":"api/#file_keeper.Registry.register","title":"<code>register(key, member)</code>","text":"<p>Add a member to registry.</p>"},{"location":"api/#file_keeper.Registry.reset","title":"<code>reset()</code>","text":"<p>Remove all members from registry.</p>"},{"location":"api/#file_keeper.IterableBytesReader","title":"<code>IterableBytesReader</code>","text":"<p>               Bases: <code>AbstractReader[Iterable[int]]</code></p> <p>Wrapper that transforms iterable of bytes into readable stream.</p> Example <p>The simplest iterable of bytes is a list that contains byte strings: <pre><code>parts = [b\"hello\", b\" \", b\"world\"]\nreader = IterableBytesReader(parts)\nassert reader.read() == b\"hello world\"\n</code></pre></p> <p>More realistic scenario is wrapping generator that produces byte string in order to initialize Upload: <pre><code>def data_generator():\n    yield b\"hello\"\n    yield b\" \"\n    yield b\"world\"\n\nstream = IterableBytesReader(data_generator())\nupload = Upload(stream, \"my_file.txt\", 11, \"text/plain\")\n</code></pre></p>"},{"location":"api/#file_keeper.adapters","title":"<code>adapters = Registry['type[Storage]']()</code>  <code>module-attribute</code>","text":""},{"location":"api/#file_keeper.parse_filesize","title":"<code>parse_filesize(value)</code>","text":"<p>Transform human-readable filesize into an integer.</p> PARAMETER DESCRIPTION <code>value</code> <p>human-readable filesize</p> <p> TYPE: <code>str</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>size cannot be parsed or uses unknown units</p> Example <pre><code>size = parse_filesize(\"10GiB\")\nassert size == 10_737_418_240\n</code></pre>"},{"location":"api/#file_keeper.humanize_filesize","title":"<code>humanize_filesize(value, base=SI_BASE)</code>","text":"<p>Transform an integer into human-readable filesize.</p> PARAMETER DESCRIPTION <code>value</code> <p>size in bytes</p> <p> TYPE: <code>int | float</code> </p> <code>base</code> <p>1000 for SI(KB, MB) or 1024 for binary(KiB, MiB)</p> <p> TYPE: <code>int</code> DEFAULT: <code>SI_BASE</code> </p> RAISES DESCRIPTION <code>ValueError</code> <p>base is not recognized</p> Example <pre><code>size = humanize_filesize(10_737_418_240, base=1024)\nassert size == \"10GiB\"\nsize = humanize_filesize(10_418_240, base=1024)\nassert size == \"9.9MiB\"\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v010a1-2025-08-12","title":"v0.1.0a1 - 2025-08-12","text":"<p>Compare with v0.0.10</p>"},{"location":"changelog/#features","title":"\ud83d\ude80 Features","text":"<ul> <li>[breaking] frozen FileData and MultipartData (cb9dbf8)</li> <li>global storage configuration via file-keeper.json (58939eb)</li> <li>define <code>RESUMABLE</code> capability (bab84fa)</li> <li>complete GCS adapter (db97e20)</li> <li>Azure Blob storage adapter (fb491b2)</li> <li>zip adapter (3b3f978)</li> <li>add SIGNED capability (fc5fcbb)</li> <li>Storage.full_path (7974aca)</li> <li>add generic disabled_capabilities option (c335071)</li> <li>add EXISTS, SCAN, MOVE and COPY to s3 adapter (2cb762e)</li> <li>add EXISTS and ANALYZE to libcloud adapter (12349aa)</li> <li>less strict typing rules for storage settings (247d1c6)</li> <li>remove str from exceptions (2ecd8a2)</li> <li>add memory storage (3abc218)</li> <li>storage.ext.register accepts optional <code>reset</code> parameter (c5edce8)</li> <li>Settings log extra options with debug level instead of raising an exception (7308578)</li> <li>add null storage (c1f8476)</li> </ul>"},{"location":"changelog/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>s3 unconditionally overrides files (c059f2b)</li> <li>storage settings keep a lot of intermediate parameters (cf69cf2)</li> <li>libcloud silently overrides existing objects (599f099)</li> </ul>"},{"location":"changelog/#refactor","title":"\ud83d\ude9c Refactor","text":"<ul> <li>[breaking] remove <code>location</code> from arguments of <code>multipart_start</code> (876ce46)</li> <li>[breaking] drop <code>MultipartData</code> and use <code>FileData</code> instead everywhere (c1c01c3)</li> <li>[breaking] <code>Storage.remove</code> does not accept <code>MultipartData</code>. Use <code>Storage.multipart_remove</code> instead (ce3e522)</li> <li>[breaking] <code>create_path</code> option for fs renamed to <code>initialize</code> (1329997)</li> <li>[breaking] Storage.temporal_link requires <code>duration</code> parameter (0d92777)</li> <li>[breaking] Storage.stream_as_upload renamed to file_as_upload (29ec68b)</li> <li>[breaking] fs and opendal settings no longer have recursive flag (3f6e29b)</li> <li>redis uses <code>bucket</code> option instead of <code>path</code> (966241f)</li> <li>remove pytz dependency (43079ea)</li> <li>rename redis_url to url in redis settings (2c998f4)</li> </ul>"},{"location":"changelog/#removal","title":"\u274c Removal","text":"<ul> <li>[breaking] drop python v3.9 support (d499876)</li> </ul>"},{"location":"changelog/#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Add overview page (6ad5c89)</li> </ul>"},{"location":"changelog/#v0010-2025-07-13","title":"v0.0.10 - 2025-07-13","text":"<p>Compare with v0.0.9</p>"},{"location":"changelog/#features_1","title":"\ud83d\ude80 Features","text":"<ul> <li>add public_prefix(and permanent_link) to libcloud (3bc7591)</li> <li>static_uuid transformer (88383e0)</li> <li>location transformers receive optional upload-or-data second argument (8e6a6dc)</li> </ul>"},{"location":"changelog/#bug-fixes_1","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>fix_extension transformer raises an error when upload is missing (a827df5)</li> </ul>"},{"location":"changelog/#removal_1","title":"\u274c Removal","text":"<ul> <li>delete Storage.public_link (da08744)</li> </ul>"},{"location":"changelog/#v009-2025-07-02","title":"v0.0.9 - 2025-07-02","text":"<p>Compare with v0.0.8</p>"},{"location":"changelog/#features_2","title":"\ud83d\ude80 Features","text":"<ul> <li>add fix_extension transformer (1345915)</li> <li>opendal got path option (d044ade)</li> </ul>"},{"location":"changelog/#bug-fixes_2","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>cast fs:multipart:position to int (dc4d768)</li> </ul>"},{"location":"changelog/#v008-2025-04-23","title":"v0.0.8 - 2025-04-23","text":"<p>Compare with v0.0.7</p>"},{"location":"changelog/#features_3","title":"\ud83d\ude80 Features","text":"<ul> <li>libcloud storage got path option (555036c)</li> </ul>"},{"location":"changelog/#bug-fixes_3","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>fs storage reports relative location of the missing file (cef9589)</li> </ul>"},{"location":"changelog/#v007-2025-03-28","title":"v0.0.7 - 2025-03-28","text":"<p>Compare with v0.0.6</p>"},{"location":"changelog/#features_4","title":"\ud83d\ude80 Features","text":"<ul> <li>storage upload and append requires Upload (ebf5fef)</li> </ul>"},{"location":"changelog/#bug-fixes_4","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>fs storage checks permission when creating folders (1791d68)</li> </ul>"},{"location":"changelog/#v006-2025-03-23","title":"v0.0.6 - 2025-03-23","text":"<p>Compare with v0.0.4</p>"},{"location":"changelog/#features_5","title":"\ud83d\ude80 Features","text":"<ul> <li>add *_synthetic methods (27c4164)</li> </ul>"},{"location":"changelog/#bug-fixes_5","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>multipart update of s3 and gcs work with arbitrary upload (902c9cb)</li> </ul>"},{"location":"changelog/#v004-2025-03-22","title":"v0.0.4 - 2025-03-22","text":"<p>Compare with v0.0.3</p>"},{"location":"changelog/#features_6","title":"\ud83d\ude80 Features","text":"<ul> <li>storages accept any type of upload (a64ee3d)</li> </ul>"},{"location":"changelog/#refactor_1","title":"\ud83d\ude9c Refactor","text":"<ul> <li>remove validation from storage (93392f9)</li> <li>remove type and size validation from append and compose (890c89a)</li> <li>remove public link method and capability (cb39151)</li> </ul>"},{"location":"changelog/#removal_2","title":"\u274c Removal","text":"<ul> <li>drop link storage (3328eb2)</li> </ul>"},{"location":"changelog/#v002-2025-03-17","title":"v0.0.2 - 2025-03-17","text":"<p>Compare with v0.0.1</p>"},{"location":"changelog/#features_7","title":"\ud83d\ude80 Features","text":"<ul> <li>stream-based composite implementation of range (7d47bd8)</li> <li>add Location wrapper around unsafe path parameters (b99f155)</li> <li>file_keeper:opendal adapter (214fb6c)</li> <li>file_keeper:redis adapter (8c7da94)</li> </ul>"},{"location":"changelog/#bug-fixes_6","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>map error during settings initialization into custom exception (f596037)</li> <li>fs adapter: infer <code>uploaded</code> size if it is not specified in <code>multipart_update</code> (620ec3a)</li> </ul>"},{"location":"changelog/#refactor_2","title":"\ud83d\ude9c Refactor","text":"<ul> <li><code>location_strategy: str</code> become <code>location_transformers: list[str]</code> (daf2dc6)</li> <li>remove default range implementation from reader (36f5f31)</li> </ul>"},{"location":"changelog/#testing","title":"\ud83e\uddea Testing","text":"<ul> <li>add standard adapter tests (d961682)</li> </ul>"},{"location":"changelog/#v001-2025-03-13","title":"v0.0.1 - 2025-03-13","text":""},{"location":"configuration/","title":"Configuration","text":"<p>Behavior of every storage is configurable through the Settings class. This page details the available settings and how to use them to customize your storage setup.</p>"},{"location":"configuration/#the-settings-class","title":"The Settings Class","text":"<p>The Settings class is the central point for configuring each storage adapter. It defines the options that control how the adapter interacts with the underlying storage.  Each adapter has its own subclass of Settings that adds adapter-specific options.</p> <p>While you can directly instantiate a Settings subclass with its arguments, the most common approach is to pass a dictionary of options to the storage adapter constructor. file-keeper automatically handles the transformation of this dictionary into a Settings object. This provides a more flexible and user-friendly configuration experience.</p> <p>Example</p> <p>Let's say you want to configure the S3 adapter. Instead of creating a Settings object directly, you can simply pass a dictionary like this:</p> <pre><code>from file_keeper import make_storage\n\ns3_settings = {\n    \"type\": \"file_keeper:s3\",\n    \"bucket\": \"my-s3-bucket\",\n    \"key\": \"YOUR_AWS_ACCESS_KEY_ID\",\n    \"secret\": \"YOUR_AWS_SECRET_ACCESS_KEY\",\n    \"region\": \"us-east-1\",\n}\n\nstorage = make_storage(\"my_s3_storage\", s3_settings)\n\n# Accessing the settings (for demonstration)\nprint(storage.settings.bucket) # (1)!\n</code></pre> <ol> <li>Output: my-s3-bucket</li> </ol> <p>In this example, <code>make_storage</code> automatically creates a <code>Settings</code> object from the <code>s3_settings</code> dictionary.</p>"},{"location":"configuration/#common-settings","title":"Common Settings","text":"<p>These settings are available for most storage adapters:</p> Setting Type Default Description <code>name</code> str <code>\"unknown\"</code> Descriptive name of the storage used for debugging. <code>override_existing</code> bool <code>False</code> If <code>True</code>, existing files will be overwritten during upload. If <code>False</code>, an <code>ExistingFileError</code> will be raised if a file with the same location already exists. <code>path</code> str <code>\"\"</code> The base path or directory where files are stored. The exact meaning depends on the adapter (e.g., a path in the filesystem for the FS adapter, a prefix-path inside the bucket for S3).  Required for most adapters. <code>location_transformers</code> list[str] <code>[]</code> A list of names of location transformers to apply to file locations. These transformers can be used to sanitize or modify file paths before they are used to store files. See the Extending file-keeper documentation for details. <code>disabled_capabilities</code> list[str] <code>[]</code> A list of capabilities to disable for the storage adapter. This can be useful for limiting the functionality of an adapter or for testing purposes. <code>initialize</code> bool <code>False</code> Prepare storage backend for uploads. The exact meaning depends on the adapter. Filesystem adapter created the upload folder if it's missing; cloud adapters create a bucket/container if it does not exists. <p>Important Considerations</p> Security Be careful when storing sensitive information like AWS access keys and secret keys in your configuration.  Consider using environment variables or a secure configuration management system. Validation file-keeper performs some basic validation of the configuration settings, but it's important to ensure that your settings are correct for your specific storage adapter. Error Handling Be prepared to handle <code>InvalidStorageConfigurationError</code> exceptions if your configuration is invalid."},{"location":"configuration/#adapter-specific-settings","title":"Adapter-Specific Settings","text":"<p>In addition to the common settings, adapters have their own specific settings:</p>"},{"location":"configuration/#file_keeperazure_blob","title":"<code>file_keeper:azure_blob</code>","text":"Setting Type Default Description <code>account_name</code> str <code>\"\"</code> Name of the account. <code>account_key</code> str <code>\"\"</code> Key for the account. <code>container_name</code> str <code>\"\"</code> Name of the storage container. <code>account_url</code> str <code>\"https://{account_name}.blob.core.windows.net\"</code> Custom resource URL. <code>client</code> BlobServiceClient <code>None</code> Existing storage client. <code>container</code> ContainerClient <code>None</code> Existing container client. <code>path</code> str <code>\"\"</code> Prefix for the file location. <code>initialize</code> bool <code>False</code> Create <code>container_name</code> if it does not exist."},{"location":"configuration/#file_keeperfs","title":"<code>file_keeper:fs</code>","text":"Setting Type Default Description <code>path</code> str <code>\"\"</code> The base directory where files are stored. <code>initialize</code> bool <code>False</code> Create <code>path</code> if it does not exist."},{"location":"configuration/#file_keepergcs","title":"<code>file_keeper:gcs</code>","text":"Setting Type Default Description <code>bucket_name</code> str <code>\"\"</code> Name of the storage bucket. <code>client</code> Client <code>None</code> Existing storage client. <code>bucket</code> Bucket <code>None</code> Existing storage bucket. <code>credentials</code> Credentials <code>None</code> Existing cloud credentials. <code>credentials_file</code> str <code>\"\"</code> Path to the JSON with cloud credentials. <code>project_id</code> str <code>\"\"</code> The project which the client acts on behalf of. <code>client_options</code> dict <code>None</code> Client options for storage client. <code>path</code> str <code>\"\"</code> Prefix for the file location. <code>initialize</code> bool <code>False</code> Create <code>bucket_name</code> if it does not exist."},{"location":"configuration/#file_keeperlibcloud","title":"<code>file_keeper:libcloud</code>","text":"Setting Type Default Description <code>provider</code> str <code>\"\"</code> Name of the Libcloud provider <code>key</code> str <code>\"\"</code> Access key of the cloud account. <code>secret</code> str or None <code>None</code> Secret key of the cloud account. <code>params</code> dict <code>{}</code> Additional parameters for cloud provider. <code>container_name</code> str <code>\"\"</code> Name of the cloud container. <code>public_prefix</code> str <code>\"\"</code> Root URL for containers with public access. This URL will be used as a prefix for the file object location when building permanent links. <code>driver</code> StorageDriver <code>None</code> Existing storage driver. <code>container</code> Container <code>None</code> Existing container object. <code>path</code> str <code>\"\"</code> Prefix for the file location. <code>initialize</code> bool <code>False</code> Create <code>container_name</code> if it does not exist."},{"location":"configuration/#file_keepermemory","title":"<code>file_keeper:memory</code>","text":"Setting Type Default Description <code>bucket</code> MutableMapping[str, bytes] <code>{}</code> Container for uploaded objects."},{"location":"configuration/#file_keepernull","title":"<code>file_keeper:null</code>","text":"<p>No specific settings</p>"},{"location":"configuration/#file_keeperopendal","title":"<code>file_keeper:opendal</code>","text":"Setting Type Default Description <code>operator</code> opendal.Operator <code>None</code> Existing OpenDAL operator <code>scheme</code> str <code>\"\"</code> Name of OpenDAL operator's scheme. <code>params</code> dict <code>{}</code> Parameters for OpenDAL operator initialization. <code>path</code> str <code>\"\"</code> Prefix for the file location."},{"location":"configuration/#file_keeperredis","title":"<code>file_keeper:redis</code>","text":"Setting Type Default Description <code>redis</code> redis.Redis <code>None</code> Optional existing connection to Redis DB <code>url</code> str <code>\"\"</code> URL of the Redis DB. <code>bucket</code> str <code>\"\"</code> Key of the Redis HASH for uploaded objects."},{"location":"configuration/#file_keepers3","title":"<code>file_keeper:s3</code>","text":"Setting Type Default Description <code>bucket</code> str <code>\"\"</code> Name of the storage bucket. <code>client</code> S3Client <code>None\"</code> Existing S3 client. <code>key</code> str <code>None</code> The AWS Access Key. <code>secret</code> str <code>None</code> The AWS Secret Key. <code>region</code> str <code>None</code> The AWS Region of the bucket. <code>endpoint</code> str <code>None</code> Custom AWS endpoint. <code>path</code> str <code>\"\"</code> Prefix for the file location. <code>initialize</code> bool <code>False</code> Create <code>bucket</code> if it does not exist."},{"location":"configuration/#file_keepersqlalchemy","title":"<code>file_keeper:sqlalchemy</code>","text":"Setting Type Default Description <code>db_url</code> str <code>\"\"</code> URL of the storage DB. <code>table_name</code> str <code>\"\"</code> Name of the storage table. <code>location_column</code> str <code>\"\"</code> Name of the column that contains file location. <code>content_column</code> str <code>\"\"</code> Name of the column that contains file content. <code>engine</code> Engine <code>None</code> Existing DB engine. <code>table</code> Engine <code>None</code> Existing DB table. <code>location</code> Engine <code>None</code> Existing column for location. <code>content</code> Engine <code>None</code> Existing column for content. <code>initialize</code> bool <code>False</code> Create <code>table_name</code> if it does not exist."},{"location":"configuration/#file_keeperzip","title":"<code>file_keeper:zip</code>","text":"Setting Type Default Description <code>path</code> str <code>\"\"</code> Path of the ZIP archive for uploaded objects."},{"location":"error_handling/","title":"Error handling","text":"<p>file-keeper provides a comprehensive set of exceptions to help you handle errors gracefully. This page documents the available exceptions and provides guidance on how to handle them.</p>"},{"location":"error_handling/#general-exception-hierarchy","title":"General exception hierarchy","text":"<p>All exceptions in file-keeper inherit from the base <code>FilesError</code> exception.  This allows you to catch all file-keeper related errors with a single <code>except</code> block.  More specific exceptions are derived from FilesError to provide more detailed error information.</p> <p>Note</p> <p>file-keeper's exceptions can be imported from <code>file_keeper.core.exceptions</code></p> <pre><code>from file_keeper.core.exceptions import FilesError\n\ntry:\n    ...\nexcept FilesError:\n    ...\n</code></pre> <p>As a shortcut, they can be accessed from the <code>exc</code> object available at the root of file-keeper module</p> <pre><code>from file_keeper import exc\n\ntry:\n    ...\nexcept exc.FilesError:\n    ...\n</code></pre>"},{"location":"error_handling/#example-error-handling","title":"Example error handling","text":"<pre><code>from file_keeper import make_storage, make_upload, exc\n\ntry:\n    storage = make_storage(\"my_storage\", {\"type\": \"file_keeper:fs\", \"path\": \"/nonexistent/path\"})\nexcept exc.InvalidStorageConfigurationError as e:\n    print(f\"Error configuring storage: {e}\")\n\nupload = make_upload(b\"Hello, file-keeper!\")\n\ntry:\n    storage.upload(\"my_file.txt\", upload)\nexcept exc.ExistingFileError as e:\n    print(f\"File already exists: {e}\")\n</code></pre>"},{"location":"error_handling/#file_keeper.exc","title":"<code>exc</code>","text":"<p>Exception definitions for the extension.</p> <p>Hierarchy:</p> <ul> <li>Exception<ul> <li>FilesError<ul> <li>StorageError<ul> <li>UnknownAdapterError</li> <li>UnknownStorageError</li> <li>UnsupportedOperationError</li> <li>PermissionError</li> <li>LocationError<ul> <li>MissingFileError</li> <li>ExistingFileError</li> </ul> </li> <li>ExtrasError<ul> <li>MissingExtrasError</li> </ul> </li> <li>InvalidStorageConfigurationError<ul> <li>MissingStorageConfigurationError</li> </ul> </li> <li>UploadError<ul> <li>WrongUploadTypeError</li> <li>LocationTransformerError</li> <li>ContentError</li> <li>LargeUploadError<ul> <li>UploadOutOfBoundError</li> </ul> </li> <li>UploadMismatchError<ul> <li>UploadTypeMismatchError</li> <li>UploadHashMismatchError</li> <li>UploadSizeMismatchError</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"error_handling/#file_keeper.exc.ContentError","title":"<code>ContentError(storage, msg)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Storage cannot accept uploaded content.</p>"},{"location":"error_handling/#file_keeper.exc.ExistingFileError","title":"<code>ExistingFileError(storage, location)</code>","text":"<p>               Bases: <code>LocationError</code></p> <p>File already exists.</p>"},{"location":"error_handling/#file_keeper.exc.ExtrasError","title":"<code>ExtrasError(problem)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Wrong extras passed during upload.</p>"},{"location":"error_handling/#file_keeper.exc.FilesError","title":"<code>FilesError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base error for catch-all scenario.</p>"},{"location":"error_handling/#file_keeper.exc.InvalidStorageConfigurationError","title":"<code>InvalidStorageConfigurationError(adapter_or_storage, problem)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Storage cannot be initialized with given configuration.</p>"},{"location":"error_handling/#file_keeper.exc.LargeUploadError","title":"<code>LargeUploadError(actual_size, max_size)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Storage cannot be initialized due to missing option.</p>"},{"location":"error_handling/#file_keeper.exc.LocationError","title":"<code>LocationError(storage, location)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Storage cannot use given location.</p>"},{"location":"error_handling/#file_keeper.exc.LocationTransformerError","title":"<code>LocationTransformerError(transformer)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Undefined location transformer.</p>"},{"location":"error_handling/#file_keeper.exc.MissingExtrasError","title":"<code>MissingExtrasError(key)</code>","text":"<p>               Bases: <code>ExtrasError</code></p> <p>Wrong extras passed to storage method.</p>"},{"location":"error_handling/#file_keeper.exc.MissingFileError","title":"<code>MissingFileError(storage, location)</code>","text":"<p>               Bases: <code>LocationError</code></p> <p>File does not exist.</p>"},{"location":"error_handling/#file_keeper.exc.MissingStorageConfigurationError","title":"<code>MissingStorageConfigurationError(adapter_or_storage, option)</code>","text":"<p>               Bases: <code>InvalidStorageConfigurationError</code></p> <p>Storage cannot be initialized due to missing option.</p>"},{"location":"error_handling/#file_keeper.exc.PermissionError","title":"<code>PermissionError(storage, operation, problem)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Storage client does not have required permissions.</p>"},{"location":"error_handling/#file_keeper.exc.StorageError","title":"<code>StorageError</code>","text":"<p>               Bases: <code>FilesError</code></p> <p>Error related to storage.</p>"},{"location":"error_handling/#file_keeper.exc.UnknownAdapterError","title":"<code>UnknownAdapterError(adapter)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Specified storage adapter is not registered.</p>"},{"location":"error_handling/#file_keeper.exc.UnknownStorageError","title":"<code>UnknownStorageError(storage)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Storage with the given name is not configured.</p>"},{"location":"error_handling/#file_keeper.exc.UnsupportedOperationError","title":"<code>UnsupportedOperationError(operation, storage)</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Requested operation is not supported by storage.</p>"},{"location":"error_handling/#file_keeper.exc.UploadError","title":"<code>UploadError</code>","text":"<p>               Bases: <code>StorageError</code></p> <p>Error related to file upload process.</p>"},{"location":"error_handling/#file_keeper.exc.UploadHashMismatchError","title":"<code>UploadHashMismatchError(actual, expected)</code>","text":"<p>               Bases: <code>UploadMismatchError</code></p> <p>Expected value of hash match the actual value.</p>"},{"location":"error_handling/#file_keeper.exc.UploadMismatchError","title":"<code>UploadMismatchError(attribute, actual, expected)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Expected value of file attribute doesn't match the actual value.</p>"},{"location":"error_handling/#file_keeper.exc.UploadOutOfBoundError","title":"<code>UploadOutOfBoundError(actual_size, max_size)</code>","text":"<p>               Bases: <code>LargeUploadError</code></p> <p>Ongoing upload exceeds expected size.</p>"},{"location":"error_handling/#file_keeper.exc.UploadSizeMismatchError","title":"<code>UploadSizeMismatchError(actual, expected)</code>","text":"<p>               Bases: <code>UploadMismatchError</code></p> <p>Expected value of upload size doesn't match the actual value.</p>"},{"location":"error_handling/#file_keeper.exc.UploadTypeMismatchError","title":"<code>UploadTypeMismatchError(actual, expected)</code>","text":"<p>               Bases: <code>UploadMismatchError</code></p> <p>Expected value of content type doesn't match the actual value.</p>"},{"location":"error_handling/#file_keeper.exc.WrongUploadTypeError","title":"<code>WrongUploadTypeError(content_type)</code>","text":"<p>               Bases: <code>UploadError</code></p> <p>Storage does not support given MIMEType.</p>"},{"location":"testing/","title":"Testing","text":"<p>This document outlines the recommended approach for writing tests for new storage adapters within the <code>file_keeper</code> project.  The goal is to ensure consistent behavior and adherence to the defined capabilities.</p>"},{"location":"testing/#core-principles","title":"Core Principles","text":"<p>Tests should primarily focus on verifying that an adapter correctly implements the capabilities. Validate the behavior of the adapter, not necessarily the internal implementation details. User has certain expectations depending on the storage capabilities and assumes that two different storage adapters with same capabilities behave identically in similar conditions. That's why testing publicly exposed interface is the highest priority.</p>"},{"location":"testing/#the-standard-class-and-inheritance","title":"The <code>Standard</code> Class and Inheritance","text":"<p>The <code>tests/file_keeper/default/adapters/standard.py</code> file provides a <code>Standard</code> class that serves as a foundation for testing adapters. This class encapsulates a comprehensive suite of tests covering various storage capabilities.</p> <p>Tip</p> <p>When creating tests for a new adapter, inherit from the <code>Standard</code> class. This automatically provides a significant amount of pre-built test coverage.  You only need to override or add tests to address specific adapter behavior or unique capabilities.</p> <p>The <code>Standard</code> class defines a series of test methods (e.g., <code>test_append_content</code>, <code>test_remove_real</code>, <code>test_move_missing</code>). Each test specifies capabilities required for verification and is automatically skipped if given capabilities are not suppored by the storage.</p> <p>If all the required capabilities are supported, test verifies that implementation of capability is predictable. For example:</p> <ul> <li>REMOVE: returns <code>True</code> when removing real file and <code>False</code> if file does not exists</li> <li>CREATE: raises ExistingFileError if file   already exists at the given location and   override_existing option is not   enabled</li> <li>STREAM: output of stream() and   content() is the same</li> <li>ANALYZE: content hash of the file is the same as one, computed during the upload</li> </ul> <p>Note</p> <p><code>Standard</code> class covers only adapter-agnostic functionality and it does not verify interal processes of the storage.</p> <p>Testing that filesystem storage actually writes data into the configured directory and cloud storage communicates with the cloud provider is still required. But these tests must be implemented individually for each provider and they are not included into generic <code>Standard</code> class.</p>"},{"location":"testing/#writing-new-tests","title":"Writing New Tests","text":""},{"location":"testing/#create-a-test-class","title":"Create a Test Class","text":"<p>Create a new test class that inherits from <code>Standard</code>.</p> <pre><code>from tests.file_keeper.default.adapters.standard import Standard\nimport pytest\n\nclass MyStorageAdapterTests(Standard):\n    # Your tests here\n    pass\n</code></pre>"},{"location":"testing/#override-existing-tests-if-necessary","title":"Override Existing Tests (if necessary)","text":"<p>If your adapter has specific behavior in addition to the default implementation, override the corresponding test method in the <code>Standard</code> class. Be sure to call <code>super()</code> to execute the original test logic first, and then add your custom assertions.</p> <pre><code>class MyStorageAdapterTests(Standard):\n    def test_create_content_non_modified(self, storage: fk.Storage, faker: Faker):\n        super().test_create_content_non_modified(storage, faker)\n        # Add custom assertions specific to your adapter\n        assert \"some_adapter_specific_check\" in storage.content(result)\n</code></pre>"},{"location":"testing/#add-new-tests","title":"Add New Tests","text":"<p>If your adapter supports new capabilities or has unique behavior that is not covered by the <code>Standard</code> class, add new test methods to your test class.</p> <pre><code>class MyStorageAdapterTests(Standard):\n\n    def test_my_custom_behavior(self, storage: fk.Storage, faker: Faker):\n        # Your test logic here\n        assert True\n</code></pre>"},{"location":"adapters/azure_blob/","title":"Azure Blob Storage","text":"<p>The <code>file_keeper:azure_blob</code> adapter allows you to use Microsoft Azure Blob Storage for storing and retrieving files. This adapter leverages the <code>azure-storage-blob</code> Python library.</p>"},{"location":"adapters/azure_blob/#overview","title":"Overview","text":"<p>This adapter provides a convenient way to integrate Azure Blob Storage with file-keeper. You'll need to have the <code>azure-storage-blob</code> library installed and configure it with the appropriate credentials for your Azure account.</p> <pre><code>pip install 'file-keeper[azure]'\n\n## or\n\npip install azure-storage-blob\n</code></pre>"},{"location":"adapters/azure_blob/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the Azure Blob Storage adapter:</p> Azure Blob StorageAzurite <pre><code>storage = make_storage(\"my_azure_storage\", {\n    \"type\": \"file_keeper:azure_blob\",\n    \"account_name\": \"***\",\n    \"account_key\": \"***\",\n    \"container_name\": \"file-keeper\",\n    \"initialize\": True,\n})\n</code></pre> <pre><code>storage = make_storage(\"my_azure_storage\", {\n    \"type\": \"file_keeper:azure_blob\",\n    \"account_name\": \"devstoreaccount1\",\n    \"account_key\": \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n    \"container_name\": \"file-keeper\",\n    \"initialize\": True,\n    \"account_url\": \"http://127.0.0.1:10000/{account_name}\",\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values with your actual Azure account credentials     and configuration.</li> <li>Ensure that you have created a container in your Azure Blob Storage account     to store the files.</li> <li>For enhanced security, consider using Azure Active Directory (Azure AD)     authentication instead of account keys.  Refer to the <code>azure-storage-blob</code>     documentation for details on Azure AD authentication.</li> <li>Refer to the Azure Blob Storage     documentation     for more information about Azure Blob Storage.</li> </ul>"},{"location":"adapters/emulate/","title":"Emulate cloud providers with Docker","text":"<p>For local development and testing, you can emulate cloud providers using Docker containers. This allows you to test your file-keeper integrations without incurring costs or requiring access to real cloud resources.</p> <p>Remember to adjust the port mappings and environment variables as needed for your specific setup.</p> MinIO (S3-compatible)Azurite (Azure Blob Storage)Fake GCS Server (Google Cloud Storage) <pre><code>docker run -d -p 9000:9000 -p 9001:9001 \\\n    --name minio \\\n    -e MINIO_PUBLIC_ADDRESS=0.0.0.0:9000 \\\n    quay.io/minio/minio server /data --console-address \":9001\"\n</code></pre> Attribute Value Endpoint http://127.0.0.1:9000 Key <code>minioadmin</code> Secret <code>minioadmin</code> <pre><code>docker run -d -p 10000:10000 \\\n    --name azurite-blob \\\n    mcr.microsoft.com/azure-storage/azurite azurite-blob --blobHost 0.0.0.0\n</code></pre> Attribute Value Endpoint http://127.0.0.1:10000 Key <code>devstoreaccount1</code> Secret <code>Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==</code> <pre><code>docker run -d -p 4443:4443 \\\n     --name gcs \\\n     fsouza/fake-gcs-server -scheme http\n</code></pre> Attribute Value Endpoint http://127.0.0.1:4443 Key Not requred Secret Not required"},{"location":"adapters/fs/","title":"Local Filesystem Adapter","text":"<p>The <code>file_keeper:fs</code> adapter allows you to use your local filesystem for storing and retrieving files. This adapter is useful for testing, development, or scenarios where you need to store files locally.</p>"},{"location":"adapters/fs/#overview","title":"Overview","text":"<p>This adapter provides a simple way to interact with the local filesystem. You'll need to specify the base path where files will be stored.</p>"},{"location":"adapters/fs/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the local filesystem adapter:</p> <pre><code>storage = make_storage(\"my_local_storage\", {\n    \"type\": \"file_keeper:fs\",\n    \"path\": \"/tmp/file-keeper\",\n    \"initialize\": True,\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace <code>/tmp/file-keeper</code> with the desired base path on your system.</li> <li>The <code>initialize</code> option (defaulting to <code>False</code>) determines whether the     adapter should attempt to create the specified directory if it doesn't     exist. If <code>initialize</code> is <code>True</code> and the directory cannot be created (e.g.,     due to permissions issues), an error will be raised.</li> <li>Ensure that the process running file-keeper has the necessary permissions     to read and write to the specified directory.</li> </ul>"},{"location":"adapters/gcs/","title":"Google Cloud Storage","text":"<p>The <code>file_keeper:gcs</code> adapter allows you to use Google Cloud Storage for storing and retrieving files. This adapter leverages the <code>google-cloud-storage</code> Python library.</p>"},{"location":"adapters/gcs/#overview","title":"Overview","text":"<p>This adapter provides a convenient way to integrate Google Cloud Storage with file-keeper. You'll need to have the <code>google-cloud-storage</code> library installed and configure it with the appropriate credentials for your Google Cloud project.</p> <pre><code>pip install 'file-keeper[gcs]'\n\n## or\n\npip install google-cloud-storage\n</code></pre>"},{"location":"adapters/gcs/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the Google Cloud Storage adapter:</p> Google Cloud StorageFake GCS <pre><code>storage = make_storage(\"my_gcs_storage\", {\n    \"type\": \"file_keeper:gcs\",\n    \"project_id\": \"file-keeper\",  # Replace with your Google Cloud project ID\n    \"bucket_name\": \"file-keeper\",\n    \"initialize\": True,\n    \"credentials_file\": \"/path/to/your/credentials.json\",  # Replace with the path to your service account key file\n})\n</code></pre> <pre><code>storage = make_storage(\"my_gcs_storage\", {\n    \"type\": \"file_keeper:gcs\",\n    \"project_id\": \"file-keeper\",\n    \"bucket_name\": \"file-keeper\",\n    \"initialize\": True,\n    \"client_options\": {\"api_endpoint\": \"http://127.0.0.1:4443\"},\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values with your actual Google Cloud project ID,     GCS bucket name, and the path to your service account key file.</li> <li>Ensure that you have created a bucket in your Google Cloud Storage account     to store the files.</li> <li>For enhanced security, it's recommended to use a service account with     limited permissions.</li> <li>Refer to the Google Cloud Storage     documentation for more information     about Google Cloud Storage.</li> </ul>"},{"location":"adapters/libcloud/","title":"Apache Libcloud Adapter","text":"<p>The <code>file_keeper:libcloud</code> adapter allows you to use Apache Libcloud to connect to a wide range of cloud storage providers. Libcloud provides a unified interface for interacting with various storage services, simplifying integration and reducing vendor lock-in.</p>"},{"location":"adapters/libcloud/#overview","title":"Overview","text":"<p>This adapter leverages the Libcloud library to provide storage functionality. You'll need to have Libcloud installed and configure it with the appropriate credentials for your chosen provider.</p> <pre><code>pip install 'file-keeper[libcloud]'\n\n## or\n\npip install apache-libcloud\n</code></pre>"},{"location":"adapters/libcloud/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the Libcloud adapter:</p> AWS S3MinIOAzurite <pre><code>storage = make_storage(\"my_libcloud_storage\", {\n    \"type\": \"file_keeper:libcloud\",\n    \"provider\": \"S3\",\n    \"key\": \"***\",\n    \"secret\": \"***\",\n    \"container_name\": \"file-keeper\",\n    \"initialize\": True,\n})\n</code></pre> <pre><code>storage = make_storage(\"my_libcloud_storage\", {\n    \"type\": \"file_keeper:libcloud\",\n    \"provider\": \"MINIO\",\n    \"key\": \"minioadmin\",\n    \"secret\": \"minioadmin\",\n    \"container_name\": \"file-keeper\",\n    \"initialize\": True,\n    \"params\": {\n        \"host\": \"127.0.0.1\",\n        \"port\": 9000,\n        \"secure\": False,\n    },\n})\n</code></pre> <pre><code>storage = make_storage(\"my_libcloud_storage\", {\n    \"type\": \"file_keeper:libcloud\",\n    \"provider\": \"AZURE_BLOBS\",\n    \"key\": \"devstoreaccount1\",\n    \"secret\": \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n    \"container_name\": \"file-keeper\",\n    \"initialize\": True,\n    \"params\": {\n        \"host\": \"127.0.0.1\",\n        \"port\": 10000,\n        \"secure\": False,\n    },\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values (provider, host, port, secure, key, secret,     container_name) with your actual provider credentials and configuration.</li> <li>Refer to the Apache Libcloud     documentation for a complete list of     supported providers and their specific configuration options.</li> <li>Ensure that the necessary Libcloud drivers are installed for your chosen     provider.</li> </ul>"},{"location":"adapters/memory/","title":"In-Memory Storage Adapter","text":"<p>The <code>file_keeper:memory</code> adapter provides a simple in-memory storage solution. This adapter is primarily intended for testing and development purposes, as data is not persisted to disk.</p>"},{"location":"adapters/memory/#overview","title":"Overview","text":"<p>This adapter stores files entirely in memory, making it very fast but also volatile. Data is lost when the application exits. It's a convenient way to simulate a storage backend without requiring external dependencies.</p>"},{"location":"adapters/memory/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the in-memory storage adapter:</p> <pre><code>storage = make_storage(\"my_memory_storage\", {\n    \"type\": \"file_keeper:memory\",\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>No configuration options are required for this adapter.</li> <li>All data is stored in memory and will be lost when the application     terminates.</li> <li>This adapter is not suitable for production environments where data     persistence is required.</li> </ul>"},{"location":"adapters/null/","title":"Null Adapter","text":"<p>The <code>file_keeper:null</code> adapter provides a no-op storage solution. It does not actually store any files and is primarily intended for testing or situations where you want to disable storage functionality.</p>"},{"location":"adapters/null/#overview","title":"Overview","text":"<p>This adapter implements all the required storage interfaces but performs no real operations. Any attempt to upload, download, or manage files will be silently ignored. It's a useful tool for isolating issues or running tests without interacting with external storage.</p>"},{"location":"adapters/null/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the null adapter:</p> <pre><code>storage = make_storage(\"my_null_storage\", {\n    \"type\": \"file_keeper:null\",\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>No configuration options are required for this adapter.</li> <li>All operations are silently ignored.</li> <li>This adapter is not suitable for production environments where data     persistence is required.</li> </ul>"},{"location":"adapters/opendal/","title":"Apache OpenDAL","text":"<p>The <code>file_keeper:opendal</code> adapter allows you to use Apache OpenDAL for storing and retrieving files. OpenDAL provides a unified interface for interacting with various object storage systems and local filesystems.</p>"},{"location":"adapters/opendal/#overview","title":"Overview","text":"<p>This adapter leverages the OpenDAL library to provide storage functionality. You'll need to have OpenDAL installed and configure it with the appropriate credentials and configuration for your chosen storage provider.</p> <pre><code>pip install 'file-keeper[opendal]'\n\n## or\n\npip install opendal\n</code></pre>"},{"location":"adapters/opendal/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the OpenDAL adapter:</p> MinIOAzuriteFake GCS <pre><code>storage = make_storage(\"my_opendal_storage\", {\n     \"type\": \"file_keeper:opendal\",\n     \"scheme\": \"s3\",\n     \"params\": {\n         \"bucket\": \"file-keeper\",\n         \"access_key_id\": \"minioadmin\",\n         \"secret_access_key\": \"minioadmin\",\n         \"endpoint\": \"http://127.0.0.1:9000\",\n         \"region\": \"auto\"\n     },\n })\n</code></pre> <pre><code>storage = make_storage(\"my_opendal_storage\", {\n     \"type\": \"file_keeper:opendal\",\n     \"scheme\": \"azblob\",\n     \"params\": {\n         \"container\": \"file-keeper\",\n         \"account_name\": \"devstoreaccount1\",\n         \"account_key\": \"Eby8vdM02xNOcqFlqUwJPLlmEtlCDXJ1OUzFT50uSRZ6IFsuFq2UVErCz4I6tq/K1SZFPTOtr/KBHBeksoGMGw==\",\n         \"endpoint\": \"http://127.0.0.1:10000/devstoreaccount1\",\n     },\n })\n</code></pre> <pre><code>storage = make_storage(\"my_opendal_storage\", {\n     \"type\": \"file_keeper:opendal\",\n     \"scheme\": \"gcs\",\n     \"params\": {\n         \"bucket\": \"file-keeper\",\n         \"endpoint\": \"http://127.0.0.1:4443\",\n     },\n })\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the params with your actual OpenDAL location and credentials.</li> <li>Refer to the Apache OpenDAL     documentation for a complete list of     supported storage systems and their specific configuration options.</li> <li>Ensure that you have the necessary credentials and permissions to access     the specified storage location.</li> </ul>"},{"location":"adapters/redis/","title":"Redis Adapter","text":"<p>The <code>file_keeper:redis</code> adapter allows you to use Redis as a storage backend. This adapter stores files as binary data within Redis HASH.</p>"},{"location":"adapters/redis/#overview","title":"Overview","text":"<p>This adapter provides a simple way to integrate Redis with file-keeper. You'll need to have a running Redis instance and the <code>redis</code> Python library installed. This adapter is suitable for smaller files and scenarios where fast access is critical.</p> <pre><code>pip install 'file-keeper[redis]'\n\n## or\n\npip install redis\n</code></pre>"},{"location":"adapters/redis/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the Redis adapter:</p> <pre><code>storage = make_storage(\"my_redis_storage\", {\n    \"type\": \"file_keeper:redis\",\n    \"host\": \"redis://localhost:6379/0\",\n    \"bucket\": \"file-keeper\",\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values with your actual Redis configuration.</li> <li>Ensure that your Redis instance is running and accessible.</li> <li>Consider the limitations of storing large files in Redis, as it is an     in-memory data store.</li> <li>The <code>bucket</code> parameter is used as name of the Redis HASH key under which     files will be stored.</li> </ul>"},{"location":"adapters/s3/","title":"AWS S3 Adapter","text":"<p>The <code>file_keeper:s3</code> adapter allows you to use Amazon S3 for storing and retrieving files. This adapter leverages the <code>boto3</code> Python library.</p>"},{"location":"adapters/s3/#overview","title":"Overview","text":"<p>This adapter provides a convenient way to integrate AWS S3 with file-keeper. You'll need to have the <code>boto3</code> library installed and configure it with the appropriate credentials for your AWS account.</p> <pre><code>pip install 'file-keeper[s3]'\n\n## or\n\npip install boto3\n</code></pre>"},{"location":"adapters/s3/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the S3 adapter:</p> AWS S3MinIO <pre><code>storage = make_storage(\"my_s3_storage\", {\n    \"type\": \"file_keeper:s3\",\n    \"key\": \"***\",\n    \"secret\": \"***\",\n    \"bucket\": \"file-keeper\",\n    \"initialize\": True,\n})\n</code></pre> <pre><code>storage = make_storage(\"my_s3_storage\", {\n    \"type\": \"file_keeper:s3\",\n    \"key\": \"minioadmin\",\n    \"secret\": \"minioadmin\",\n    \"bucket\": \"file-keeper\",\n    \"endpoint\": \"http://127.0.0.1:9000/\",\n    \"initialize\": True,\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values (bucket, region, access_key_id,     secret_access_key) with your actual AWS account credentials and     configuration.</li> <li>Ensure that you have created a bucket in your AWS S3 account to store the     files.</li> <li>For enhanced security, consider using IAM roles instead of hardcoding     access keys.</li> <li>Refer to the AWS S3 documentation for more     information about AWS S3.</li> </ul>"},{"location":"adapters/sqlalchemy/","title":"SQLAlchemy Adapter","text":"<p>The <code>file_keeper:sqlalchemy</code> adapter allows you to use a relational database managed by SQLAlchemy for storing and retrieving file content. This adapter stores file content as binary data within the database.</p>"},{"location":"adapters/sqlalchemy/#overview","title":"Overview","text":"<p>This adapter provides a way to integrate a relational database with file-keeper. You'll need to have SQLAlchemy installed and a configured database connection string.  This adapter is suitable for scenarios where you need to store file metadata alongside the file content in a structured manner.</p> <pre><code>pip install 'file-keeper[sqlalchemy]'\n\n## or\n\npip install sqlalchemy\n</code></pre>"},{"location":"adapters/sqlalchemy/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the SQLAlchemy adapter:</p> <pre><code>storage = make_storage(\"my_sqlalchemy_storage\", {\n    \"type\": \"file_keeper:sqlalchemy\",\n    \"db_url\": \"sqlite:///:memory:\",\n    \"table_name\": \"file-keeper\",\n    \"location_column\": \"location\",\n    \"content_column\": \"content\",\n    \"initialize\": True,\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace the placeholder values with your actual database     connection string and table name.</li> <li>Enable <code>initialize</code> flag or ensure that the specified table exists in your     database.</li> <li>The database connection string should be in a format supported by     SQLAlchemy.  Refer to the SQLAlchemy     documentation for     details.</li> <li>Consider the performance implications of storing large files directly in     the database.</li> </ul>"},{"location":"adapters/zip/","title":"ZIP Archive Adapter","text":"<p>The <code>file_keeper:zip</code> adapter allows you to use a ZIP archive as a storage backend. This adapter is useful for packaging and distributing files in a single archive.</p>"},{"location":"adapters/zip/#overview","title":"Overview","text":"<p>This adapter stores files within a ZIP archive. You'll need to provide the path to the ZIP archive.  The adapter can create the ZIP archive if it doesn't exist.</p>"},{"location":"adapters/zip/#initialization-example","title":"Initialization Example","text":"<p>Here's an example of how to initialize the ZIP archive adapter:</p> <pre><code>storage = make_storage(\"my_zip_storage\", {\n    \"type\": \"file_keeper:zip\",\n    \"path\": \"/tmp/my_archive.zip\",\n})\n</code></pre> <p>Important Notes:</p> <ul> <li>Replace <code>/tmp/my_archive.zip</code> with the desired path to your ZIP archive.</li> <li>This adapter is suitable for smaller files, as ZIP archives can become slow     to process with a large number of files.</li> </ul>"},{"location":"core_concepts/capabilities/","title":"Capabilities","text":"<p>The Capability enum defines the features supported by different storage backends. It's a fundamental concept in file-keeper, enabling the system to adapt to the limitations and strengths of each storage provider. This document explains the Capability enum, how to check for support, and provides practical examples.</p>"},{"location":"core_concepts/capabilities/#what-are-capabilities","title":"What are Capabilities?","text":"<p>Not all storage systems are created equal. Some support resumable uploads, while others don't. Some offer efficient copy and move operations, while others require you to handle this type of tasks yourself. Capabilities provide a standardized way to describe what a particular storage backend can do.</p> <p>The Capability enum is a set of flags, each representing a specific feature.  By checking which capabilities a Storage object possesses, file-keeper can dynamically adjust its behavior to ensure compatibility and optimal performance.</p>"},{"location":"core_concepts/capabilities/#the-capability-enum","title":"The Capability Enum","text":"<p>Here's a breakdown of the key capabilities:</p> Capability Description ANALYZE Return file details from the storage. APPEND Add content to the existing file. COMPOSE Combine multiple files into a new one in the same storage. COPY Make a copy of the file inside the same storage. CREATE Create a file as an atomic object. EXISTS Check if file exists. LINK_PERMANENT Make permanent download link. LINK_TEMPORAL Make expiring download link. LINK_ONE_TIME Make one-time download link. MOVE Move file to a different location inside the same storage. MULTIPART Create file in 3 stages: initialize, upload(repeatable), complete. RANGE Return specific range of bytes from the file. REMOVE Remove file from the storage. RESUMABLE Perform resumable uploads that can be continued after interruption. SCAN Iterate over all files in the storage. SIGNED Generate signed URL for specific operation. STREAM Return file content as stream of bytes."},{"location":"core_concepts/capabilities/#checking-for-capability-support","title":"Checking for Capability Support","text":"<p>You can determine if a Storage object supports a specific capability using the supports() method:</p> <pre><code>from file_keeper import Capability, make_storage, make_upload\n\nstorage = make_storage(\"my_storage\", {\"adapter\": \"s3\", \"region\": \"us-east-1\"})\n\nif storage.supports(Capability.REMOVE | Capability.CREATE):\n    upload = make_upload(b\"hello world\")\n    info = storage.upload(\"hello.txt\", upload)\n    storage.remove(info)\n\nelse:\n    raise TypeError(\"File creation and removal is not supported by this storage.\")\n\n\nif storage.supports(Capability.EXISTS):\n    assert not storage.exists(info)\n\nelse:\n    print(\"File existence cannot be checked by this storage.\")\n</code></pre> <p>The <code>supports()</code> method returns <code>True</code> if the storage backend has the specified capability, and <code>False</code> otherwise.</p>"},{"location":"core_concepts/data_model/","title":"Data model","text":"<p>This document details the core data models used within the file-keeper system: Storage, FileData, and Location. It explains the why behind these concepts and how they work together to provide a flexible and reliable file storage solution.</p>"},{"location":"core_concepts/data_model/#storage","title":"Storage","text":"<p>The Storage class is the central point of interaction with any underlying storage system \u2013 whether that's a cloud provider like AWS S3 or Google Cloud Storage, a local filesystem, or something else entirely. Think of it as an adapter that translates file-keeper's generic requests into the specific language of the storage backend.</p> <p>Instead of directly dealing with the complexities of each storage system, you interact with Storage objects.  file-keeper handles the details of connecting, authenticating, and performing operations.  Each Storage instance is equipped with specialized services \u2013 Uploader, Reader, and Manager \u2013 to handle different types of tasks.  This separation of concerns makes the system more modular and easier to extend.</p>"},{"location":"core_concepts/data_model/#filedata","title":"FileData","text":"<p>FileData represents a file as known to file-keeper. It's a metadata record that contains everything we need to identify and manage a file, regardless of where it's physically stored.</p> <p>Crucially, FileData is independent of the underlying storage. It's a consistent representation that allows file-keeper to work with different storage backends seamlessly.  It tracks essential information like the file's name, size, content type, and content hash.  It can also store additional, storage-specific metadata in the storage_data field.</p> <p>FileData is the key to operations like tracking progress during uploads, managing file versions, and providing consistent access to file information across different storage systems.</p>"},{"location":"core_concepts/data_model/#location","title":"Location","text":"<p>Location represents the address of a file within a specific storage system.  It's a simple string, but it's given a specific meaning within file-keeper.</p> <p>Think of it as a path or key that uniquely identifies a file within its storage backend.  The format of a Location will vary depending on the storage system (e.g., an S3 key, a filesystem path).</p> <p>file-keeper uses Location objects to tell the Storage adapter where to find a file.  It also provides a mechanism for transforming Location objects to different formats if needed, allowing for flexibility and compatibility with different storage systems.</p>"},{"location":"core_concepts/uploads/","title":"Upload","text":"<p>The Upload class represents the data you want to store in a storage backend. It's a key component in file-keeper, encapsulating the file's content, metadata, and instructions for how to transfer it to storage. This document explains how to create and use Upload objects, covering streaming uploads and hashing for data integrity.</p>"},{"location":"core_concepts/uploads/#what-is-an-upload","title":"What is an Upload?","text":"<p>Think of an Upload as a package containing everything needed to send a file to storage. It's more than just the raw data; it includes information about the file itself, like its name, size, and content type. This metadata is essential for proper storage and retrieval.</p> <p>The Upload object decouples the source of the data from the transfer process. This allows file-keeper to handle various data sources \u2013 files on disk, in-memory buffers, network streams \u2013 without changing the core storage logic.</p>"},{"location":"core_concepts/uploads/#creating-an-upload-object","title":"Creating an Upload Object","text":"<p>You can create an Upload object in several ways, depending on the source of your data. The recommended way is using make_upload helper:</p> File-like objectByte stringWerkzeug's FileStorageManually <p>If you have an open file, you can directly pass it to the make_upload function. file-keeper will handle reading the data from the file.</p> <pre><code>src = open(\"my_image.jpg\", \"rb\")\nupload = make_upload(src)\n</code></pre> <p>If your data is already in memory as a byte string, you can pass it directly.</p> <pre><code>data = b\"This is the content of my file.\"\nupload = make_upload(data)\n</code></pre> <p>When writing an application using werkzeug-based framework you can handle uploaded files in this way.</p> <pre><code>from werkzeug.datastructures import FileStorage\n\ndata = FileStorage(..., \"my_data.txt\")\nupload = make_upload(data)\n</code></pre> <p>This is useful for large files that don't fit in memory. You need to provide an object that has methods <code>read</code> and <code>__iter__</code> producing byte string as a first argument to Upload class. If you have a generator that yields data, wrap it into IterableBytesReader instead of manually implementing class with required methods:</p> <pre><code>from file_keeper import Upload, IterableBytesReader\n\ndef data_generator():\n    yield b\"hello\"\n    yield b\" \"\n    yield b\"world\"\n\nstream = IterableBytesReader(data_generator())\nupload = Upload(stream, \"my_file.txt\", 11, \"text/plain\")\n</code></pre> <p>The make_upload function automatically determines the file size and content type for supported source types.</p>"},{"location":"core_concepts/uploads/#streaming-uploads","title":"Streaming Uploads","text":"<p>For very large files, loading the entire content into memory is impractical. Streaming uploads allow you to send the data in chunks, reducing memory usage and improving performance.</p> <p>As shown in the example above, you can create an Upload object from an iterable of bytes. file-keeper will then stream the data to the storage backend as it becomes available. This is the preferred method for handling large files, but it expects that you compute the size and content type of the upload in advance and provide these details to the Upload constructor.</p>"},{"location":"core_concepts/uploads/#hashing-for-data-integrity","title":"Hashing for Data Integrity","text":"<p>Data integrity is crucial when transferring files. Hashing ensures that the data you upload is exactly the same as the data stored in the backend. file-keeper automatically calculates a hash of the upload data during the transfer process.</p> <p>The calculated hash is stored as part of the FileData metadata. When you retrieve the file, file-keeper can recalculate the hash via Storage.analyze method to verify data integrity. If the hashes don't match, it indicates that the file has been corrupted or tampered with.</p> <p>Hashing is performed transparently during the upload process, so you don't need to worry about implementing it yourself. It provides an extra layer of assurance that your data is stored reliably.</p>"},{"location":"extending/adapters/","title":"Storage adapters","text":"<p>The core of file-keeper's flexibility lies in its storage adapters. Adapters encapsulate the logic for interacting with a specific storage system, allowing file-keeper to remain agnostic to the underlying implementation.  To create a custom adapter, you'll need to define a class that inherits from Storage and implements its services.</p>"},{"location":"extending/adapters/#steps-to-create-a-custom-adapter","title":"Steps to create a custom adapter","text":""},{"location":"extending/adapters/#define-a-settings-class","title":"Define a Settings class","text":"<p>Create a dataclass to hold the configuration options for your storage adapter. This class should inherit from Settings.  This allows file-keeper to handle validation and default values.</p> <p>Example</p> <pre><code>from dataclasses import dataclass\nimport file_keeper as fk\n\n@dataclass\nclass MyStorageSettings(fk.Settings):\n    api_key: str = \"\"\n    endpoint: str = \"\"\n</code></pre>"},{"location":"extending/adapters/#extend-the-storage-class","title":"Extend the Storage class","text":"<p>Create a class that inherits from Storage and sets the SettingsFactory class attribute to your settings class. It also sets UploaderFactory and ReaderFactory in the same way. The implementation will follow soon.</p> <p>Example</p> <pre><code>...\n\nclass MyStorage(fk.Storage):\n    settings: MyStorageSettings\n\n    SettingsFactory = MyStorageSettings\n    UploaderFactory = MyUploader\n    ReaderFactory = MyReader\n</code></pre>"},{"location":"extending/adapters/#implement-uploader-and-reader-services","title":"Implement Uploader and Reader services","text":"<p>Create classes for UploaderFactory and ReaderFactory that inherit from Uploader and Reader respectively. These classes will contain the logic for uploading and reading files to and from your storage system.</p> <p>Make sure to add CREATE capability to <code>Uploader</code> and STREAM capability to <code>Reader</code>. Otherwise storage will pretend that these services do not support these operations</p> <p>Example</p> <pre><code>class MyUploader(fk.Uploader):\n\n    capabilities = fk.Capability.CREATE\n\n    def upload(self, location: fk.Location, upload: fk.Upload, extras: dict[str, Any]) -&gt; fk.FileData:\n        # Implement your upload logic here\n        reader = upload.hashing_reader()\n        for chunk in reader:\n            # send fragment to storage\n\n        return fk.FileData(location, upload.size, upload.content_type, reader.get_hash())\n\n\nclass MyReader(fk.Reader):\n\n    capabilities = fk.Capability.STREAM\n\n    def stream(self, data: fk.FileData, extras: dict[str, Any]) -&gt; Iterable[bytes]:\n        # Implement your streaming logic here\n        for chunk in file_stream:\n            yield chunk\n</code></pre>"},{"location":"extending/adapters/#register-the-adapter","title":"Register the adapter","text":"<p>If you are going to use custom adapter only inside a single script, you can register it directly using <code>adapters</code> registry:</p> <pre><code>fk.adapters.register(\"local\", MyStorage)\n</code></pre> <p>If you are writing a library that will be used accross multiple project it's better to register storage using entrypoints of the python package.</p> <p>Use the <code>register_adapters</code> hook to register your adapter. This makes it available as a <code>type</code> inside make_storage.</p> <pre><code>@fk.hookimpl\ndef register_adapters(registry: fk.Registry[type[fk.Storage]]):\n    registry.register(\"local\", MyStorage)\n</code></pre>"},{"location":"extending/adapters/#initialize-the-adapter","title":"Initialize the adapter","text":"<p>Now you can use your custom adapter:</p> <pre><code>storage = fk.make_storage(\"local\", {\n    \"adapter\": \"local\",\n    \"api_key\": \"123\",\n    \"endpoint\": \"https://example.local\",\n})\n</code></pre> <p>This is a basic example, but it demonstrates the fundamental principles of creating a custom storage adapter.  You can extend this example to support more complex features and integrate with a wider range of storage systems.</p>"},{"location":"extending/dissection/","title":"Dissecting storage adapter","text":"<p>This guide walks you through implementation of filesystem adapter. We'll look at every part of it from the outer layer and going inwards, analyzing the meaning of the units and reasons they exist.</p>"},{"location":"extending/dissection/#register-an-adapter","title":"Register an adapter","text":"<p>Adapters must be registered to make them available via make_storage.</p> <p>Define an entry-point with the name <code>file_keeper_ext</code> in the distribution. This entry-point specifies python module that contains implementation of the pluggy hooks that extends file-keeper.</p> <pre><code>[project.entry-points.file_keeper_ext]\nfile_keeper = \"file_keeper.default\"\n</code></pre> <p>file-keeper expects to find a function decorated with <code>@file_keeper.hookimpl</code> and named <code>register_adapters</code> inside the module. This function registers all custom adapters of the module via call to <code>register()</code> method of the Registry with adapters. This call accepts the name of the adapter as a first argument, and the class of the adapter as a second argument.</p> <pre><code>@ext.hookimpl\ndef register_adapters(registry: Registry[type[Storage]]):\n    \"\"\"Built-in storage adapters.\"\"\"\n    registry.register(\"file_keeper:fs\", adapters.FsStorage)\n</code></pre> <p>Adapter names have no restrictions regarding length or allowed symbols, but it's recommended to use <code>&lt;package&gt;:&lt;type&gt;</code> structure, like in <code>file_keeper:fs</code>. If you provide multiple similar adapters, consider adding third segment, i.e. <code>file_keeper:fs:v1</code>, <code>file_keeper:fs:v2</code>.</p>"},{"location":"extending/dissection/#create-an-adapter","title":"Create an adapter","text":"<p>The adapter itself is simple and usually contains just few lines of code.</p> <p>It must extend Storage and, optionally, it can override SettingsFactory, UploaderFactory, ManagerFactory, and ReaderFactory.</p> <pre><code>class FsStorage(fk.Storage):\n    \"\"\"Store files in local filesystem.\"\"\"\n\n    settings: Settings\n\n    SettingsFactory = Settings\n    UploaderFactory = Uploader\n    ReaderFactory = Reader\n    ManagerFactory = Manager\n</code></pre> <p>Filesystem adapter overrides all these attributes because:</p> <ul> <li>it contains custom settings(<code>SettingsFactory</code>)</li> <li>it defines how the file is uploaded (<code>UploaderFactory</code>)</li> <li>it defines how the file is managed, i.e. removed, copied, analyzed (<code>ManagerFactory</code>)</li> <li>it defines how file is read (<code>ReaderFactory</code>)</li> </ul> <p>Additionally it specifies type of <code>settings</code> attribute as <code>settings: Settings</code>, i.e. custom <code>Settings</code> class that is defined in the same module. This is done to simplify typing and does not affect the behavior of the adapter. Without this line typechecker assumes that storage uses base Settings and complains when custom options are accessed.</p>"},{"location":"extending/dissection/#define-storage-settings","title":"Define storage settings","text":"<p>Create a dataclass <code>Settings</code> to hold configuration options specific to your storage. This class should inherit from Settings.</p> <pre><code>@dataclasses.dataclass()\nclass Settings(fk.Settings):\n    \"\"\"Settings for FS storage.\"\"\"\n</code></pre> <p>Filesystem settings do not introduce new options, so there are no attributes here. But some other provider would do the following:</p> <pre><code>@dataclasses.dataclass()\nclass Settings(fk.Settings):\n    bucket: str = \"\"\n    username: str = \"\"\n    password: str = \"\"\n    params: dict[str, Any] = dataclasses.field(default_factory=dict)\n</code></pre> <p>These options must include default values due to dataclass restrictions, even if storage will not work with these defaults. E.g., empty password and username won't work usually, but you still have to specify them.</p> <p>And now happens validation. It's provided by the <code>__post_init__</code> method of the dataclass.</p> <pre><code>    def __post_init__(self, **kwargs: Any):\n        super().__post_init__(**kwargs)\n\n        if not os.path.exists(self.path):\n            if not self.initialize:\n                raise fk.exc.InvalidStorageConfigurationError(\n                    self.name,\n                    f\"path `{self.path}` does not exist\",\n                )\n\n            try:\n                os.makedirs(self.path)\n            except PermissionError as err:\n                raise fk.exc.InvalidStorageConfigurationError(\n                    self.name,\n                    f\"path `{self.path}` is not writable\",\n                ) from err\n</code></pre> <p>Filesystem adapter relies on two generic options. First is <code>initialize</code>. When this flag is enabled, storage checks whether directory for files exists and, if it's missing, tries to create this directory. If task fails, InvalidStorageConfigurationError is raised. That's how storage reacts on problems with configuration.</p> <p>Second option is <code>path</code>. <code>path</code>, usually absolute, defines the location in filesystem where files are stored. Other storages may treat it differently: cloud storages use <code>path</code> as a prefix of the file name, because cloud storages do not support directory hierarchy; SQLAlchemy storage ignores path as it has no meaning in DB context.</p> <p>Apart from this, storages often initialize and store connections to external services as storage attributes. For example, <code>file_keeper:azure_blob</code> has <code>container_name</code> string options that holds the name of cloud container where files are stored. Inside <code>__post_init__</code> it connects to the container and stores the container object itself as <code>container</code> property, so that storage doesn't need to constantly re-connect to the containers.</p>"},{"location":"extending/dissection/#create-the-uploader-service","title":"Create the uploader service","text":"<p>The next target is <code>Uploader</code>. It's a service reesponsible for file creation that must extend Uploader.</p> <pre><code>class Uploader(fk.Uploader):\n    \"\"\"Filesystem uploader.\"\"\"\n</code></pre> <p>Any service consists of two parts - methods and capabilities. Methods describe the logic but are hidden from storage initially. I.e., if you only define methods and ignore capabilities, storage will pretend that service cannot perform the task.</p> <pre><code>    @override\n    def upload(self, location: fk.types.Location, upload: fk.Upload, extras: dict[str, Any]) -&gt; fk.FileData:\n        ...\n</code></pre> <p>And capabilities actually tell storage about operations supported by storage. Because of this separation, you can pretend that storage cannot perform an operation, even when it's supported. In this way you can transform filesystem into a read-only storage without code changes and guarantee that files won't be accidentally removed from it.</p> <pre><code>    capabilities: fk.Capability = fk.Capability.CREATE | fk.Capability.MULTIPART\n</code></pre> <p>As you can see, FS storage supports CREATE and MULTIPART. Capabilities are implemented as bit masks and can be combined using <code>|</code> operator.</p> <p>Let's look closer at the <code>upload()</code> method.</p> <p>It computes full path to the file location in the beginning. <code>full_path()</code> is a generic method of the storage that naively combine <code>path</code> option of the storage with <code>location</code> of the file. Every storage has <code>path</code> option and every storage can use <code>full_path()</code> to attach <code>path</code> as a prefix to the location, if it makes any sense.</p> <pre><code>        dest = self.storage.full_path(location)\n</code></pre> <p>Now storage uses another generic option, <code>override_existing</code>. If it's disabled and given location already taken by another file, uploader raises ExistingFileError. That's recommended reaction in such situation and you'll notice that other storages also follow this process.</p> <pre><code>        if os.path.exists(dest) and not self.storage.settings.override_existing:\n            raise fk.exc.ExistingFileError(self.storage, location)\n</code></pre> <p>Then storage ensures that all intermediate folders from the final file's location are present. If you expect that files are loaded directly into <code>path</code>, it may seem redundant. But FS storage does not imply such restrictions and there may be nested directories under the <code>path</code>, so verifying that all folders are created is a safest option.</p> <pre><code>        os.makedirs(os.path.dirname(dest), exist_ok=True)\n</code></pre> <p>Then file is actually written to the FS. You an read file content using <code>upload.read()</code>, but here we create HashingReader using <code>hashing_reader()</code> method of the Upload. This object also has <code>read()</code> method, but in addition it computes the content hash of the file while it's consumed. As result we have the hash in the and almost for free.</p> <pre><code>        reader = upload.hashing_reader()\n        with open(dest, \"wb\") as fd:\n            for chunk in reader:\n                fd.write(chunk)\n</code></pre> <p>Other storages, like AWS S3 or Azure Blob Storage, do not need this step, because content hash is computed by cloud provided and returned with the metadata of the uploaded object. But if you don't have a cheap way to obtain the hash, using HashingReader is the recommended option.</p> <p>In the end, <code>upload()</code> method of the service builds FileData with details of the uploaded file and returns it to the caller.</p> <pre><code>        return fk.FileData(\n            location,\n            os.path.getsize(dest),\n            upload.content_type,\n            reader.get_hash(),\n        )\n</code></pre>"},{"location":"extending/dissection/#create-the-reader-service","title":"Create the reader service","text":"<p><code>Reader</code> service is much simpler than <code>Uploader</code>. It exposes STREAM capability to notify the storage, that files can be read from the storage. And it implements <code>stream()</code> method, that explains how exactly bytes of content are obtained.</p> <pre><code>class Reader(fk.Reader):\n    \"\"\"Filesystem reader.\"\"\"\n\n    storage: FsStorage\n    capabilities: fk.Capability = fk.Capability.STREAM\n\n    @override\n    def stream(self, data: fk.FileData, extras: dict[str, Any]) -&gt; IO[bytes]:\n        \"\"\"Return file open in binary-read mode.\n\n        Raises:\n            MissingFileError: file does not exist\n\n        Returns:\n            File content iterator\n        \"\"\"\n        filepath = self.storage.full_path(data.location)\n        if not os.path.exists(filepath):\n            raise fk.exc.MissingFileError(self.storage, data.location)\n\n        return open(filepath, \"rb\")  # noqa: SIM115\n</code></pre> <p>Note how it computes the path to the file using <code>full_path()</code>, just as <code>Uploader</code> did. Basically, every method that access the file should use <code>full_path()</code>.</p> <p>Also, pay attention to MissingFileError raised if file does not exist. That's the recommended way to handle missing files.</p> <p>Finally, look at return result. The <code>stream()</code> method must return <code>Iterable[bytes]</code>, but not just bytes, e.g. <code>return b\"hello\"</code> is not valid output.</p> <p>Anything that can be used in a for-loop and produce <code>bytes</code> is a valid output of the <code>steam()</code> method. Few examples:</p> List of byte stringsGenerator of bytesio.BytesIODescriptor of the file opened in <code>rb</code> mode <pre><code>...\nreturn [b\"hello\", b\" \", b\"world\"]\n</code></pre> <pre><code>...\nyield b\"hello\"\nyield b\" \"\nyield b\"world\"\n</code></pre> <pre><code>...\nreturn BytesIO(b\"hello world\")\n</code></pre> <pre><code>...\nreturn open(path, \"rb\")\n</code></pre>"},{"location":"extending/dissection/#create-the-manager-service","title":"Create the manager service","text":"<p><code>Manager</code> service contains a lot of methods and capabilities, but all of them are pretty straight forward.</p> <p>Remember, that capabilities tell the storage \"what\" service can do, while method implementations explain \"how\" it's done. Usually, capability and method come in pair, unless you are certain that you need to separate them.</p> <pre><code>    capabilities: fk.Capability = (\n        fk.Capability.REMOVE\n        | fk.Capability.SCAN\n        | fk.Capability.EXISTS\n        | fk.Capability.ANALYZE\n        | fk.Capability.COPY\n        | fk.Capability.MOVE\n        | fk.Capability.COMPOSE\n        | fk.Capability.APPEND\n    )\n</code></pre> <p><code>remove()</code> method removes the object. If it's removed, the result is <code>True</code>. If it's not removed(because it does not exists), the result is <code>False</code>.</p> <pre><code>    @override\n    def remove(self, data: fk.FileData, extras: dict[str, Any]) -&gt; bool:\n        \"\"\"Remove the file.\"\"\"\n        filepath = self.storage.full_path(data.location)\n        if not os.path.exists(filepath):\n            return False\n\n        os.remove(filepath)\n        return True\n</code></pre> <p><code>scan()</code> returns an iterable of strings with names of all files available in the storage.</p> <pre><code>    @override\n    def scan(self, extras: dict[str, Any]) -&gt; Iterable[str]:\n        \"\"\"Discover filenames under storage path.\"\"\"\n        path = self.storage.settings.path\n        search_path = os.path.join(path, \"**\")\n\n        for entry in glob.glob(search_path, recursive=True):\n            if not os.path.isfile(entry):\n                continue\n            yield os.path.relpath(entry, path)\n</code></pre> <p><code>exists()</code> returns <code>True</code> if file exists, and <code>False</code> if file is missing.</p> <pre><code>    @override\n    def exists(self, data: fk.FileData, extras: dict[str, Any]) -&gt; bool:\n        \"\"\"Check if file exists.\"\"\"\n        filepath = self.storage.full_path(data.location)\n        return os.path.exists(filepath)\n</code></pre> <p><code>analyze()</code> returns the same FileData as one, produced during <code>upload()</code>.</p> <pre><code>    @override\n    def analyze(self, location: fk.types.Location, extras: dict[str, Any]) -&gt; fk.FileData:\n        \"\"\"Return all details about location.\n\n        Raises:\n            MissingFileError: file does not exist\n        \"\"\"\n        filepath = self.storage.full_path(location)\n        if not os.path.exists(filepath):\n            raise fk.exc.MissingFileError(self.storage, location)\n\n        with open(filepath, \"rb\") as src:\n            reader = fk.HashingReader(src)\n            content_type = magic.from_buffer(next(reader, b\"\"), True)\n            reader.exhaust()\n\n        return fk.FileData(\n            location,\n            size=reader.position,\n            content_type=content_type,\n            hash=reader.get_hash(),\n        )\n</code></pre> <p><code>copy()</code> creates a copy of the file, raising MissingFileError if source file is missing and ExistingFileError if destination file already exist and <code>override_existing</code> is disabled.</p> <pre><code>    @override\n    def copy(self, location: fk.types.Location, data: fk.FileData, extras: dict[str, Any]) -&gt; fk.FileData:\n        \"\"\"Copy file inside the storage.\n\n        Raises:\n            ExistingFileError: file exists and overrides are not allowed\n            MissingFileError: source file does not exist\n        \"\"\"\n        src = self.storage.full_path(data.location)\n        dest = self.storage.full_path(location)\n\n        if not os.path.exists(src):\n            raise fk.exc.MissingFileError(self.storage, data.location)\n\n        if os.path.exists(dest) and not self.storage.settings.override_existing:\n            raise fk.exc.ExistingFileError(self.storage, location)\n\n        shutil.copy(src, dest)\n        return fk.FileData.from_object(data, location=location)\n</code></pre> <p><code>move()</code> behaves exactly like <code>copy()</code>. In addition, the original file is not available after the move. Basically, <code>move()</code> does \"rename\" if possible, and \"copy\" + \"remove\" if not.</p> <pre><code>    @override\n    def move(self, location: fk.types.Location, data: fk.FileData, extras: dict[str, Any]) -&gt; fk.FileData:\n        \"\"\"Move file to a different location inside the storage.\n\n        Raises:\n            ExistingFileError: file exists and overrides are not allowed\n            MissingFileError: source file does not exist\n        \"\"\"\n        src = self.storage.full_path(data.location)\n        dest = self.storage.full_path(location)\n\n        if not os.path.exists(src):\n            raise fk.exc.MissingFileError(self.storage, data.location)\n\n        if os.path.exists(dest):\n            if self.storage.settings.override_existing:\n                os.remove(dest)\n            else:\n                raise fk.exc.ExistingFileError(self.storage, location)\n\n        shutil.move(src, dest)\n        return fk.FileData.from_object(data, location=location)\n</code></pre> <p><code>compose()</code> is the most challenging method of the <code>Manager</code>. It takes few existing files and combines them into a new one, similar to the <code>cat</code> utility.</p> <pre><code>    @override\n    def compose(self, location: fk.types.Location, datas: Iterable[fk.FileData], extras: dict[str, Any]) -&gt; fk.FileData:\n        \"\"\"Combine multipe files inside the storage into a new one.\n\n        If final content type is not supported by the storage, the file is\n        removed.\n\n        Raises:\n            ExistingFileError: file exists and overrides are not allowed\n            MissingFileError: source file does not exist\n        \"\"\"\n        dest = self.storage.full_path(location)\n\n        if os.path.exists(dest) and not self.storage.settings.override_existing:\n            raise fk.exc.ExistingFileError(self.storage, location)\n\n        sources: list[str] = []\n        for data in datas:\n            src = self.storage.full_path(data.location)\n\n            if not os.path.exists(src):\n                raise fk.exc.MissingFileError(self.storage, data.location)\n\n            sources.append(src)\n\n        with open(dest, \"wb\") as to_fd:\n            for src in sources:\n                with open(src, \"rb\") as from_fd:\n                    shutil.copyfileobj(from_fd, to_fd)\n\n        return self.analyze(location, extras)\n</code></pre> <p><code>append()</code> takes content of the existing file and adds it in the end of another file.</p> <pre><code>    @override\n    def append(self, data: fk.FileData, upload: fk.Upload, extras: dict[str, Any]) -&gt; fk.FileData:\n        \"\"\"Append content to existing file.\n\n        If final content type is not supported by the storage, original file is\n        removed.\n\n        Raises:\n            MissingFileError: file does not exist\n        \"\"\"\n        dest = self.storage.full_path(data.location)\n        if not os.path.exists(dest):\n            raise fk.exc.MissingFileError(self.storage, data.location)\n\n        with open(dest, \"ab\") as fd:\n            fd.write(upload.stream.read())\n\n        return self.analyze(data.location, extras)\n</code></pre>"},{"location":"extending/location_transformers/","title":"Location transformers","text":"<p>Location transformers allow you to modify the Location string before it's used to access a file in the storage backend. This is useful for scenarios where you need to perform additional processing or formatting on the location, such as adding prefixes, encoding characters, or generating unique identifiers.</p>"},{"location":"extending/location_transformers/#what-are-location-transformers","title":"What are location transformers?","text":"<p>A location transformer is a callable (usually a function) that takes the original Location string, optional Upload or FileData object, and any extra data as input, and returns a modified Location string. They provide a flexible way to customize how locations are handled by file-keeper.</p> <p>Location transformers are set per-storage via location_transformers option. To apply them call prepare_location() method.</p> <p>Example</p> <pre><code>storage = make_storage(\"test\", {\n    \"type\": \"file_keeper:fs\",\n    \"location_transformers\": [\"safe_relative_path\"],\n    \"path\": \"/tmp\",\n})\n\nunsafe_location = \"../etc/passwd\"\n\nsafe_location = storage.prepare_location(unsafe_location)\n\nstorage.upload(safe_location, ...)\n</code></pre>"},{"location":"extending/location_transformers/#steps-to-create-a-custom-location-transformer","title":"Steps to create a custom location transformer","text":""},{"location":"extending/location_transformers/#define-your-transformer","title":"Define your transformer","text":"<p>Create a function that accepts the Location, optional Upload or FileData, and <code>extras</code> as input and returns the transformed Location.</p> <pre><code>def my_location_transformer(location, data, extras):\n    # Perform custom transformation here\n    return \"prefix_\" + location\n</code></pre>"},{"location":"extending/location_transformers/#register-the-transformer","title":"Register the transformer","text":"<p>Use the <code>register_location_transformers</code> hook to register your transformer. This makes it available for use when creating or accessing files.</p> <pre><code>import file_keeper as fk\n\n@fk.hookimpl\ndef register_location_transformers(registry):\n    registry.register(\"my_transformer\", my_location_transformer)\n</code></pre>"},{"location":"extending/location_transformers/#using-your-custom-transformer","title":"Using Your Custom Transformer","text":"<p>To use your custom transformer, specify its name when creating a <code>Storage</code> object in the settings.</p> <pre><code>storage = make_storage(\"my_storage\", {\n    \"adapter\": \"s3\",\n    \"location_transformers\": [\"my_transformer\"],\n})\n</code></pre> <p>And apply <code>Storage.prepare_location</code> to original location:</p> <pre><code>transformed = storage.prepare_location(\"hello.txt\")\n\nassert transformed == \"prefix_hello.txt\"\n</code></pre>"},{"location":"extending/overview/","title":"Register file-keeper extension","text":"<p>file-keeper is designed to be highly extensible, allowing you to add new storage backends, upload factories, and location transformers without modifying the core library. This is achieved through the use of the Pluggy framework.</p>"},{"location":"extending/overview/#overview","title":"Overview","text":"<p>If you want to make your module that extends of file-keeper externally available, register it as a <code>file_keeper_ext</code> entry-point of your distribution.</p> pyproject.tomlsetup.py <pre><code>...\n\n[project.entry-points.file_keeper_ext]\nmy_storage_extension = \"my_storage.my_module\"\n</code></pre> <pre><code>from setuptools import setup\n\nsetup(\n    ...,\n    entry_points={\n        \"file_keeper_ext\": [\"my_storage_extension = my_storage.my_module\"],\n    },\n)\n</code></pre> <p>file-keeper iterates through all modules that are registered under <code>file_keeper_ext</code> entry-point and extract pluggy hook implementations from them. In this way, anyone who've installed your library will have access to your customizations.</p>"},{"location":"extending/overview/#available-extension-points","title":"Available extension points","text":"<p>The module that contains file-keeper's extension has to define functions that register new functionality. These functions must have the same name as one of file-keeper's hooks and be decorated with <code>@file_keeper.hookimpl</code> decorator.</p> <p>The following hooks are currently available:</p> Hook Description <code>register_adapters</code> Use this function to register new storage adapters. The <code>registry</code> object allows you to add your <code>Storage</code> class to the list of available storage options. <code>register_location_transformers</code> Use this function to register new location transformers. The <code>registry</code> object allows you to add your <code>LocationTransformer</code> function to the list of available location transformers. <p>Example</p> <p>Register new storage adapter:</p> <pre><code>@fk.hookimpl\ndef register_adapters(registry):\n    registry.register(\"my_custom_adapter\", MyStorageClass)\n</code></pre> <p>Register new location transformers:</p> <pre><code>@fk.hookimpl\ndef register_location_transformers(registry):\n    registry.register(\"my_custom_transformer\", transformer_func)\n</code></pre>"},{"location":"extending/overview/#example-registering-a-custom-storage-adapter","title":"Example: registering a custom storage adapter","text":"<p>Let's say you want to add a new storage adapter that stores files in a local directory.  Here's how you would do it:</p> <ol> <li> <p>Create a new Python package (<code>my_storage_extension</code>)</p> </li> <li> <p>Inside your package, create a module ( <code>my_storage.py</code>) with the following content:</p> <pre><code>import file_keeper as fk\n\nclass MyLocalStorage(fk.Storage):\n    ...\n\n@fk.hookimpl\ndef register_adapters(registry):\n    registry.register(\"my_local\", MyLocalStorage)\n</code></pre> </li> <li> <p>Add an entry point to your <code>setup.py</code> or <code>pyproject.toml</code> file:</p> setup.pypyproject.toml <pre><code>from setuptools import setup\n\nsetup(\n    ...,\n    entry_points={\n        \"file_keeper_ext\": [\"my_storage_extension = my_storage\"],\n    },\n)\n</code></pre> <pre><code>...\n\n[project.entry-points.file_keeper_ext]\nmy_storage_extension = \"my_storage\"\n</code></pre> </li> </ol> <p>Now, when file-keeper discovers your extension, it will register <code>MyLocalStorage</code> as a new storage option, accessible by the name \"my_local\".</p>"},{"location":"extending/overview/#example-registering-a-custom-location-transformer","title":"Example: registering a custom location transformer","text":"<p>Let's say you want to add a location transformer that prepends a prefix to all locations.</p> <ol> <li> <p>Create a new Python package</p> </li> <li> <p>Inside your package, create a file  with the following content:</p> <pre><code>import file_keeper as fk\n\ndef my_location_transformer(location: str, upload, extras: dict[str, any]) -&gt; str:\n    return f\"prefix_{location}\"\n\n@fk.hookimpl\ndef register_location_transformers(registry: Registry[types.LocationTransformer]):\n    registry.register(\"my_prefix\", my_location_transformer)\n</code></pre> </li> <li> <p>Add an entry point to your <code>setup.py</code> or <code>pyproject.toml</code> file</p> </li> </ol> <p>Now, when file-keeper discovers your extension, it will register a new location transformer, accessible by the name \"my_prefix\".</p>"},{"location":"extending/overview/#manual-extension","title":"Manual extension","text":"<p>Entry-points are good for python packages, but sometimes you need a custom storage just for the current script and don't want to mess with packages. In this case you can register adapter or location transformer manually, using <code>file_keeper.core.storage.adapters</code> and <code>file_keeper.core.storage.location_transformers</code> Registries.</p> <p>Example</p> <p>Let's say you have <code>MyStorage</code> adapter and <code>my_transformer</code> transformer. In the following snippet, they will be available after the <code>register()</code> calls in the correspoinding registries.</p> <pre><code>from file_keeper.core.storage import location_transformers, adapters\n\n# not available\n\nadapters.register(\"my_storage\", MyStorage)\nlocation_transformers.register(\"my_transformer\", my_transformer)\n\n# available\n\nstorage = make_storage(\"test\", {\n    \"type\": \"my_storage\",\n    \"location_transformers\": [\"my_transformer\"]\n})\n</code></pre>"}]}